<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark on 关注Java、Hadoop、Kubernetes和BI</title>
    <link>https://blog.javachen.space/categories/spark/</link>
    <description>Recent content in spark on 关注Java、Hadoop、Kubernetes和BI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 26 Jun 2015 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://blog.javachen.space/categories/spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>spark-shell脚本分析</title>
      <link>https://blog.javachen.space/2015/06/26/spark-shell-command/</link>
      <pubDate>Fri, 26 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/06/26/spark-shell-command/</guid>
      <description>本文主要分析spark-shell脚本的运行逻辑，涉及到spark-submit、spark-class等脚本的分析，希望通过分析脚本以了解</description>
    </item>
    
    <item>
      <title>Spark On YARN内存分配</title>
      <link>https://blog.javachen.space/2015/06/09/memory-in-spark-on-yarn/</link>
      <pubDate>Tue, 09 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/06/09/memory-in-spark-on-yarn/</guid>
      <description>本文主要了解Spark On YARN部署模式下的内存分配情况，因为没有深入研究Spark的源代码，所以只能根据日志去看相关的源代码，从而了解“为</description>
    </item>
    
    <item>
      <title>Spark配置参数</title>
      <link>https://blog.javachen.space/2015/06/07/spark-configuration/</link>
      <pubDate>Sun, 07 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/06/07/spark-configuration/</guid>
      <description>以下是整理的Spark中的一些配置参数，官方文档请参考Spark Configuration。 Spark提供三个位置用来配置系统： Spark属</description>
    </item>
    
    <item>
      <title>如何使用Spark ALS实现协同过滤</title>
      <link>https://blog.javachen.space/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als/</guid>
      <description>本文主要记录最近一段时间学习和实现Spark MLlib中的协同过滤的一些总结，希望对大家熟悉Spark ALS算法有所帮助。 更新： 【2016.</description>
    </item>
    
    <item>
      <title>编译CDH Spark源代码</title>
      <link>https://blog.javachen.space/2015/04/28/compile-cdh-spark-source-code/</link>
      <pubDate>Tue, 28 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/04/28/compile-cdh-spark-source-code/</guid>
      <description>本文以Cloudera维护的Spark分支项目为例，记录跟新Spark分支以及编译Spark源代码的过程。 下载代码 在Github上fork C</description>
    </item>
    
    <item>
      <title>Spark MLlib中的协同过滤</title>
      <link>https://blog.javachen.space/2015/04/17/spark-mllib-collaborative-filtering/</link>
      <pubDate>Fri, 17 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/04/17/spark-mllib-collaborative-filtering/</guid>
      <description>本文主要通过Spark官方的例子理解ALS协同过滤算法的原理和编码过程，然后通过对电影进行推荐来熟悉一个完整的推荐过程。 协同过滤 协同过滤常被</description>
    </item>
    
    <item>
      <title>Spark SQL中的数据源</title>
      <link>https://blog.javachen.space/2015/04/03/spark-sql-datasource/</link>
      <pubDate>Fri, 03 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/04/03/spark-sql-datasource/</guid>
      <description>Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、c</description>
    </item>
    
    <item>
      <title>Spark本地模式运行</title>
      <link>https://blog.javachen.space/2015/03/30/spark-test-in-local-mode/</link>
      <pubDate>Mon, 30 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/03/30/spark-test-in-local-mode/</guid>
      <description>Spark的安装分为几种模式，其中一种是本地运行模式，只需要在单节点上解压即可运行，这种模式不需要依赖Hadoop 环境。在本地运行模式中，m</description>
    </item>
    
    <item>
      <title>Spark SQL中的DataFrame</title>
      <link>https://blog.javachen.space/2015/03/26/spark-sql-dataframe/</link>
      <pubDate>Thu, 26 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/03/26/spark-sql-dataframe/</guid>
      <description>在2014年7月1日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上。在会议上，Databricks 表示，Shark 更多是</description>
    </item>
    
    <item>
      <title>如何将Avro数据加载到Spark</title>
      <link>https://blog.javachen.space/2015/03/24/how-to-load-some-avro-data-into-spark/</link>
      <pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/03/24/how-to-load-some-avro-data-into-spark/</guid>
      <description>这是一篇翻译，原文来自：How to load some Avro data into Spark。 首先，为什么使用 Avro ？ 最基本的格式是 CSV ，其廉价并且不需要顶一个一个 schema 和数据关联。 随后流行</description>
    </item>
    
    <item>
      <title>如何在CDH5上运行Spark应用</title>
      <link>https://blog.javachen.space/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/</link>
      <pubDate>Wed, 04 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/</guid>
      <description>这篇文章参考 How-to: Run a Simple Apache Spark App in CDH 5 编写而成，没有完全参照原文翻译，而是重新进行了整理，例如：spark 版本改为 1.3.0，添加了 Python 版的程序。 创</description>
    </item>
    
    <item>
      <title>Spark编程指南笔记</title>
      <link>https://blog.javachen.space/2015/02/03/spark-programming-guide/</link>
      <pubDate>Tue, 03 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2015/02/03/spark-programming-guide/</guid>
      <description>本文是参考Spark官方编程指南（Spark 版本为1.2）整理出来的学习笔记，主要是用于加深对 Spark 的理解，并记录一些知识点。 1. Spark介绍 S</description>
    </item>
    
    <item>
      <title>Spark集群安装和使用</title>
      <link>https://blog.javachen.space/2014/07/01/spark-install-and-usage/</link>
      <pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://blog.javachen.space/2014/07/01/spark-install-and-usage/</guid>
      <description>本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。 安装环境如下： 操作系统：CentOs 6.5 Hadoop 版本：cdh-5.4.0</description>
    </item>
    
  </channel>
</rss>