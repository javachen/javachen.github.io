<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on JavaChen Blog - Ramblings of a coder</title>
    <link>http://javachen.github.io/categories/hadoop/</link>
    <description>Recent content in hadoop on JavaChen Blog - Ramblings of a coder</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 28 Mar 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://javachen.github.io/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cloudera Manager安装Haddop集群</title>
      <link>http://javachen.github.io/2019/03/28/install-hadoop-cluster-with-cm6/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2019/03/28/install-hadoop-cluster-with-cm6/</guid>
      <description>在开始之前，请参考我博客中的关于如何安装cdh集群的文章，这里只做简单说明。因为只是为了测试，所以是在vagrant虚拟机中创建三个虚拟机搭</description>
    </item>
    
    <item>
      <title>将Avro数据转换为Parquet格式</title>
      <link>http://javachen.github.io/2015/03/25/converting-avro-data-to-parquet-format/</link>
      <pubDate>Wed, 25 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2015/03/25/converting-avro-data-to-parquet-format/</guid>
      <description>本文主要测试将Avro数据转换为Parquet格式的过程并查看 Parquet 文件的 schema 和元数据。 准备 将文本数据转换为 Parquet 格式并读取内容，可以参考 Cloudera 的 MapReduce 例子：</description>
    </item>
    
    <item>
      <title>Avro介绍</title>
      <link>http://javachen.github.io/2015/03/20/about-avro/</link>
      <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2015/03/20/about-avro/</guid>
      <description>1. 介绍 Avro 是 Hadoop 中的一个子项目，也是 Apache 中一个独立的项目，Avro 是一个基于二进制数据传输高性能的中间件。在 Hadoop 的其他项目中，例如 HBase 和 Hive 的 Client 端与服务</description>
    </item>
    
    <item>
      <title>安装和测试Kafka</title>
      <link>http://javachen.github.io/2015/03/17/install-and-test-kafka/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2015/03/17/install-and-test-kafka/</guid>
      <description>本文主要介绍如何在单节点上安装 Kafka 并测试 broker、producer 和 consumer 功能。 下载 进入下载页面：http://kafka.apache.or</description>
    </item>
    
    <item>
      <title>Hadoop Streaming 原理</title>
      <link>http://javachen.github.io/2015/02/12/hadoop-streaming/</link>
      <pubDate>Thu, 12 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2015/02/12/hadoop-streaming/</guid>
      <description>简介 Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的</description>
    </item>
    
    <item>
      <title>Useful Hadoop Commands</title>
      <link>http://javachen.github.io/2015/02/10/useful-commands-in-hadoop/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2015/02/10/useful-commands-in-hadoop/</guid>
      <description>hadoop 解压 gz 文件到文本文件 1 $ hadoop fs -text /hdfs_path/compressed_file.gz | hadoop fs -put - /tmp/uncompressed-file.txt 解压本地文件 gz 文件并上传到 hdfs 1 $ gunzip -c filename.txt.gz | hadoop fs -put - /tmp/filename.txt 使用 awk 处理 csv 文件，参考 Using awk and friends with Hadoop: 1 $ hadoop fs -cat</description>
    </item>
    
    <item>
      <title>安装和部署Presto</title>
      <link>http://javachen.github.io/2015/01/26/install-and-deploy-presto/</link>
      <pubDate>Mon, 26 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2015/01/26/install-and-deploy-presto/</guid>
      <description>1. 安装环境 操作系统：CentOs6.5 Hadoop 集群：CDH5.3 JDK 版本：jdk1.8.0_31 为了测试简单，我是将 Presto 的 coordinator 和 worker 都部署在 cdh1 节点上，并且</description>
    </item>
    
    <item>
      <title>Presto介绍</title>
      <link>http://javachen.github.io/2015/01/23/presto-overview/</link>
      <pubDate>Fri, 23 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2015/01/23/presto-overview/</guid>
      <description>1. 简介 Presto 是一个运行在集群之上的分布式系统。一个完全的安装报考一个 coordinator 进程和多个 workers 进程。查询通过一个客户端例如 Presto CLI 提交到 coordinator 进程。这个 coordinator 进程解析、</description>
    </item>
    
    <item>
      <title>Hadoop集群部署权限总结</title>
      <link>http://javachen.github.io/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop/</link>
      <pubDate>Tue, 25 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop/</guid>
      <description>这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项。如果你想了解详细的过程，请参考本博客中其他的文章。 1. 开始之前 hadoop 集群一共</description>
    </item>
    
    <item>
      <title>Zookeeper配置Kerberos认证</title>
      <link>http://javachen.github.io/2014/11/18/config-kerberos-in-cdh-zookeeper/</link>
      <pubDate>Tue, 18 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/11/18/config-kerberos-in-cdh-zookeeper/</guid>
      <description>参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下： 1 2 3 192.168.56.121 cdh1 NameNode</description>
    </item>
    
    <item>
      <title>Hadoop配置LDAP集成Kerberos</title>
      <link>http://javachen.github.io/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop/</link>
      <pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop/</guid>
      <description>本文主要记录 cdh hadoop 集群集成 ldap 的过程，这里 ldap 安装的是 OpenLDAP 。LDAP 用来做账号管理，Kerberos作为认证。授权一般来说是由应用来决定的，通过在 LDAP</description>
    </item>
    
    <item>
      <title>HDFS配置Kerberos认证</title>
      <link>http://javachen.github.io/2014/11/04/config-kerberos-in-cdh-hdfs/</link>
      <pubDate>Tue, 04 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/11/04/config-kerberos-in-cdh-hdfs/</guid>
      <description>本文主要记录 CDH Hadoop 集群上配置 HDFS 集成 Kerberos 的过程，包括 Kerberos 的安装和 Hadoop 相关配置修改说明。 1. 环境说明 系统环境： 操作系统：CentOs 6.6 Hadoop版本：C</description>
    </item>
    
    <item>
      <title>升级cdh4到cdh5</title>
      <link>http://javachen.github.io/2014/08/19/upgrading-from-cdh4-to-cdh5/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/08/19/upgrading-from-cdh4-to-cdh5/</guid>
      <description>本文主要记录从CDH4升级到CDH5的过程和遇到的问题，当然本文同样适用于CDH5低版本向最新版本的升级。 1. 不兼容的变化 升级前，需要注意 cdh5 有</description>
    </item>
    
    <item>
      <title>Flume-ng的原理和使用</title>
      <link>http://javachen.github.io/2014/07/22/flume-ng/</link>
      <pubDate>Tue, 22 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/07/22/flume-ng/</guid>
      <description>1. 介绍 Flume NG是Cloudera提供的一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中</description>
    </item>
    
    <item>
      <title>CDH中配置HDFS HA</title>
      <link>http://javachen.github.io/2014/07/18/install-hdfs-ha-in-cdh/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/07/18/install-hdfs-ha-in-cdh/</guid>
      <description>最近又安装 hadoop 集群， 故尝试了一下配置 HDFS 的 HA，CDH4支持Quorum-based Storage和shared storage using NFS两种HA方案，而CDH</description>
    </item>
    
    <item>
      <title>不用Cloudera Manager安装Cloudera Search</title>
      <link>http://javachen.github.io/2014/06/03/install_cloudera_search_without_cm/</link>
      <pubDate>Tue, 03 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/06/03/install_cloudera_search_without_cm/</guid>
      <description>Cloudera Search 用来在 hadoop 基础上建立索引和全文检索，本文主要记录如何安装 CLoudera Search 的过程，其中也包括如何安装和启动 Zookeeper、Solr、MapReduc</description>
    </item>
    
    <item>
      <title>Ubuntu系统编译Bigtop</title>
      <link>http://javachen.github.io/2014/04/17/building-bigtop-on-ubuntu/</link>
      <pubDate>Thu, 17 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2014/04/17/building-bigtop-on-ubuntu/</guid>
      <description>1. 安装系统依赖 系统更新并安装新的包 1 2 3 4 5 6 7 sudo apt-get update sudo apt-get install -y cmake git-core git-svn subversion checkinstall build-essential dh-make debhelper ant ant-optional autoconf automake liblzo2-dev libzip-dev sharutils libfuse-dev reprepro libtool libssl-dev asciidoc xmlto ssh curl sudo apt-get install -y devscripts sudo apt-get build-dep pkg-config 安装Sun JDK 6</description>
    </item>
    
    <item>
      <title>使用ZooKeeper实现配置同步</title>
      <link>http://javachen.github.io/2013/08/23/publish-proerties-using-zookeeper/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2013/08/23/publish-proerties-using-zookeeper/</guid>
      <description>前言 应用项目中都会有一些配置信息，这些配置信息数据量少，一般会保存到内存、文件或者数据库，有时候需要动态更新。当需要在多个应用服务器中修改这</description>
    </item>
    
    <item>
      <title>远程调试Hadoop各组件</title>
      <link>http://javachen.github.io/2013/08/01/remote-debug-hadoop/</link>
      <pubDate>Thu, 01 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2013/08/01/remote-debug-hadoop/</guid>
      <description>远程调试对应用程序开发十分有用。例如，为不能托管开发平台的低端机器开发程序，或在专用的机器上（比如服务不能中断的 Web 服务器）调试程序。其他情况</description>
    </item>
    
    <item>
      <title>安装RHadoop</title>
      <link>http://javachen.github.io/2013/07/20/install-rhadoop/</link>
      <pubDate>Sat, 20 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2013/07/20/install-rhadoop/</guid>
      <description>1. R Language Install 安装相关依赖 1 2 3 4 yum install -y perl* pcre-devel tcl-devel zlib-devel bzip2-devel libX11-devel tk-devel tetex-latex *gfortran* compat-readline5 yum install libRmath-* rpm -Uvh --force --nodeps R-core-2.10.0-2.el5.x86_64.rpm rpm -Uvh R-2.10.0-2.el5.x86_64.rpm R-devel-2.10.0-2.el5.x86_64.rpm 编译安装：R-3.0.1 1 2 3 4 5 tar -zxvf R-3.0.1 ./configure make make install #R运行 export HADOOP_CMD=/usr/bin/hadoop 排</description>
    </item>
    
    <item>
      <title>使用yum源安装CDH集群</title>
      <link>http://javachen.github.io/2013/04/06/install-cloudera-cdh-by-yum/</link>
      <pubDate>Sat, 06 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2013/04/06/install-cloudera-cdh-by-yum/</guid>
      <description>本文主要是记录使用yum安装CDH Hadoop集群的过程，包括HDFS、Yarn、Hive和HBase。目前使用的是CDH6.2.0安装集群</description>
    </item>
    
    <item>
      <title>手动安装Hadoop集群</title>
      <link>http://javachen.github.io/2013/03/24/manual-install-Cloudera-Hadoop-CDH/</link>
      <pubDate>Sun, 24 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>http://javachen.github.io/2013/03/24/manual-install-Cloudera-Hadoop-CDH/</guid>
      <description>安装版本 centos版本为6。 hadoop各个组件和jdk版本如下： 1 2 3 4 hadoop-2.0.0-cdh4.6.0 hbase-0.94.15-cdh4.6.0 hive-0.10.0-cdh4.6.0 jdk1.6.0_38 hadoop各组件可以在这里下载。 安装前说明 确定安装目录</description>
    </item>
    
  </channel>
</rss>