<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JavaChen Blog - Ramblings of a coder</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on JavaChen Blog - Ramblings of a coder</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 05 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Tue, 01 Sep 2009 00:00:00 +0800</pubDate>
      
      <guid>http://localhost:1313/about/</guid>
      <description>关于作者 网名 JavaChen，86后，湖北武汉人，目前在武汉光谷； 资深开发工程师，平时以Mac OSX为日常操作系统，主要关注Java、Had</description>
    </item>
    
    <item>
      <title>kubernetes使用acme.sh生成letsencrypt证书</title>
      <link>http://localhost:1313/2019/11/05/using-acme-sh-with-nginx-ingress/</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/11/05/using-acme-sh-with-nginx-ingress/</guid>
      <description>kubernetes中很多地方都要配置SSL证书，权威的证书要钱，免费的证书数量有限，而acme.sh 实现了 acme 协议，可以从 letsencrypt 生成免费的证书，</description>
    </item>
    
    <item>
      <title>Kubernetes SSL证书管理</title>
      <link>http://localhost:1313/2019/11/04/kubernetes-SSL-TLS/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/11/04/kubernetes-SSL-TLS/</guid>
      <description>HTTP over SSL 要保证Web浏览器到服务器的安全连接，HTTPS几乎是唯一选择。HTTPS其实就是HTTP over SSL，也就是让HTTP连接建立在SSL安</description>
    </item>
    
    <item>
      <title>使用Cert Manager配置Let’s Encrypt证书</title>
      <link>http://localhost:1313/2019/11/04/using-cert-manager-with-nginx-ingress/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/11/04/using-cert-manager-with-nginx-ingress/</guid>
      <description>在 Kubernetes 集群中，可以使用Cert-Manager创建 HTTPS 证书并自动续期，支持 Let’s Encrypt、CA、HashiCorp Vault 这些免费证书的签发</description>
    </item>
    
    <item>
      <title>使用Helm3安装Rancher HA集群</title>
      <link>http://localhost:1313/2019/11/03/install-rancher-ha-with-helm3/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/11/03/install-rancher-ha-with-helm3/</guid>
      <description>本文主要记录是由Helm来安装Rancher的过程。Rancher是一个企业级多集群Kubernetes管理平台；用户可以在Rancher上</description>
    </item>
    
    <item>
      <title>使用Helm3安装Gitea和Drone</title>
      <link>http://localhost:1313/2019/11/01/install-gitea-and-drone-with-helm3/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/11/01/install-gitea-and-drone-with-helm3/</guid>
      <description>Drone是一种基于容器技术的持续交付系统。Drone使用简单的YAML配置文件来定义和执行Docker容器中的Pipelines。Dron</description>
    </item>
    
    <item>
      <title>使用Helm3安装Harbor</title>
      <link>http://localhost:1313/2019/10/31/install-harbor-with-helm3/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/10/31/install-harbor-with-helm3/</guid>
      <description>Harbor是构建企业级私有docker镜像的仓库的开源解决方案，它是Docker Registry的更高级封装，它除了提供友好的Web UI界</description>
    </item>
    
    <item>
      <title>安装Helm</title>
      <link>http://localhost:1313/2019/10/31/install-helm/</link>
      <pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/10/31/install-helm/</guid>
      <description>Helm 是由 Deis 发起的一个开源工具，有助于简化部署和管理 Kubernetes 应用。本文主要是记录Helm 2的安装过程。 安装Helm2 安装 Helm 客户端 1 2 3 4 5 helm_version=v2.15.1 curl -s https://storage.googleapis.com/kubernetes-helm/helm-${helm_version}-linux-amd64.tar.gz \ |</description>
    </item>
    
    <item>
      <title>使用Kubeadm安装单节点kubernetes</title>
      <link>http://localhost:1313/2019/10/30/install-single-k8s-with-kubeadm/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/10/30/install-single-k8s-with-kubeadm/</guid>
      <description>本文使用Kubeadm来安装一个单节点的Kubernetes环境，以加深对Kubernetes各个组件和安装过程的理解。 环境准备 请参考 使用R</description>
    </item>
    
    <item>
      <title>使用RKE安装单节点kubernetes</title>
      <link>http://localhost:1313/2019/10/30/install-single-k8s-with-rke/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/10/30/install-single-k8s-with-rke/</guid>
      <description>Kubernetes 是Google的一种基于容器的开源服务编排解决方案。在我们进行Kubernetes的学习前，为了对Kubernetes的工作原理有一个大概</description>
    </item>
    
    <item>
      <title>Docker Compose安装Redis集群</title>
      <link>http://localhost:1313/2019/08/11/install-redis-cluster-with-docker-compose/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/08/11/install-redis-cluster-with-docker-compose/</guid>
      <description>拉取镜像： 1 docker pull redis 使用host网络进行搭建集群 1 2 3 4 5 6 7 8 9 10 11 12 13 14 docker run -d --name redis-node01 --net host \ -v /data/docker/redis/node01:/data redis --cluster-enabled yes \ --cluster-config-file node01.conf --port 6380 --appendonly yes \ --requirepass &amp;#34;123456&amp;#34; docker run -d --name redis-node02 --net host \ -v /data/docker/redis/node02:/data</description>
    </item>
    
    <item>
      <title>Tengine&#43;Lua&#43;GraphicsMagick动态裁剪图片</title>
      <link>http://localhost:1313/2019/08/10/tengine-lua-graphicsMagick-resize-picture/</link>
      <pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/08/10/tengine-lua-graphicsMagick-resize-picture/</guid>
      <description>软件列表 Tengine：https://github.com/alibaba/tengine Lua：http://www.lua.org/f</description>
    </item>
    
    <item>
      <title>Vagrant搭建Docker开发环境</title>
      <link>http://localhost:1313/2019/08/04/docker-with-vagrant/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/08/04/docker-with-vagrant/</guid>
      <description>安装Virtual Box Virtual Box 下载地址 https://www.virtualbox.org/wiki/Downloads 安装Vagrant Vagrant 是一款可以结合 Virtual Box 进行虚拟机安装、 管理的软件，基于 Ruby ，因为已经编译为应用程序，所以可以</description>
    </item>
    
    <item>
      <title>缓存和SpringCache介绍</title>
      <link>http://localhost:1313/2019/07/27/cache-and-spring-cache/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/27/cache-and-spring-cache/</guid>
      <description>1 缓存 1.1 什么是缓存？ 我们使用缓存时，我们的业务系统大概的调用流程如下图： 当我们查询一条数据时，先去查询缓存，如果缓存有就直接返回，如果没有就</description>
    </item>
    
    <item>
      <title>网络通信协议</title>
      <link>http://localhost:1313/2019/07/27/internet-protocal/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/27/internet-protocal/</guid>
      <description>1 网络通信 1.1 协议 TCP/IP UDP/IP Multcast 单播.每次只有两个实体相互通信，发送端和接收端都是唯一确定的 IP4中，0.0.0.0到223.255.255.255属</description>
    </item>
    
    <item>
      <title>Docker搭建hadoop和hive环境</title>
      <link>http://localhost:1313/2019/07/26/install-hadoop-and-hive-with-docker/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/26/install-hadoop-and-hive-with-docker/</guid>
      <description>文将介绍如何在docker上从零开始安装hadoop以及hive环境。本文不会介绍如何安装docker，也不会过多的介绍docker各个命令</description>
    </item>
    
    <item>
      <title>SpringBoot中Tomcat调优</title>
      <link>http://localhost:1313/2019/07/26/tomcat-tuning-in-spring-boot/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/26/tomcat-tuning-in-spring-boot/</guid>
      <description>Spring Boot中Tomcat调优 1 2 3 4 5 6 server:tomcat:accept-count:100#等待队列长度，默认为100max-co</description>
    </item>
    
    <item>
      <title>使用Docker安装MongoDB</title>
      <link>http://localhost:1313/2019/07/16/install-mongodb-with-docker/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/16/install-mongodb-with-docker/</guid>
      <description>使用Docker安装 下载镜像： 1 docker pull mongo 创建目录： 1 2 3 mkdir -p /data/docker/mongodb/data/ mkdir -p /data/docker/mongo/configdb 开启端口： 1 firewall-cmd --zone=public --add-port=27017/tcp --permanent 运行容器： 1 2 3 4 docker run -d -p 27017:27017 \ -v /data/docker/mongodb/data/:/data/db/ \ -v /data/docker/mongo/configdb:/data/configdb \ --auth --name=mongo mongo --a</description>
    </item>
    
    <item>
      <title>使用Docker安装MySql</title>
      <link>http://localhost:1313/2019/07/16/install-mysql-with-docker/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/16/install-mysql-with-docker/</guid>
      <description>使用Docker安装 下载镜像： 1 docker pull mysql 运行容器： 1 2 3 4 5 6 docker run -d -p 3306:3306 \ -v /data/docker/mysql/conf:/etc/mysql \ -v /data/docker/mysql/logs:/var/log/mysql \ -v /data/docker/mysql/data:/var/lib/mysql \ -e MYSQL_ROOT_PASSWORD=123456 \ --name mysql mysql 命令参数： -p 3306:3306：将容器</description>
    </item>
    
    <item>
      <title>安装阿波罗配置中心</title>
      <link>http://localhost:1313/2019/07/15/apollo/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/15/apollo/</guid>
      <description>安装 安装阿波罗 1、 在虚拟机安装git和mysql 1 yum install git mysql -y 2、克隆安装脚本 1 git clone https://github.com/nobodyiam/apollo-build-scripts.git 3、配置数据策略 在虚拟机访问mysql，然后初始化数据：</description>
    </item>
    
    <item>
      <title>Struts2</title>
      <link>http://localhost:1313/2019/07/01/struts2/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/07/01/struts2/</guid>
      <description>Struts2概述 ​Struts2 是一个基于 MVC 设计模式的 Web 应用框架，它本质上相当于一个 Servlet，在 MVC 设计模式中，Struts2 作为控制</description>
    </item>
    
    <item>
      <title>Java Server Faces</title>
      <link>http://localhost:1313/2019/06/29/jsf/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/06/29/jsf/</guid>
      <description>1、JSF介绍 JSF 的诞生 JSF的诞生还要追溯到2001年，在2001年5月，Sun制定了一个用户界面框架的规范JSR#127，而JSF 规范的1</description>
    </item>
    
    <item>
      <title>Java Web框架发展历程</title>
      <link>http://localhost:1313/2019/06/29/java-web-framework/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/06/29/java-web-framework/</guid>
      <description>Model1 ​在 Web 早期的开发中，通常采用的都是 Model1 模式，就是使用 JSP+JavaBean 技术，将页面显示和业务逻辑处理分开，由JSP页面来接收客户端请求，用JavaBean或</description>
    </item>
    
    <item>
      <title>LVS负载均衡</title>
      <link>http://localhost:1313/2019/06/29/lvs/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/06/29/lvs/</guid>
      <description>1、介绍 ​ 负载均衡(Load Balance)是一种服务器或网络设备的集群技术。负载均衡将特定的业务(网络服务、网络流量等)分担给多个服务器或</description>
    </item>
    
    <item>
      <title>Nginx服务器</title>
      <link>http://localhost:1313/2019/06/29/nginx/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/06/29/nginx/</guid>
      <description>1、常用Web服务器介绍 apache、Nginx、tomcat、weblogic、iis、jboss、websphere、jetty、net</description>
    </item>
    
    <item>
      <title>OSI参考模型</title>
      <link>http://localhost:1313/2019/06/29/osi/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/06/29/osi/</guid>
      <description>1、OSI参考模型 ## 1.1、OSI的来源 OSI（Open System Interconnect），即开放式系统互联。 一般都叫OSI参考模型，是ISO（国</description>
    </item>
    
    <item>
      <title>不使用Eureka创建Spring Cloud微服务</title>
      <link>http://localhost:1313/2019/04/19/spring-cloud-eureka-service-discovery-and-register-example/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/04/19/spring-cloud-eureka-service-discovery-and-register-example/</guid>
      <description>首先创建一个普通项目，分为生产者和消费者两部分，然后在逐步集成SpringCloud的相关组件。 1、创建基本应用 1.1、创建工程 创建spri</description>
    </item>
    
    <item>
      <title>Spring Cloud之Eureka配置示例</title>
      <link>http://localhost:1313/2019/04/15/spring-cloud-eureka-config-example/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/04/15/spring-cloud-eureka-config-example/</guid>
      <description>Eureka是Netflix开源的服务发现组件，本身是一个基于REST的服务，包含Server和Client两部分，Spring Cloud将</description>
    </item>
    
    <item>
      <title>Spring Boot整合Servlet、Filter、Listener</title>
      <link>http://localhost:1313/2019/04/07/spring-boot-with-servlet-filter-listener/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/04/07/spring-boot-with-servlet-filter-listener/</guid>
      <description>Spring Boot整合Servlet、Filter、Listener有两种方式：一是通过注解扫描完成；二是通过方法完成。 通过注解扫描完成 主要是用到了</description>
    </item>
    
    <item>
      <title>Spring Boot整合视图层</title>
      <link>http://localhost:1313/2019/04/07/spring-boot-with-web-view/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/04/07/spring-boot-with-web-view/</guid>
      <description>SpringBoot访问静态资源 1、从classpath/static目录访问静态资源，目录名称必须是static 2、从ServletCon</description>
    </item>
    
    <item>
      <title>Zabbix安装过程</title>
      <link>http://localhost:1313/2019/03/29/install-zabbix/</link>
      <pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/03/29/install-zabbix/</guid>
      <description>环境介绍： OS：rhel6.2 软件版本：zabbix-2.0.6 serverIP：192.168.56.10 clientIP：92.168.</description>
    </item>
    
    <item>
      <title>Cloudera Manager安装Haddop集群</title>
      <link>http://localhost:1313/2019/03/28/install-hadoop-cluster-with-cm6/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2019/03/28/install-hadoop-cluster-with-cm6/</guid>
      <description>在开始之前，请参考我博客中的关于如何安装cdh集群的文章，这里只做简单说明。因为只是为了测试，所以是在vagrant虚拟机中创建三个虚拟机搭</description>
    </item>
    
    <item>
      <title>MyBatis源码分析：SqlSession</title>
      <link>http://localhost:1313/2016/04/21/mybatis-sqlSession-source-code/</link>
      <pubDate>Thu, 21 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/04/21/mybatis-sqlSession-source-code/</guid>
      <description>从这段代码中可以看到MyBatis的初始化过程： 1 2 3 4 5 6 7 8 DataSource dataSource = ...; TransactionFactory transactionFactory = new JdbcTransactionFactory(); Environment environment = new Environment(&amp;#34;Production&amp;#34;, transactionFactory, dataSource); Configuration configuration = new Configuration(environment); configuration.setLazyLoadingEnabled(true); configuration.getTypeAliasRegistry().registerAlias(Blog.class); configuration.addMapper(BlogMapper.class); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); 在创建了Confi</description>
    </item>
    
    <item>
      <title>MyBatis源码分析：如何解析配置文件</title>
      <link>http://localhost:1313/2016/04/21/how-to-parse-mybatis-configuration/</link>
      <pubDate>Thu, 21 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/04/21/how-to-parse-mybatis-configuration/</guid>
      <description>MyBatis可以使用xml或者注解的方式进行配置，不管是哪种方式，最终会将获取到的配置参数设置到Configuration类中，例如，Sq</description>
    </item>
    
    <item>
      <title>UML类之间关系</title>
      <link>http://localhost:1313/2016/04/01/uml-class-realation/</link>
      <pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/04/01/uml-class-realation/</guid>
      <description>前面两篇文章讲到了使用PlantUML来画类图，要想准确地画出类与类之间的关系，必须理清类和类之间的关系。类的关系有泛化(Generaliz</description>
    </item>
    
    <item>
      <title>PlantUML安装和使用</title>
      <link>http://localhost:1313/2016/02/29/plantuml-install-and-usage/</link>
      <pubDate>Mon, 29 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/02/29/plantuml-install-and-usage/</guid>
      <description>什么是PlantUML PlantUML是一个快速创建UML图形的组件，PlantUML支持的图形有： sequence diagram, use case diagram, class diagram, activity diagram, component diagram, state diagram, object diagram, wireframe graphical interface Pl</description>
    </item>
    
    <item>
      <title>PlantUML类图</title>
      <link>http://localhost:1313/2016/02/29/plantuml-class-diagram/</link>
      <pubDate>Mon, 29 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/02/29/plantuml-class-diagram/</guid>
      <description>类之间的关系 PlantUML用下面的符号来表示类之间的关系： 泛化，Generalization：&amp;lt;|-- 关联，Association：</description>
    </item>
    
    <item>
      <title>MyBatis源码分析：Configuration</title>
      <link>http://localhost:1313/2016/02/26/source-code-analytic-of-mybatis-configuration/</link>
      <pubDate>Fri, 26 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/02/26/source-code-analytic-of-mybatis-configuration/</guid>
      <description>MyBatis依赖的jar不多，而且代码行数也没多少，其中使用了大量的设计模式，值得好好学习。下图是MyBatis的一张架构图，来自Java</description>
    </item>
    
    <item>
      <title>DevTools in Spring Boot</title>
      <link>http://localhost:1313/2016/02/22/devtools-in-spring-boot/</link>
      <pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/02/22/devtools-in-spring-boot/</guid>
      <description>本文主要了解Spring Boot 1.3.0新添加的spring-boot-devtools模块的使用，该模块主要是为了提高开发者开发Spring B</description>
    </item>
    
    <item>
      <title>Spring Boot Profile使用</title>
      <link>http://localhost:1313/2016/02/22/profile-usage-in-spring-boot/</link>
      <pubDate>Mon, 22 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/02/22/profile-usage-in-spring-boot/</guid>
      <description>Spring Boot使用@Profile注解可以实现不同环境下配置参数的切换，任何@Component或@Configuration注解的类都可以使用</description>
    </item>
    
    <item>
      <title>Scala Reading List</title>
      <link>http://localhost:1313/2016/01/23/scala-reading-list/</link>
      <pubDate>Sat, 23 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2016/01/23/scala-reading-list/</guid>
      <description>学习教程 为 Java程序员准备的Scala教程 Scala 初学指南：本书是 The Neophyte&amp;rsquo;s Guide to Scala 的中文翻译，是 Daniel Westheide 写的一系列有关 Scala 的文章。 http://ifeve.com/tag/scala/ Scala 指南：开始精彩的Sca</description>
    </item>
    
    <item>
      <title>Bash条件判断</title>
      <link>http://localhost:1313/2015/07/08/bash-if-else/</link>
      <pubDate>Wed, 08 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/07/08/bash-if-else/</guid>
      <description>每个完整并且合理的程序语言都具有条件判断的功能，并且可以根据条件测试的结果做下一步的处理。Bash有test命令、各种中括号和圆括号操作，和</description>
    </item>
    
    <item>
      <title>Bash中的变量</title>
      <link>http://localhost:1313/2015/07/07/bash-variable/</link>
      <pubDate>Tue, 07 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/07/07/bash-variable/</guid>
      <description>变量是脚本编程中进行数据表现的一种方法。说白了，变量不过是计算机为了保留数据项，而在内存中分配的一个位置或一组位置的标识或名字。变量既可以出</description>
    </item>
    
    <item>
      <title>Bash中的特殊字符</title>
      <link>http://localhost:1313/2015/07/06/bash-special-characters/</link>
      <pubDate>Mon, 06 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/07/06/bash-special-characters/</guid>
      <description>Bash中，用在脚本和其他地方的字符叫做特殊字符。下面依次举例介绍每个字符的用途。 # 行首以#(#!是个例外)开头是注释。 1 # This line is a comment. 注释也</description>
    </item>
    
    <item>
      <title>高级Bash脚本编程入门</title>
      <link>http://localhost:1313/2015/06/29/advanced-bash-script-programming/</link>
      <pubDate>Mon, 29 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/29/advanced-bash-script-programming/</guid>
      <description>最近在看《Advanced Bash Scripting Guide》这本书，第二章举了一个清除日志的例子，来讲述如何使用Bash进行编程并聊到了一些编程规范。本文主要</description>
    </item>
    
    <item>
      <title>spark-shell脚本分析</title>
      <link>http://localhost:1313/2015/06/26/spark-shell-command/</link>
      <pubDate>Fri, 26 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/26/spark-shell-command/</guid>
      <description>本文主要分析spark-shell脚本的运行逻辑，涉及到spark-submit、spark-class等脚本的分析，希望通过分析脚本以了解</description>
    </item>
    
    <item>
      <title>Scala中的对象</title>
      <link>http://localhost:1313/2015/06/19/scala-object/</link>
      <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/19/scala-object/</guid>
      <description>Scala中没有静态方法或静态字段，但可以使用object这个语法结构来实现相同的功能。对象与类在语法层面上很相似，除了不能提供构造器参数外</description>
    </item>
    
    <item>
      <title>Scala中的类</title>
      <link>http://localhost:1313/2015/06/19/scala-class/</link>
      <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/19/scala-class/</guid>
      <description>阅读《Programming in Scala》，整理Scala类、继承、重载相关的一些知识点。 类 Scala使用class来定义类。 1 2 3 4 5 class Counter</description>
    </item>
    
    <item>
      <title>使用Scala高价函数简化代码</title>
      <link>http://localhost:1313/2015/06/18/simplify-code-using-scala-higher-order-function/</link>
      <pubDate>Thu, 18 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/18/simplify-code-using-scala-higher-order-function/</guid>
      <description>在Scala里，带有其他函数做参数的函数叫做高阶函数，使用高阶函数可以简化代码。 减少重复代码 有这样一段代码，查找当前目录样以某一个字符串结尾</description>
    </item>
    
    <item>
      <title>推荐系统笔记</title>
      <link>http://localhost:1313/2015/06/15/note-about-recommendation-system/</link>
      <pubDate>Mon, 15 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/15/note-about-recommendation-system/</guid>
      <description>1、产生原因 信息过载 无明确需求 2、什么是推荐？ 在信息过载又没有明确需求的情况下，找到用户感兴趣的东西。 《Mahout实战》上的定义是：推荐就</description>
    </item>
    
    <item>
      <title>使用Mahout实现协同过滤</title>
      <link>http://localhost:1313/2015/06/10/collaborative-filtering-using-mahout/</link>
      <pubDate>Wed, 10 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/10/collaborative-filtering-using-mahout/</guid>
      <description>Mahout算法框架自带的推荐器有下面这些： GenericUserBasedRecommender：基于用户的推荐器，用户数量少时速度快； G</description>
    </item>
    
    <item>
      <title>Spark On YARN内存分配</title>
      <link>http://localhost:1313/2015/06/09/memory-in-spark-on-yarn/</link>
      <pubDate>Tue, 09 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/09/memory-in-spark-on-yarn/</guid>
      <description>本文主要了解Spark On YARN部署模式下的内存分配情况，因为没有深入研究Spark的源代码，所以只能根据日志去看相关的源代码，从而了解“为</description>
    </item>
    
    <item>
      <title>Spark配置参数</title>
      <link>http://localhost:1313/2015/06/07/spark-configuration/</link>
      <pubDate>Sun, 07 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/07/spark-configuration/</guid>
      <description>以下是整理的Spark中的一些配置参数，官方文档请参考Spark Configuration。 Spark提供三个位置用来配置系统： Spark属</description>
    </item>
    
    <item>
      <title>YARN的内存和CPU配置</title>
      <link>http://localhost:1313/2015/06/05/yarn-memory-and-cpu-configuration/</link>
      <pubDate>Fri, 05 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/05/yarn-memory-and-cpu-configuration/</guid>
      <description>Hadoop YARN同时支持内存和CPU两种资源的调度，本文介绍如何配置YARN对内存和CPU的使用。 YARN作为一个资源调度器，应该考虑到集群里面每</description>
    </item>
    
    <item>
      <title>如何使用Spark ALS实现协同过滤</title>
      <link>http://localhost:1313/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/06/01/how-to-implement-collaborative-filtering-using-spark-als/</guid>
      <description>本文主要记录最近一段时间学习和实现Spark MLlib中的协同过滤的一些总结，希望对大家熟悉Spark ALS算法有所帮助。 更新： 【2016.</description>
    </item>
    
    <item>
      <title>安装和配置Sentry</title>
      <link>http://localhost:1313/2015/04/30/install-and-config-sentry/</link>
      <pubDate>Thu, 30 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/30/install-and-config-sentry/</guid>
      <description>本文主要记录安装和配置Sentry的过程，关于Sentry的介绍，请参考Apache Sentry架构介绍。 1. 环境说明 系统环境： 操作系统：Ce</description>
    </item>
    
    <item>
      <title>测试Hive集成Sentry</title>
      <link>http://localhost:1313/2015/04/30/test-hive-with-sentry/</link>
      <pubDate>Thu, 30 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/30/test-hive-with-sentry/</guid>
      <description>本文在安装和配置Sentry基础之上测试Hive集成Sentry。注意：这里Hive中并没有配置Kerberos认证。 关于配置了Kerber</description>
    </item>
    
    <item>
      <title>Apache Sentry架构介绍</title>
      <link>http://localhost:1313/2015/04/29/apache-sentry-architecture/</link>
      <pubDate>Wed, 29 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/29/apache-sentry-architecture/</guid>
      <description>介绍 Apache Sentry是Cloudera公司发布的一个Hadoop开源组件，截止目前还是Apache的孵化项目，它提供了细粒度级、基于角色的授权</description>
    </item>
    
    <item>
      <title>编译CDH Spark源代码</title>
      <link>http://localhost:1313/2015/04/28/compile-cdh-spark-source-code/</link>
      <pubDate>Tue, 28 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/28/compile-cdh-spark-source-code/</guid>
      <description>本文以Cloudera维护的Spark分支项目为例，记录跟新Spark分支以及编译Spark源代码的过程。 下载代码 在Github上fork C</description>
    </item>
    
    <item>
      <title>Scala中下划线的用途</title>
      <link>http://localhost:1313/2015/04/23/all-the-uses-of-an-underscore-in-scala/</link>
      <pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/23/all-the-uses-of-an-underscore-in-scala/</guid>
      <description>存在性类型： 1 2 3 def foo(l: List[Option[_]]) = def f(m: M[_]) 高阶类型参数： 1 2 3 case class A[K[_],T](a: K[T]) def f[M[_]] 临时变量： 1 val _ = 5 临时参数： 1 List(1, 2, 3) foreach { _ =&amp;gt; println(&amp;#34;Hi&amp;#34;) } //List(1, 2, 3) foreach { t =&amp;gt; println(&amp;#34;Hi&amp;#34;) } 通配模式</description>
    </item>
    
    <item>
      <title>Scala集合</title>
      <link>http://localhost:1313/2015/04/22/scala-collections/</link>
      <pubDate>Wed, 22 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/22/scala-collections/</guid>
      <description>Scala有一个非常通用，丰富，强大，可组合的集合库；集合是高阶的(high level)并暴露了一大套操作方法。很多集合的处理和转换可以被表</description>
    </item>
    
    <item>
      <title>Scala基本语法和概念</title>
      <link>http://localhost:1313/2015/04/20/basic-of-scala/</link>
      <pubDate>Mon, 20 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/20/basic-of-scala/</guid>
      <description>本文主要包括Scala的安装过程并理解Scala的基本语法和概念，包括表达式、变量、基本类型、函数、流程控制等相关内容。 1. 安装 从All Versions Sc</description>
    </item>
    
    <item>
      <title>Spark MLlib中的协同过滤</title>
      <link>http://localhost:1313/2015/04/17/spark-mllib-collaborative-filtering/</link>
      <pubDate>Fri, 17 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/17/spark-mllib-collaborative-filtering/</guid>
      <description>本文主要通过Spark官方的例子理解ALS协同过滤算法的原理和编码过程，然后通过对电影进行推荐来熟悉一个完整的推荐过程。 协同过滤 协同过滤常被</description>
    </item>
    
    <item>
      <title>Spark SQL中的数据源</title>
      <link>http://localhost:1313/2015/04/03/spark-sql-datasource/</link>
      <pubDate>Fri, 03 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/04/03/spark-sql-datasource/</guid>
      <description>Spark 支持通过 DataFrame 来操作大量的数据源，包括外部文件（如 json、avro、parquet、sequencefile 等等）、hive、关系数据库、c</description>
    </item>
    
    <item>
      <title>Reading List 2015-03</title>
      <link>http://localhost:1313/2015/03/30/reading-list-2015-03/</link>
      <pubDate>Mon, 30 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/30/reading-list-2015-03/</guid>
      <description>这个月主要在关注流式处理和推荐系统方面的技术。如何从零构建一个推荐系统？网上能找到的有指导意义的资料太少，只能一点点摸索？ Spark LeanCloud 离线数据分析功</description>
    </item>
    
    <item>
      <title>Spark本地模式运行</title>
      <link>http://localhost:1313/2015/03/30/spark-test-in-local-mode/</link>
      <pubDate>Mon, 30 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/30/spark-test-in-local-mode/</guid>
      <description>Spark的安装分为几种模式，其中一种是本地运行模式，只需要在单节点上解压即可运行，这种模式不需要依赖Hadoop 环境。在本地运行模式中，m</description>
    </item>
    
    <item>
      <title>Spark SQL中的DataFrame</title>
      <link>http://localhost:1313/2015/03/26/spark-sql-dataframe/</link>
      <pubDate>Thu, 26 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/26/spark-sql-dataframe/</guid>
      <description>在2014年7月1日的 Spark Summit 上，Databricks 宣布终止对 Shark 的开发，将重点放到 Spark SQL 上。在会议上，Databricks 表示，Shark 更多是</description>
    </item>
    
    <item>
      <title>将Avro数据转换为Parquet格式</title>
      <link>http://localhost:1313/2015/03/25/converting-avro-data-to-parquet-format/</link>
      <pubDate>Wed, 25 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/25/converting-avro-data-to-parquet-format/</guid>
      <description>本文主要测试将Avro数据转换为Parquet格式的过程并查看 Parquet 文件的 schema 和元数据。 准备 将文本数据转换为 Parquet 格式并读取内容，可以参考 Cloudera 的 MapReduce 例子：</description>
    </item>
    
    <item>
      <title>如何将Avro数据加载到Spark</title>
      <link>http://localhost:1313/2015/03/24/how-to-load-some-avro-data-into-spark/</link>
      <pubDate>Tue, 24 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/24/how-to-load-some-avro-data-into-spark/</guid>
      <description>这是一篇翻译，原文来自：How to load some Avro data into Spark。 首先，为什么使用 Avro ？ 最基本的格式是 CSV ，其廉价并且不需要顶一个一个 schema 和数据关联。 随后流行</description>
    </item>
    
    <item>
      <title>Avro介绍</title>
      <link>http://localhost:1313/2015/03/20/about-avro/</link>
      <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/20/about-avro/</guid>
      <description>1. 介绍 Avro 是 Hadoop 中的一个子项目，也是 Apache 中一个独立的项目，Avro 是一个基于二进制数据传输高性能的中间件。在 Hadoop 的其他项目中，例如 HBase 和 Hive 的 Client 端与服务</description>
    </item>
    
    <item>
      <title>安装和测试Kafka</title>
      <link>http://localhost:1313/2015/03/17/install-and-test-kafka/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/17/install-and-test-kafka/</guid>
      <description>本文主要介绍如何在单节点上安装 Kafka 并测试 broker、producer 和 consumer 功能。 下载 进入下载页面：http://kafka.apache.or</description>
    </item>
    
    <item>
      <title>Spring Boot之一：如何运行项目</title>
      <link>http://localhost:1313/2015/03/13/how-to-run-spring-boot-application/</link>
      <pubDate>Fri, 13 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/13/how-to-run-spring-boot-application/</guid>
      <description>介绍 Spring Boot 是 Spring 产品中一个新的子项目，致力于简便快捷地搭建基于 Spring 的独立可运行的应用。大多数的 Spring Boot 应用只需要非常少的 Spring 配置。 你能够使用 Spring Boot 创建 Java 应</description>
    </item>
    
    <item>
      <title>Spring Boot之二：特性</title>
      <link>http://localhost:1313/2015/03/13/some-spring-boot-features/</link>
      <pubDate>Fri, 13 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/13/some-spring-boot-features/</guid>
      <description>1. SpringApplication SpringApplication 类是启动 Spring Boot 应用的入口类，你可以创建一个包含 main() 方法的类，来运行 SpringApplication.run 这个静态方法： 1 2 3 public static void main(String[] args) { SpringApplication.run(MySpringConfiguration.class, args); } 运行该类会有如下输出： 1 2 3 4 5</description>
    </item>
    
    <item>
      <title>Spring AOP Example Tutorial</title>
      <link>http://localhost:1313/2015/03/11/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration/</link>
      <pubDate>Wed, 11 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/11/spring-aop-example-tutorial-aspect-advice-pointcut-joinpoint-annotations-xml-configuration/</guid>
      <description>这是一篇翻译，原文：Spring AOP Example Tutorial – Aspect, Advice, Pointcut, JoinPoint, Annotations, XML Configuration Spring 框架发展出了两个核心概念：依赖注入 和面向切面编程（AOP）。我们已经了解了 Spring 的依赖注</description>
    </item>
    
    <item>
      <title>快速了解RESTEasy</title>
      <link>http://localhost:1313/2015/03/10/quick-start-of-resteasy/</link>
      <pubDate>Tue, 10 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/10/quick-start-of-resteasy/</guid>
      <description>什么是 RESTEasy RESTEasy 是 JBoss 的一个开源项目，提供各种框架帮助你构建 RESTful Web Services 和 RESTful Java 应用程序。它是 JAX-RS 规范的一个完整实现并通过 JCP 认证。作为一个 JBOSS 的项目，它当然能和</description>
    </item>
    
    <item>
      <title>Java笔记：异常</title>
      <link>http://localhost:1313/2015/03/04/note-about-java-exception/</link>
      <pubDate>Wed, 04 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/03/04/note-about-java-exception/</guid>
      <description>定义 在《java编程思想》中这样定义异常：阻止当前方法或作用域继续执行的问题。异常是Java程序设计中不可分割的一部分，如果不了解如何使用它</description>
    </item>
    
    <item>
      <title>安装和配置Hue</title>
      <link>http://localhost:1313/2015/02/28/install-and-config-hue/</link>
      <pubDate>Sat, 28 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/28/install-and-config-hue/</guid>
      <description>本文主要记录使用 yum 源安装 Hue 以及配置 Hue 集成 Hdfs、Hive、Impala、Yarn、Kerberos、LDAP、Sentry、Solr 等的过</description>
    </item>
    
    <item>
      <title>Hadoop Streaming 原理</title>
      <link>http://localhost:1313/2015/02/12/hadoop-streaming/</link>
      <pubDate>Thu, 12 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/12/hadoop-streaming/</guid>
      <description>简介 Hadoop Streaming 是 Hadoop 提供的一个 MapReduce 编程工具，它允许用户使用任何可执行文件、脚本语言或其他编程语言来实现 Mapper 和 Reducer，从而充分利用 Hadoop 并行计算框架的</description>
    </item>
    
    <item>
      <title>Reading List 2015-02</title>
      <link>http://localhost:1313/2015/02/10/reading-list-2015-02/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/10/reading-list-2015-02/</guid>
      <description>一直有个想法没有付诸实践，想做个分享知识的网站，类似 Leanote、开发者头条、GitHunt 等等的可检索的有思想的一个产品。作为尝试，在想</description>
    </item>
    
    <item>
      <title>Useful Hadoop Commands</title>
      <link>http://localhost:1313/2015/02/10/useful-commands-in-hadoop/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/10/useful-commands-in-hadoop/</guid>
      <description>hadoop 解压 gz 文件到文本文件 1 $ hadoop fs -text /hdfs_path/compressed_file.gz | hadoop fs -put - /tmp/uncompressed-file.txt 解压本地文件 gz 文件并上传到 hdfs 1 $ gunzip -c filename.txt.gz | hadoop fs -put - /tmp/filename.txt 使用 awk 处理 csv 文件，参考 Using awk and friends with Hadoop: 1 $ hadoop fs -cat</description>
    </item>
    
    <item>
      <title>如何在CDH5上运行Spark应用</title>
      <link>http://localhost:1313/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/</link>
      <pubDate>Wed, 04 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/</guid>
      <description>这篇文章参考 How-to: Run a Simple Apache Spark App in CDH 5 编写而成，没有完全参照原文翻译，而是重新进行了整理，例如：spark 版本改为 1.3.0，添加了 Python 版的程序。 创</description>
    </item>
    
    <item>
      <title>Spark编程指南笔记</title>
      <link>http://localhost:1313/2015/02/03/spark-programming-guide/</link>
      <pubDate>Tue, 03 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/03/spark-programming-guide/</guid>
      <description>本文是参考Spark官方编程指南（Spark 版本为1.2）整理出来的学习笔记，主要是用于加深对 Spark 的理解，并记录一些知识点。 1. Spark介绍 S</description>
    </item>
    
    <item>
      <title>Require.JS快速入门</title>
      <link>http://localhost:1313/2015/02/02/quick-start-of-requirejs/</link>
      <pubDate>Mon, 02 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/02/quick-start-of-requirejs/</guid>
      <description>Require.JS 介绍 Require.JS 是一个基于 AMD 规范的 JavaScript 模块加载框架。实现 JavaScript 文件的异步加载，管理模块之间的依赖性，提升网页的加载速度。 AMD 是 Asynchronous Module Definition 的缩写，意思就是 异步模块</description>
    </item>
    
    <item>
      <title>用Yeoman构建AngularJS项目</title>
      <link>http://localhost:1313/2015/02/02/build-angularjs-app-with-yeoman/</link>
      <pubDate>Mon, 02 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/02/02/build-angularjs-app-with-yeoman/</guid>
      <description>这篇文章不是一篇翻译也不是一篇原创文章，类似于一篇学习笔记，主要是记录一些关键的过程，方便查阅加深理解和记忆。 Yeoman 介绍 Yeoman 是 Google 的团队和外部贡献者</description>
    </item>
    
    <item>
      <title>Django中SQL查询</title>
      <link>http://localhost:1313/2015/01/30/raw-sql-query-in-django/</link>
      <pubDate>Fri, 30 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/30/raw-sql-query-in-django/</guid>
      <description>当 Django 中模型提供的查询 API 不能满足要求时，你可能需要使用原始的 sql 查询，这时候就需要用到 Manager.raw() 方法。 Manager 类提供下面的一个方法，可以用于执行 sql： 1 Manager.raw(raw_query, params=None,</description>
    </item>
    
    <item>
      <title>安装和部署Presto</title>
      <link>http://localhost:1313/2015/01/26/install-and-deploy-presto/</link>
      <pubDate>Mon, 26 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/26/install-and-deploy-presto/</guid>
      <description>1. 安装环境 操作系统：CentOs6.5 Hadoop 集群：CDH5.3 JDK 版本：jdk1.8.0_31 为了测试简单，我是将 Presto 的 coordinator 和 worker 都部署在 cdh1 节点上，并且</description>
    </item>
    
    <item>
      <title>CDH 5.2中Impala认证集成LDAP和Kerberos</title>
      <link>http://localhost:1313/2015/01/23/new-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos/</link>
      <pubDate>Fri, 23 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/23/new-in-cdh-5-2-impala-authentication-with-ldap-and-kerberos/</guid>
      <description>这是一篇翻译的文章，原文为 New in CDH 5.2: Impala Authentication with LDAP and Kerberos。由于翻译水平有限，难免会一些翻译不准确的地方，欢迎指正！ Impala 认证现在可以通过 LDAP 和</description>
    </item>
    
    <item>
      <title>Presto介绍</title>
      <link>http://localhost:1313/2015/01/23/presto-overview/</link>
      <pubDate>Fri, 23 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/23/presto-overview/</guid>
      <description>1. 简介 Presto 是一个运行在集群之上的分布式系统。一个完全的安装报考一个 coordinator 进程和多个 workers 进程。查询通过一个客户端例如 Presto CLI 提交到 coordinator 进程。这个 coordinator 进程解析、</description>
    </item>
    
    <item>
      <title>Maven的一些技巧</title>
      <link>http://localhost:1313/2015/01/20/maven-skills/</link>
      <pubDate>Tue, 20 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/20/maven-skills/</guid>
      <description>本文主要收集一些 Maven 的使用技巧，包括 Maven 常见命令、创建多模块项目、上传本地 jar 到插件以及常用的插件等等，本篇文章会保持不停的更新。 创建 maven 项目 1 $ mvn</description>
    </item>
    
    <item>
      <title>Django中的ORM</title>
      <link>http://localhost:1313/2015/01/15/django-orm/</link>
      <pubDate>Thu, 15 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/15/django-orm/</guid>
      <description>通过《如何创建一个Django网站》大概清楚了如何创建一个简单的 Django 网站，并了解了Django 中模板和模型使用方法。本篇文章主要在此基础上，了</description>
    </item>
    
    <item>
      <title>Django中的模型</title>
      <link>http://localhost:1313/2015/01/14/django-model/</link>
      <pubDate>Wed, 14 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/14/django-model/</guid>
      <description>Django 中的模型主要用于定义数据的来源信息，其包括一些必要的字段和一些对存储的数据的操作。通常，一个模型对应着数据库中的一个表。 简单的概念： Django 中每</description>
    </item>
    
    <item>
      <title>AngularJS PhoneCat代码分析</title>
      <link>http://localhost:1313/2015/01/09/angular-phonecat-examples/</link>
      <pubDate>Fri, 09 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/09/angular-phonecat-examples/</guid>
      <description>AngularJS 官方网站提供了一个用于学习的示例项目：PhoneCat。这是一个Web应用，用户可以浏览一些Android手机，了解它们的详细信息，并进行</description>
    </item>
    
    <item>
      <title>Gradle构建多模块项目</title>
      <link>http://localhost:1313/2015/01/07/build-multi-module-project-with-gradle/</link>
      <pubDate>Wed, 07 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/07/build-multi-module-project-with-gradle/</guid>
      <description>废话不多说，直接进入主题。 1. 创建项目 首先创建项目，名称为 test： 1 2 mkdir test &amp;amp;&amp;amp; cd test gradle init 这时候的项目结构如下： 1 2 3 4 5 6 7 8 9 10 11 12 ➜ test tree . ├</description>
    </item>
    
    <item>
      <title>使用Spring Boot和Gradle创建AngularJS项目</title>
      <link>http://localhost:1313/2015/01/06/build-app-with-spring-boot-and-gradle/</link>
      <pubDate>Tue, 06 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2015/01/06/build-app-with-spring-boot-and-gradle/</guid>
      <description>Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化新 Spring 应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定</description>
    </item>
    
    <item>
      <title>Hadoop集群部署权限总结</title>
      <link>http://localhost:1313/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop/</link>
      <pubDate>Tue, 25 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/25/quikstart-for-config-kerberos-ldap-and-sentry-in-hadoop/</guid>
      <description>这是一篇总结的文章，主要介绍 Hadoop 集群快速部署权限的步骤以及一些注意事项。如果你想了解详细的过程，请参考本博客中其他的文章。 1. 开始之前 hadoop 集群一共</description>
    </item>
    
    <item>
      <title>Spring集成JPA2.0</title>
      <link>http://localhost:1313/2014/11/24/spring-with-jpa2/</link>
      <pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/24/spring-with-jpa2/</guid>
      <description>JPA 全称 Java Persistence API，是Java EE 5标准之一，是一个 ORM 规范，由厂商来实现该规范，目前有 Hibernate、OpenJPA、TopLink、Ecl</description>
    </item>
    
    <item>
      <title>Zookeeper配置Kerberos认证</title>
      <link>http://localhost:1313/2014/11/18/config-kerberos-in-cdh-zookeeper/</link>
      <pubDate>Tue, 18 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/18/config-kerberos-in-cdh-zookeeper/</guid>
      <description>参考 使用yum安装CDH Hadoop集群 安装 hadoop 集群，集群包括三个节点，每个节点的ip、主机名和部署的组件分配如下： 1 2 3 192.168.56.121 cdh1 NameNode</description>
    </item>
    
    <item>
      <title>配置安全的Hive集群集成Sentry</title>
      <link>http://localhost:1313/2014/11/14/config-secured-hive-with-sentry/</link>
      <pubDate>Fri, 14 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/14/config-secured-hive-with-sentry/</guid>
      <description>本文主要记录配置安全的Hive集群集成Sentry的过程。Hive上配置了Kerberos认证，配置的过程请参考： 使用yum安装CDH Had</description>
    </item>
    
    <item>
      <title>配置安全的Impala集群集成Sentry</title>
      <link>http://localhost:1313/2014/11/14/config-secured-impala-with-sentry/</link>
      <pubDate>Fri, 14 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/14/config-secured-impala-with-sentry/</guid>
      <description>本文主要记录配置安全的Impala集群集成Sentry的过程。Impala集群上配置了Kerberos认证，并且需要提前配置好Hive与Ke</description>
    </item>
    
    <item>
      <title>Hadoop配置LDAP集成Kerberos</title>
      <link>http://localhost:1313/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop/</link>
      <pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/12/config-ldap-with-kerberos-in-cdh-hadoop/</guid>
      <description>本文主要记录 cdh hadoop 集群集成 ldap 的过程，这里 ldap 安装的是 OpenLDAP 。LDAP 用来做账号管理，Kerberos作为认证。授权一般来说是由应用来决定的，通过在 LDAP</description>
    </item>
    
    <item>
      <title>Hive配置Kerberos认证</title>
      <link>http://localhost:1313/2014/11/06/config-kerberos-in-cdh-hive/</link>
      <pubDate>Thu, 06 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/06/config-kerberos-in-cdh-hive/</guid>
      <description>1. 环境说明 系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为</description>
    </item>
    
    <item>
      <title>Impala配置Kerberos认证</title>
      <link>http://localhost:1313/2014/11/06/config-kerberos-in-cdh-impala/</link>
      <pubDate>Thu, 06 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/06/config-kerberos-in-cdh-impala/</guid>
      <description>1. 环境说明 系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.4 JDK版本：1.7.0_71 运行用户：root 集群各节点角色规划为</description>
    </item>
    
    <item>
      <title>YARN配置Kerberos认证</title>
      <link>http://localhost:1313/2014/11/05/config-kerberos-in-cdh-yarn/</link>
      <pubDate>Wed, 05 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/05/config-kerberos-in-cdh-yarn/</guid>
      <description>关于 Kerberos 的安装和 HDFS 配置 kerberos 认证，请参考 HDFS配置kerberos认证。 1. 环境说明 系统环境： 操作系统：CentOs 6.6 Hadoop版本：CDH5.</description>
    </item>
    
    <item>
      <title>HDFS配置Kerberos认证</title>
      <link>http://localhost:1313/2014/11/04/config-kerberos-in-cdh-hdfs/</link>
      <pubDate>Tue, 04 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/11/04/config-kerberos-in-cdh-hdfs/</guid>
      <description>本文主要记录 CDH Hadoop 集群上配置 HDFS 集成 Kerberos 的过程，包括 Kerberos 的安装和 Hadoop 相关配置修改说明。 1. 环境说明 系统环境： 操作系统：CentOs 6.6 Hadoop版本：C</description>
    </item>
    
    <item>
      <title>Mac上使用homebrew安装PostgreSql</title>
      <link>http://localhost:1313/2014/10/30/install-postgresql-on-mac-using-homebrew/</link>
      <pubDate>Thu, 30 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/10/30/install-postgresql-on-mac-using-homebrew/</guid>
      <description>安装 brew 安装 postgresql ： 1 $ brew install postgresql 查看安装的版本： 1 2 $ pg_ctl -V pg_ctl (PostgreSQL) 9.3.5 安装成功之后，安装路径为：/usr/local/var/postgres 接下来，初始</description>
    </item>
    
    <item>
      <title>Impala查询功能测试</title>
      <link>http://localhost:1313/2014/10/24/impala-query-table-tutorial/</link>
      <pubDate>Fri, 24 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/10/24/impala-query-table-tutorial/</guid>
      <description>关于 Impala 使用方法的一些测试，包括加载数据、查看数据库、聚合关联查询、子查询等等。 1. 准备测试数据 以下测试以 impala 用户来运行： 1 2 3 4 5 6 7 8 9 10 $ su</description>
    </item>
    
    <item>
      <title>当前数据仓库建设过程</title>
      <link>http://localhost:1313/2014/10/23/hive-warehouse-in-2014/</link>
      <pubDate>Thu, 23 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/10/23/hive-warehouse-in-2014/</guid>
      <description>一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。 1. 数据采集</description>
    </item>
    
    <item>
      <title>Spring源码整体架构</title>
      <link>http://localhost:1313/2014/09/29/spring-source-codes/</link>
      <pubDate>Mon, 29 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/29/spring-source-codes/</guid>
      <description>前言 Spring 是一个开源框架，是为了解决企业应用程序开发复杂性而创建的。框架的主要优势之一就是其分层架构，分层架构允许您选择使用哪一个组件，同时为 J2EE</description>
    </item>
    
    <item>
      <title>编译Dubbo源码并测试</title>
      <link>http://localhost:1313/2014/09/24/compile-and-test-dubbo/</link>
      <pubDate>Wed, 24 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/24/compile-and-test-dubbo/</guid>
      <description>Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，每天为2000+ 个服务提供3,000,000,000+ 次访问量支持，并被广泛应用于</description>
    </item>
    
    <item>
      <title>Mahout推荐引擎介绍</title>
      <link>http://localhost:1313/2014/09/22/mahout-recommend-engine/</link>
      <pubDate>Mon, 22 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/22/mahout-recommend-engine/</guid>
      <description>Mahout 是一个来自 Apache 的、开源的机器学习软件库，他主要关注于推荐引擎（协同过滤）、聚类和分类。 推荐一般是基于物品或者用户进行推荐相关。 聚类是讲大量的</description>
    </item>
    
    <item>
      <title>使用Gradle构建项目</title>
      <link>http://localhost:1313/2014/09/15/build-project-with-gradle/</link>
      <pubDate>Mon, 15 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/15/build-project-with-gradle/</guid>
      <description>Gradle 是一款基于 Groovy 语言、免费开源的构建工具，它既保持了 Maven 的优点，又通过使用 Groovy 定义的 DSL 克服了 Maven 中使用 XML 繁冗以及不灵活的缺点。 Gradle 官方网站：http:</description>
    </item>
    
    <item>
      <title>使用Groovy操作文件</title>
      <link>http://localhost:1313/2014/09/12/file-operation-in-groovy/</link>
      <pubDate>Fri, 12 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/12/file-operation-in-groovy/</guid>
      <description>Java 读写文件比较麻烦，那 Groovy 操作文件又如何呢？ 1. 读文件 读文件内容 在groovy中输出文件的内容： 1 println new File(&amp;#34;tmp.csv&amp;#34;).text 上面代码非常简单，没有流的出现，没有资源</description>
    </item>
    
    <item>
      <title>Llama的使用</title>
      <link>http://localhost:1313/2014/09/09/llama/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/09/llama/</guid>
      <description>1. 介绍 Llama (Low Latency Application MAster) 是一个 Yarn 的 Application Master，用于协调 Impala 和 Yarn 之间的集群资源的管理和监控。Llama 使 Impala 能够获取、使用和释放资源配额，而不需要 Impala 使</description>
    </item>
    
    <item>
      <title>从零开始创建Grails应用</title>
      <link>http://localhost:1313/2014/09/09/create-a-grails-app-step-by-step/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/09/create-a-grails-app-step-by-step/</guid>
      <description>本篇文章主要介绍如何从零开始一步一步创建一个 Grails 应用程序。整个过程中，你将学到如何改变 Grails 运行的端口，了解 Grails 应用的基础组成部分(领域类、控制器和</description>
    </item>
    
    <item>
      <title>Groovy语法介绍</title>
      <link>http://localhost:1313/2014/09/05/about-groovy/</link>
      <pubDate>Fri, 05 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/09/05/about-groovy/</guid>
      <description>1. 介绍 Groovy 是基于 JRE 的脚本语言，和Perl，Python等等脚本语言一样，它能以快速简洁的方式来完成一些工作：如访问数据库，编写单元测试用例，快</description>
    </item>
    
    <item>
      <title>安装Azkaban</title>
      <link>http://localhost:1313/2014/08/25/install-azkaban/</link>
      <pubDate>Mon, 25 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/08/25/install-azkaban/</guid>
      <description>Azkaban 是由 Linkedin 开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban 定义了一种 KV 文件格式来建立任务之</description>
    </item>
    
    <item>
      <title>升级cdh4到cdh5</title>
      <link>http://localhost:1313/2014/08/19/upgrading-from-cdh4-to-cdh5/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/08/19/upgrading-from-cdh4-to-cdh5/</guid>
      <description>本文主要记录从CDH4升级到CDH5的过程和遇到的问题，当然本文同样适用于CDH5低版本向最新版本的升级。 1. 不兼容的变化 升级前，需要注意 cdh5 有</description>
    </item>
    
    <item>
      <title>Sqoop导入关系数据库到Hive</title>
      <link>http://localhost:1313/2014/08/04/import-data-to-hive-with-sqoop/</link>
      <pubDate>Mon, 04 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/08/04/import-data-to-hive-with-sqoop/</guid>
      <description>Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。 1. 安装 Sqoop 使用 rpm 安装即可。 1 yum install sqoop sqoop-metastore -y 安装完之后需要</description>
    </item>
    
    <item>
      <title>2014年7月总结</title>
      <link>http://localhost:1313/2014/07/31/summary-of-july-in-2014/</link>
      <pubDate>Thu, 31 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/07/31/summary-of-july-in-2014/</guid>
      <description>在休息了将近三个月之后，7月9日终于开始上班了，新的工作还是和 hadoop 相关。7月主要的工作内容如下： 搭建新的 hadoop 集群，hadoop 版本为 CDH4.7</description>
    </item>
    
    <item>
      <title>Phoenix Quick Start</title>
      <link>http://localhost:1313/2014/07/28/phoenix-quick-start/</link>
      <pubDate>Mon, 28 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/07/28/phoenix-quick-start/</guid>
      <description>1. 介绍 Phoenix 是 Salesforce.com 开源的一个 Java 中间件，可以让开发者在Apache HBase 上执行 SQL 查询。Phoenix完全使用Java编写，代码位于 GitHub 上，并且提供了一个客</description>
    </item>
    
    <item>
      <title>采集日志到Hive</title>
      <link>http://localhost:1313/2014/07/25/collect-log-to-hive/</link>
      <pubDate>Fri, 25 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/07/25/collect-log-to-hive/</guid>
      <description>我们现在的需求是需要将线上的日志以小时为单位采集并存储到 hive 数据库中，方便以后使用 mapreduce 或者 impala 做数据分析。为了实现这个目标调研了 flume 如何采集数据到 h</description>
    </item>
    
    <item>
      <title>Flume-ng的原理和使用</title>
      <link>http://localhost:1313/2014/07/22/flume-ng/</link>
      <pubDate>Tue, 22 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/07/22/flume-ng/</guid>
      <description>1. 介绍 Flume NG是Cloudera提供的一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中</description>
    </item>
    
    <item>
      <title>CDH中配置HDFS HA</title>
      <link>http://localhost:1313/2014/07/18/install-hdfs-ha-in-cdh/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/07/18/install-hdfs-ha-in-cdh/</guid>
      <description>最近又安装 hadoop 集群， 故尝试了一下配置 HDFS 的 HA，CDH4支持Quorum-based Storage和shared storage using NFS两种HA方案，而CDH</description>
    </item>
    
    <item>
      <title>Spark集群安装和使用</title>
      <link>http://localhost:1313/2014/07/01/spark-install-and-usage/</link>
      <pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/07/01/spark-install-and-usage/</guid>
      <description>本文主要记录 CDH5 集群中 Spark 集群模式的安装过程配置过程并测试 Spark 的一些基本使用方法。 安装环境如下： 操作系统：CentOs 6.5 Hadoop 版本：cdh-5.4.0</description>
    </item>
    
    <item>
      <title>HBase中的一些注意事项</title>
      <link>http://localhost:1313/2014/06/26/some-tips-about-hbase/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/26/some-tips-about-hbase/</guid>
      <description>1. 安装集群前 配置SSH无密码登陆 DNS。HBase使用本地 hostname 才获得IP地址，正反向的DNS都是可以的。你还可以设置 hbase.regionserver.dns.interface 来指定主接口，设置 hbase.regionserver.dns.nameserver 来指</description>
    </item>
    
    <item>
      <title>HBase和Cassandra比较</title>
      <link>http://localhost:1313/2014/06/24/hbase-vs-cassandra/</link>
      <pubDate>Tue, 24 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/24/hbase-vs-cassandra/</guid>
      <description>HBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Googl</description>
    </item>
    
    <item>
      <title>MapReduce任务参数调优</title>
      <link>http://localhost:1313/2014/06/24/tuning-in-mapreduce/</link>
      <pubDate>Tue, 24 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/24/tuning-in-mapreduce/</guid>
      <description>本文主要记录Hadoop 2.x版本中MapReduce参数调优，不涉及Yarn的调优。 Hadoop的默认配置文件（以cdh5.0.1为例）：</description>
    </item>
    
    <item>
      <title>MapReduce任务运行过程</title>
      <link>http://localhost:1313/2014/06/24/the-running-process-of-mapreduce-job/</link>
      <pubDate>Tue, 24 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/24/the-running-process-of-mapreduce-job/</guid>
      <description>下图是MapReduce任务运行过程的一个图： Map-Reduce的处理过程主要涉及以下四个部分： 客户端Client：用于提交Map-red</description>
    </item>
    
    <item>
      <title>Hive中的排序语法</title>
      <link>http://localhost:1313/2014/06/22/sort-in-hive-query/</link>
      <pubDate>Sun, 22 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/22/sort-in-hive-query/</guid>
      <description>ORDER BY hive中的ORDER BY语句和关系数据库中的sql语法相似。他会对查询结果做全局排序，这意味着所有的数据会传送到一个Reduce任务上</description>
    </item>
    
    <item>
      <title>Lucene介绍</title>
      <link>http://localhost:1313/2014/06/21/the-introduction-of-lucene/</link>
      <pubDate>Sat, 21 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/21/the-introduction-of-lucene/</guid>
      <description>1. Lucene是什么 Lucene 是一个开源的、成熟的全文索引与信息检索(IR)库，采用Java实现。信息检索式指文档搜索、文档内信息搜索或者文档相关的</description>
    </item>
    
    <item>
      <title>Storm集群安装部署步骤</title>
      <link>http://localhost:1313/2014/06/19/how-to-install-and-deploy-a-storm-cluster/</link>
      <pubDate>Thu, 19 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/19/how-to-install-and-deploy-a-storm-cluster/</guid>
      <description>开始学习Storm，本文主要记录Storm集群安装部署步骤，不包括对Storm的介绍。 安装storm集群，需要依赖以下组件： Zookeeper Python Zeromq Storm JDK JZMQ 故安</description>
    </item>
    
    <item>
      <title>Effective Java 笔记</title>
      <link>http://localhost:1313/2014/06/17/note-about-effective-java/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/17/note-about-effective-java/</guid>
      <description>创建和销毁对象 NO.1 考虑用静态工厂方法代替构造函数 静态工厂方法好处： 1、构造函数有命名的限制，而静态方法有自己的名字，更加易于理解。 2、静态工厂</description>
    </item>
    
    <item>
      <title>HBase源码分析：HTable put过程</title>
      <link>http://localhost:1313/2014/06/13/hbase-code-about-htable-put/</link>
      <pubDate>Fri, 13 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/13/hbase-code-about-htable-put/</guid>
      <description>HBase版本：0.94.15-cdh4.7.0 在 HBase中，大部分的操作都是在RegionServer完成的，Client端想要插入、删</description>
    </item>
    
    <item>
      <title>HBase实现简单聚合计算</title>
      <link>http://localhost:1313/2014/06/12/hbase-aggregate-client/</link>
      <pubDate>Thu, 12 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/12/hbase-aggregate-client/</guid>
      <description>本文主要记录如何通过打补丁的方式将“hbase中实现简单聚合计算”的特性引入hbase源代码中，并介绍通过命令行和java代码的使用方法。 支</description>
    </item>
    
    <item>
      <title>HBase客户端实现并行扫描</title>
      <link>http://localhost:1313/2014/06/12/hbase-parallel-client-scanner/</link>
      <pubDate>Thu, 12 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/12/hbase-parallel-client-scanner/</guid>
      <description>HBase中有一个类可以实现客户端扫描数据，叫做ClientScanner，该类不是并行的，有没有办法实现一个并行的扫描类，加快扫描速度呢？</description>
    </item>
    
    <item>
      <title>Hive Over HBase的介绍</title>
      <link>http://localhost:1313/2014/06/12/intro-of-hive-over-hbase/</link>
      <pubDate>Thu, 12 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/12/intro-of-hive-over-hbase/</guid>
      <description>Hive Over HBase是基于Hive的HQL查询引擎支持对hbase表提供及时查询的功能，它并不是将hql语句翻译成mapreduce来运行，其响应</description>
    </item>
    
    <item>
      <title>Hive中数据的加载和导出</title>
      <link>http://localhost:1313/2014/06/09/hive-data-manipulation-language/</link>
      <pubDate>Mon, 09 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/09/hive-data-manipulation-language/</guid>
      <description>关于 Hive DML 语法，你可以参考 apache 官方文档的说明:Hive Data Manipulation Language。 apache的hive版本现在应该是 0.13.0，而我使用的 hadoop 版本是</description>
    </item>
    
    <item>
      <title>Hive中的FetchTask任务</title>
      <link>http://localhost:1313/2014/06/09/fetchtask-in-hive/</link>
      <pubDate>Mon, 09 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/09/fetchtask-in-hive/</guid>
      <description>Hive中有各种各样的Task任务，其中FetchTask算是最简单的一种了。FetchTask不同于MapReduce任务，它不会启动ma</description>
    </item>
    
    <item>
      <title>使用Scrapy爬取知乎网站</title>
      <link>http://localhost:1313/2014/06/08/using-scrapy-to-cralw-zhihu/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/08/using-scrapy-to-cralw-zhihu/</guid>
      <description>本文主要记录使用使用 Scrapy 登录并爬取知乎网站的思路。Scrapy的相关介绍请参考 使用Scrapy抓取数据。 相关代码，见 https://github.com/javachen/scrapy-zhihu-github ，在阅读这部分代码之前，</description>
    </item>
    
    <item>
      <title>MongoDB介绍</title>
      <link>http://localhost:1313/2014/06/06/about-mongodb/</link>
      <pubDate>Fri, 06 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/06/about-mongodb/</guid>
      <description>MongoDB 是一个开源的，高性能，无模式（或者说是模式自由），使用 C++ 语言编写的面向文档的数据库。正因为 MongoDB 是面向文档的，所以它可以管理类似 JSON 的文档集合。</description>
    </item>
    
    <item>
      <title>不用Cloudera Manager安装Cloudera Search</title>
      <link>http://localhost:1313/2014/06/03/install_cloudera_search_without_cm/</link>
      <pubDate>Tue, 03 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/06/03/install_cloudera_search_without_cm/</guid>
      <description>Cloudera Search 用来在 hadoop 基础上建立索引和全文检索，本文主要记录如何安装 CLoudera Search 的过程，其中也包括如何安装和启动 Zookeeper、Solr、MapReduc</description>
    </item>
    
    <item>
      <title>关于CAP理论的一些笔记</title>
      <link>http://localhost:1313/2014/05/30/note-about-brewers-cap-theorem/</link>
      <pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/05/30/note-about-brewers-cap-theorem/</guid>
      <description>CAP的概念 2000年，Eric Brewer 教授在 ACM 分布式计算年会上指出了著名的 CAP 理论： 分布式系统不可能同时满足一致性(C: Consistency)，</description>
    </item>
    
    <item>
      <title>使用Scrapy抓取数据</title>
      <link>http://localhost:1313/2014/05/24/using-scrapy-to-cralw-data/</link>
      <pubDate>Sat, 24 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/05/24/using-scrapy-to-cralw-data/</guid>
      <description>Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广</description>
    </item>
    
    <item>
      <title>Nutch介绍及使用</title>
      <link>http://localhost:1313/2014/05/20/nutch-intro/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/05/20/nutch-intro/</guid>
      <description>1. Nutch介绍 Nutch是一个开源的网络爬虫项目，更具体些是一个爬虫软件，可以直接用于抓取网页内容。 现在Nutch分为两个版本，1.x和2</description>
    </item>
    
    <item>
      <title>Python开发框架Flask</title>
      <link>http://localhost:1313/2014/05/11/flask-intro/</link>
      <pubDate>Sun, 11 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/05/11/flask-intro/</guid>
      <description>1. Flask介绍 Flask 是一个基于Python的微型的web开发框架。虽然Flask是微框架，不过我们并不需要像别的微框架建议的那样把所有代码都写</description>
    </item>
    
    <item>
      <title>Bower介绍</title>
      <link>http://localhost:1313/2014/05/10/bower-intro/</link>
      <pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/05/10/bower-intro/</guid>
      <description>1. bower介绍 Bower 是用于 web 前端开发的包管理器。对于前端包管理方面的问题，它提供了一套通用、客观的解决方案。它通过一个 API 暴露包之间的依赖模型，</description>
    </item>
    
    <item>
      <title>All Things Markdown</title>
      <link>http://localhost:1313/2014/04/24/all-things-markdown/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/24/all-things-markdown/</guid>
      <description>目录 概述 特点 语法 编辑器 浏览器插件 实现版本 参考资料 概述 Markdown 是一种轻量级标记语言，创始人为约翰·格鲁伯（John Gruber）和亚伦·斯沃茨（Aa</description>
    </item>
    
    <item>
      <title>重装Mac系统之后</title>
      <link>http://localhost:1313/2014/04/23/after-reinstall-mac/</link>
      <pubDate>Wed, 23 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/23/after-reinstall-mac/</guid>
      <description>本文主要记录重装Mac系统之后的一些软件安装和环境变量配置。 #系统偏好设置 触控板 系统设置 &amp;gt; 触控板 光标与点击 ✓ 轻拍来点按 ✓ 辅助点按 ✓ 查找 ✓ 三指</description>
    </item>
    
    <item>
      <title>Ubuntu系统编译Bigtop</title>
      <link>http://localhost:1313/2014/04/17/building-bigtop-on-ubuntu/</link>
      <pubDate>Thu, 17 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/17/building-bigtop-on-ubuntu/</guid>
      <description>1. 安装系统依赖 系统更新并安装新的包 1 2 3 4 5 6 7 sudo apt-get update sudo apt-get install -y cmake git-core git-svn subversion checkinstall build-essential dh-make debhelper ant ant-optional autoconf automake liblzo2-dev libzip-dev sharutils libfuse-dev reprepro libtool libssl-dev asciidoc xmlto ssh curl sudo apt-get install -y devscripts sudo apt-get build-dep pkg-config 安装Sun JDK 6</description>
    </item>
    
    <item>
      <title>Java笔记：Java内存模型</title>
      <link>http://localhost:1313/2014/04/09/note-about-jvm-memery-model/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/09/note-about-jvm-memery-model/</guid>
      <description>1. 基本概念 《深入理解Java内存模型》详细讲解了java的内存模型，这里对其中的一些基本概念做个简单的笔记。以下内容摘自 《深入理解Java内</description>
    </item>
    
    <item>
      <title>RHEL系统下安装atlassian-jira-5</title>
      <link>http://localhost:1313/2014/04/09/install-jira5-on-rhel-system/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/09/install-jira5-on-rhel-system/</guid>
      <description>部署环境 操作系统：RHEL 6.4 x86_64 Jira版本：atlassian-jira-5.2.11-x64.bin 安装路径:/opt/atlassian</description>
    </item>
    
    <item>
      <title>PostgreSQL测试工具PGbench</title>
      <link>http://localhost:1313/2014/04/08/a-benchmark-tool-on-postgresql/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/08/a-benchmark-tool-on-postgresql/</guid>
      <description>pgbench 是一个简单的给 PostgreSQL 做性能测试的程序。它反复运行同样的 SQL 命令序列，可能是在多个并发数据库会话上头，然后检查平均的事务速度（每秒的事务数 tps）</description>
    </item>
    
    <item>
      <title>PostgreSQL监控指标</title>
      <link>http://localhost:1313/2014/04/07/some-metrics-in-postgresql/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/07/some-metrics-in-postgresql/</guid>
      <description>数据库版本：9.3.1（不同版本数据库相关表列名可能略有不同） 数据库状态信息 数据库状态信息主要体现数据库的当前状态 1.目前客户端的连接数 1 postgres=#</description>
    </item>
    
    <item>
      <title>RHEL系统安装PostgreSQL</title>
      <link>http://localhost:1313/2014/04/07/install-postgresql-on-rhel-system/</link>
      <pubDate>Mon, 07 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/07/install-postgresql-on-rhel-system/</guid>
      <description>环境说明 OS：RHEL6.4（x86_64） postgresql版本：PostgreSQL9.2.8 安装步骤 1. 下载所需的PostgreSQL</description>
    </item>
    
    <item>
      <title>RHEL系统安装MySql</title>
      <link>http://localhost:1313/2014/04/06/install-mysql-on-rhel-system/</link>
      <pubDate>Sun, 06 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/06/install-mysql-on-rhel-system/</guid>
      <description>环境说明 操作系统:linux6.4 MySql版本：percona 5.6.14 rpm包下载地址：http://www.percona.com/downl</description>
    </item>
    
    <item>
      <title>RHEL系统安装MySQL主备环境</title>
      <link>http://localhost:1313/2014/04/06/mysql-config-for-master-slave-replication/</link>
      <pubDate>Sun, 06 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/06/mysql-config-for-master-slave-replication/</guid>
      <description>环境准备 操作系统： rhel6.4 数据库： percona 5.6.14 使用3306端口保证端口未被占用，selinux关闭状态 原理说明 mysql的复制（Replication)是</description>
    </item>
    
    <item>
      <title>使用Lua和OpenResty搭建验证码服务器</title>
      <link>http://localhost:1313/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty/</link>
      <pubDate>Tue, 01 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/01/deploy-a-captcha-server-using-lua-and-openresty/</guid>
      <description>Lua下有个Lua-GD图形库，通过简单的Lua语句就能控制、生成图片。 环境说明： 操作系统：RHEL6.4 RHEL系统默认已安装RPM包的L</description>
    </item>
    
    <item>
      <title>使用SaltStack安装JDK1.6</title>
      <link>http://localhost:1313/2014/04/01/install-jdk-with-saltstack/</link>
      <pubDate>Tue, 01 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/04/01/install-jdk-with-saltstack/</guid>
      <description>创建states文件 在/srv/salt目录下创建jdk目录，并在jdk目录创建init.sls文件，init.sls文件内容如下： 1 2 3 4</description>
    </item>
    
    <item>
      <title>Python模拟新浪微博登录</title>
      <link>http://localhost:1313/2014/03/18/simulate-weibo-login-in-python/</link>
      <pubDate>Tue, 18 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/18/simulate-weibo-login-in-python/</guid>
      <description>看到一篇Python模拟新浪微博登录的文章，想熟悉一下其中实现方式，并且顺便掌握python相关知识点。 代码 下面的代码是来自上面这篇文章，并</description>
    </item>
    
    <item>
      <title>IDH HBase中实现的一些特性</title>
      <link>http://localhost:1313/2014/03/15/new-features-in-idh-hbase/</link>
      <pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/15/new-features-in-idh-hbase/</guid>
      <description>IDH为Intel&amp;rsquo;s Distribution of Hadoop的简称，中文为英特尔Hadoop发行版，目前应该没有人在维护该产品了。这里简单介绍一下ID</description>
    </item>
    
    <item>
      <title>Solr的schema.xml</title>
      <link>http://localhost:1313/2014/03/15/schema-in-solr/</link>
      <pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/15/schema-in-solr/</guid>
      <description>schema.xml是Solr一个配置文件，它包含了你的文档所有的字段，以及当文档被加入索引或查询字段时，这些字段是如何被处理的。这个文件被</description>
    </item>
    
    <item>
      <title>BroadLeaf项目集成SolrCloud</title>
      <link>http://localhost:1313/2014/03/14/broadleaf-project-with-solrcloud/</link>
      <pubDate>Fri, 14 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/14/broadleaf-project-with-solrcloud/</guid>
      <description>这篇文章主要介绍 BroadLeaf 项目如何集成 SolrCloud 集群。 1、SolrCloud环境搭建 参考 《Apache SolrCloud安装》，搭建Solr集群环境，将 Demosite 所</description>
    </item>
    
    <item>
      <title>在Solr中使用中文分词</title>
      <link>http://localhost:1313/2014/03/14/split-chinese-in-solr/</link>
      <pubDate>Fri, 14 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/14/split-chinese-in-solr/</guid>
      <description>使用全文检索，中文分词是离不开的，这里我采用的是 mmseg4j 分词器。mmseg4j分词器内置了对solr的支持，最新版本可支持4.X版本的sorl，使</description>
    </item>
    
    <item>
      <title>Apache SolrCloud安装</title>
      <link>http://localhost:1313/2014/03/10/how-to-install-solrcloud/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/10/how-to-install-solrcloud/</guid>
      <description>SolrCloud 通过 ZooKeeper 集群来进行协调，使一个索引进行分片，各个分片可以分布在不同的物理节点上，多个物理分片组成一个完成的索引 Collection。Solr</description>
    </item>
    
    <item>
      <title>HBase源码：HMaster启动过程</title>
      <link>http://localhost:1313/2014/03/09/hbase-note-about-hmaster-startup/</link>
      <pubDate>Sun, 09 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/09/hbase-note-about-hmaster-startup/</guid>
      <description>版本：HBase 0.94.15-cdh4.7.0 调试HMaster 说明： 这部分参考和使用了https://github.com/codefollower/HBase-Re</description>
    </item>
    
    <item>
      <title>HBase源码：HRegionServer启动过程</title>
      <link>http://localhost:1313/2014/03/09/hbase-note-about-hregionserver-startup/</link>
      <pubDate>Sun, 09 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/09/hbase-note-about-hregionserver-startup/</guid>
      <description>版本：HBase 0.94.15-cdh4.7.0 关于HMaster启动过程，请参考HBase源码：HMaster启动过程。先启动了HMaster之后，再启动HRegion</description>
    </item>
    
    <item>
      <title>Apache Solr查询语法</title>
      <link>http://localhost:1313/2014/03/03/solr-query-syntax/</link>
      <pubDate>Mon, 03 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/03/03/solr-query-syntax/</guid>
      <description>查询参数 常用： q - 查询字符串，必须的。 fl - 指定返回那些字段内容，用逗号或空格分隔多个。 start - 返回第一条记录在完整找到结果中的偏移位置，0开始，</description>
    </item>
    
    <item>
      <title>Apache Solr介绍及安装</title>
      <link>http://localhost:1313/2014/02/26/how-to-install-solr/</link>
      <pubDate>Wed, 26 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/02/26/how-to-install-solr/</guid>
      <description>Solr是什么 Solr是一个基于Lucene java库的企业级搜索服务器，包含XML/HTTP，JSON API，高亮查询结果，缓存，复制，还</description>
    </item>
    
    <item>
      <title>使用Vagrant创建虚拟机</title>
      <link>http://localhost:1313/2014/02/23/create-virtualbox-by-vagrant/</link>
      <pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/02/23/create-virtualbox-by-vagrant/</guid>
      <description>1、安装VirtualBox 下载地址：https://www.virtualbox.org/wiki/Downloads/ 提示：虽然 Vagrant 也支持</description>
    </item>
    
    <item>
      <title>Python基础入门</title>
      <link>http://localhost:1313/2014/02/22/python-introduction-of-basics/</link>
      <pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/02/22/python-introduction-of-basics/</guid>
      <description>1. Python介绍 Python是一种解释性的面向对象的语言。Python使用C语言编写，不需要事先声明变量的类型（动态类型），但是一旦变量有</description>
    </item>
    
    <item>
      <title>Confluence 5.4.2安装</title>
      <link>http://localhost:1313/2014/02/21/install-confluence5-4-2/</link>
      <pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/02/21/install-confluence5-4-2/</guid>
      <description>Confluence是Atlassian公司出品的团队协同与知识管理工具。 Confluence是一个专业的企业知识管理与协同软件，也可以用于</description>
    </item>
    
    <item>
      <title>Backbone中的模型</title>
      <link>http://localhost:1313/2014/02/16/backbone-model/</link>
      <pubDate>Sun, 16 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/02/16/backbone-model/</guid>
      <description>创建model 模型是所有Javascript应用程序的核心，包括交互数据及相关的大量逻辑： 转换、验证、计算属性和访问控制。你可以用特定的方法</description>
    </item>
    
    <item>
      <title>在CentOs6系统上安装Ganglia</title>
      <link>http://localhost:1313/2014/01/25/how-to-install-ganglia-on-centos6/</link>
      <pubDate>Sat, 25 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/01/25/how-to-install-ganglia-on-centos6/</guid>
      <description>Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad</description>
    </item>
    
    <item>
      <title>在RHEL系统上安装Nagios</title>
      <link>http://localhost:1313/2014/01/24/how-to-install-nagios-on-rhel6/</link>
      <pubDate>Fri, 24 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/01/24/how-to-install-nagios-on-rhel6/</guid>
      <description>在管理机上安装rpm包 1 2 3 4 5 $ rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm $ rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm $ yum -y install nagios nagios-plugins-all nagios-plugins-nrpe nrpe php httpd $ chkconfig httpd on &amp;amp;&amp;amp; chkconfig nagios on $ service httpd start &amp;amp;&amp;amp; service nagios start 设置管理界面密码 1 $ htpasswd -c /etc/nagios/passwd nagiosadmin 密码和用户名</description>
    </item>
    
    <item>
      <title>All Things OpenTSDB</title>
      <link>http://localhost:1313/2014/01/22/all-things-opentsdb/</link>
      <pubDate>Wed, 22 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/01/22/all-things-opentsdb/</guid>
      <description>1. OpenTSDB介绍 OpenTSDB用HBase存储所有的时序（无须采样）来构建一个分布式、可伸缩的时间序列数据库。它支持秒级数据采集所有</description>
    </item>
    
    <item>
      <title>SSH远程连接时环境变量问题</title>
      <link>http://localhost:1313/2014/01/18/bash-problem-when-ssh-access/</link>
      <pubDate>Sat, 18 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/01/18/bash-problem-when-ssh-access/</guid>
      <description>1. 问题 RHEL服务器A有个启动脚本（普通用户user01运行），里面使用ifconfig获取ip地址如下： 1 Localhost_ip=$(ifconfig |awk -F &amp;#39;addr:|Bcast&amp;#39; &amp;#39;/Bcast/{print $2}&amp;#39;) 由于普通用户user</description>
    </item>
    
    <item>
      <title>HBase笔记：Region拆分策略</title>
      <link>http://localhost:1313/2014/01/16/hbase-region-split-policy/</link>
      <pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/01/16/hbase-region-split-policy/</guid>
      <description>Region 概念 Region是表获取和分布的基本元素，由每个列族的一个Store组成。对象层级图如下： 1 2 3 4 5 6 Table (HBase table) Region (Regions for the table) Store (Store per ColumnFamily for each Region for the table)</description>
    </item>
    
    <item>
      <title>Hive使用HAProxy配置HA</title>
      <link>http://localhost:1313/2014/01/08/hive-ha-by-haproxy/</link>
      <pubDate>Wed, 08 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2014/01/08/hive-ha-by-haproxy/</guid>
      <description>HAProxy是一款提供高可用性、负载均衡以及基于TCP（第四层）和HTTP（第七层）应用的代理软件，HAProxy是完全免费的、借助HAP</description>
    </item>
    
    <item>
      <title>Git配置和一些常用命令</title>
      <link>http://localhost:1313/2013/12/27/some-git-configs-and-cammands/</link>
      <pubDate>Fri, 27 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/12/27/some-git-configs-and-cammands/</guid>
      <description>Git是一个分布式版本控制／软件配置管理软件，原来是linux内核开发者林纳斯·托瓦兹（Linus Torvalds）为了更好地管理linux</description>
    </item>
    
    <item>
      <title>SaltStack学习笔记</title>
      <link>http://localhost:1313/2013/11/18/study-note-of-saltstack/</link>
      <pubDate>Mon, 18 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/11/18/study-note-of-saltstack/</guid>
      <description>1. 关于本文档 这份文档如其名，是我自己整理的学习 SaltStack 的过程记录。只是过程记录，没有刻意像教程那样去做。所以呢，从前至后，中间不免有一些概念不清不</description>
    </item>
    
    <item>
      <title>使用SaltStack安装JBoss</title>
      <link>http://localhost:1313/2013/11/16/install-jboss-with-saltstack/</link>
      <pubDate>Sat, 16 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/11/16/install-jboss-with-saltstack/</guid>
      <description>SaltStack是一个具备puppet与func功能为一身的集中化管理平台，其基于python实现，功能十分强大，各模块融合度及复用性极高</description>
    </item>
    
    <item>
      <title>安装SaltStack和Halite</title>
      <link>http://localhost:1313/2013/11/11/install-saltstack-and-halite/</link>
      <pubDate>Mon, 11 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/11/11/install-saltstack-and-halite/</guid>
      <description>本文记录安装SaltStack和halite过程。 首先准备两台rhel或者centos虚拟机sk1和sk2，sk1用于安装master，sk</description>
    </item>
    
    <item>
      <title>在Eclipse中调试运行HBase</title>
      <link>http://localhost:1313/2013/11/01/debug-hbase-in-eclipse/</link>
      <pubDate>Fri, 01 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/11/01/debug-hbase-in-eclipse/</guid>
      <description>这篇文章记录一下如何在eclipse中调试运行hbase。 下载并编译源代码 请参考编译hbase源代码并打补丁 修改配置文件 修改 conf/hba</description>
    </item>
    
    <item>
      <title>编译CDH HBase源代码并打补丁</title>
      <link>http://localhost:1313/2013/10/28/compile-hbase-source-code-and-apply-patches/</link>
      <pubDate>Mon, 28 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/10/28/compile-hbase-source-code-and-apply-patches/</guid>
      <description>写了一篇博客记录编译CDH HBase源代码并打补丁的过程，如有不正确的，欢迎指出！ 下载源代码 从Cloudera github上下载最新分支源代</description>
    </item>
    
    <item>
      <title>HiveServer2中使用jdbc客户端用户运行mapreduce</title>
      <link>http://localhost:1313/2013/10/17/run-mapreduce-with-client-user-in-hive-server2/</link>
      <pubDate>Thu, 17 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/10/17/run-mapreduce-with-client-user-in-hive-server2/</guid>
      <description>最近做了个web系统访问hive数据库，类似于官方自带的hwi、安居客的hwi改进版和大众点评的polestar(github地址)系统，但</description>
    </item>
    
    <item>
      <title>Hive连接产生笛卡尔集</title>
      <link>http://localhost:1313/2013/10/17/cartesian-product-in-hive-inner-join/</link>
      <pubDate>Thu, 17 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/10/17/cartesian-product-in-hive-inner-join/</guid>
      <description>在使用hive过程中遇到这样的一个异常： 1 FAILED: ParseException line 1:18 Failed to recognize predicate &amp;#39;a&amp;#39;. Failed rule: &amp;#39;kwInner&amp;#39; in join type specifier 执行的hql语句如下： 1 [root@javachen.com ~]# hive -e &amp;#39;select a.* from t a, t b where a.id=b.id&amp;#39; 从异常信息中很难</description>
    </item>
    
    <item>
      <title>Hive中如何确定map数</title>
      <link>http://localhost:1313/2013/09/04/how-to-decide-map-number/</link>
      <pubDate>Wed, 04 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/09/04/how-to-decide-map-number/</guid>
      <description>Hive 是基于 Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的 sql 查询功能，可以将 sq l语句转换为 MapReduce 任务进行运行</description>
    </item>
    
    <item>
      <title>使用ZooKeeper实现配置同步</title>
      <link>http://localhost:1313/2013/08/23/publish-proerties-using-zookeeper/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/08/23/publish-proerties-using-zookeeper/</guid>
      <description>前言 应用项目中都会有一些配置信息，这些配置信息数据量少，一般会保存到内存、文件或者数据库，有时候需要动态更新。当需要在多个应用服务器中修改这</description>
    </item>
    
    <item>
      <title>远程调试Hadoop各组件</title>
      <link>http://localhost:1313/2013/08/01/remote-debug-hadoop/</link>
      <pubDate>Thu, 01 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/08/01/remote-debug-hadoop/</guid>
      <description>远程调试对应用程序开发十分有用。例如，为不能托管开发平台的低端机器开发程序，或在专用的机器上（比如服务不能中断的 Web 服务器）调试程序。其他情况</description>
    </item>
    
    <item>
      <title>安装RHadoop</title>
      <link>http://localhost:1313/2013/07/20/install-rhadoop/</link>
      <pubDate>Sat, 20 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/07/20/install-rhadoop/</guid>
      <description>1. R Language Install 安装相关依赖 1 2 3 4 yum install -y perl* pcre-devel tcl-devel zlib-devel bzip2-devel libX11-devel tk-devel tetex-latex *gfortran* compat-readline5 yum install libRmath-* rpm -Uvh --force --nodeps R-core-2.10.0-2.el5.x86_64.rpm rpm -Uvh R-2.10.0-2.el5.x86_64.rpm R-devel-2.10.0-2.el5.x86_64.rpm 编译安装：R-3.0.1 1 2 3 4 5 tar -zxvf R-3.0.1 ./configure make make install #R运行 export HADOOP_CMD=/usr/bin/hadoop 排</description>
    </item>
    
    <item>
      <title>HBase笔记：存储结构</title>
      <link>http://localhost:1313/2013/06/15/hbase-note-about-data-structure/</link>
      <pubDate>Sat, 15 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/06/15/hbase-note-about-data-structure/</guid>
      <description>从HBase的架构图上可以看出，HBase中的存储包括HMaster、HRegionServer、HRegion、Store、MemStor</description>
    </item>
    
    <item>
      <title>Java笔记：集合框架实现原理</title>
      <link>http://localhost:1313/2013/06/08/java-collection-framework/</link>
      <pubDate>Sat, 08 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/06/08/java-collection-framework/</guid>
      <description>这篇文章是对http://www.cnblogs.com/skywang12345/category/455711.html中java集合框</description>
    </item>
    
    <item>
      <title>使用yum源安装CDH集群</title>
      <link>http://localhost:1313/2013/04/06/install-cloudera-cdh-by-yum/</link>
      <pubDate>Sat, 06 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/04/06/install-cloudera-cdh-by-yum/</guid>
      <description>本文主要是记录使用yum安装CDH Hadoop集群的过程，包括HDFS、Yarn、Hive和HBase。目前使用的是CDH6.2.0安装集群</description>
    </item>
    
    <item>
      <title>使用yum源安装Impala过程</title>
      <link>http://localhost:1313/2013/03/29/install-impala/</link>
      <pubDate>Fri, 29 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/03/29/install-impala/</guid>
      <description>与Hive类似，Impala也可以直接与HDFS和HBase库直接交互。只不过Hive和其它建立在MapReduce上的框架适合需要长时间运</description>
    </item>
    
    <item>
      <title>手动安装Cloudera HBase CDH</title>
      <link>http://localhost:1313/2013/03/24/manual-install-Cloudera-hbase-CDH/</link>
      <pubDate>Sun, 24 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/03/24/manual-install-Cloudera-hbase-CDH/</guid>
      <description>本文主要记录手动安装Cloudera HBase集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，ha</description>
    </item>
    
    <item>
      <title>手动安装Cloudera Hive CDH</title>
      <link>http://localhost:1313/2013/03/24/manual-install-Cloudera-hive-CDH/</link>
      <pubDate>Sun, 24 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/03/24/manual-install-Cloudera-hive-CDH/</guid>
      <description>本文主要记录手动安装Cloudera Hive集群过程，环境设置及Hadoop安装过程见手动安装Cloudera Hadoop CDH,参考这篇文章，had</description>
    </item>
    
    <item>
      <title>手动安装Hadoop集群</title>
      <link>http://localhost:1313/2013/03/24/manual-install-Cloudera-Hadoop-CDH/</link>
      <pubDate>Sun, 24 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/2013/03/24/manual-install-Cloudera-Hadoop-CDH/</guid>
      <description>安装版本 centos版本为6。 hadoop各个组件和jdk版本如下： 1 2 3 4 hadoop-2.0.0-cdh4.6.0 hbase-0.94.15-cdh4.6.0 hive-0.10.0-cdh4.6.0 jdk1.6.0_38 hadoop各组件可以在这里下载。 安装前说明 确定安装目录</description>
    </item>
    
  </channel>
</rss>