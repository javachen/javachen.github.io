<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>当前数据仓库建设过程 - JavaChen Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="JavaChen" /><meta name="description" content="一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。" /><meta name="keywords" content="hadoop,hive" />


<meta name="baidu-site-verification" content="OMsbiDfo1G" />



<meta name="generator" content="Hugo 0.54.0 with theme even" />


<link rel="canonical" href="https://blog.javachen.space/2014/10/23/hive-warehouse-in-2014/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<link href="/dist/even.b90a1cc1.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">
<link rel="stylesheet" href="/css/custom.css">


<meta property="og:title" content="当前数据仓库建设过程" />
<meta property="og:description" content="一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.javachen.space/2014/10/23/hive-warehouse-in-2014/" />
<meta property="article:published_time" content="2014-10-23T08:00:00&#43;08:00"/>
<meta property="article:modified_time" content="2014-10-23T08:00:00&#43;08:00"/>

<meta itemprop="name" content="当前数据仓库建设过程">
<meta itemprop="description" content="一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。">


<meta itemprop="datePublished" content="2014-10-23T08:00:00&#43;08:00" />
<meta itemprop="dateModified" content="2014-10-23T08:00:00&#43;08:00" />
<meta itemprop="wordCount" content="2519">



<meta itemprop="keywords" content="hive," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="当前数据仓库建设过程"/>
<meta name="twitter:description" content="一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">JavaChen Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">JavaChen Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">当前数据仓库建设过程</h1>

      <div class="post-meta">
        <span class="post-time"> 2014-10-23 </span>
        <div class="post-category">
            <a href="/categories/hive/"> hive </a>
            </div>
          <span class="more-meta"> 约 2519 字 </span>
          <span class="more-meta"> 预计阅读 6 分钟 </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#1-数据采集和存储">1. 数据采集和存储</a>
<ul>
<li><a href="#使用-sqoop">使用 Sqoop</a></li>
</ul></li>
<li><a href="#2-数据加工">2. 数据加工</a>
<ul>
<li><a href="#任务调度">任务调度</a></li>
</ul></li>
<li><a href="#3-数据展现">3. 数据展现</a></li>
<li><a href="#4-总结">4. 总结</a></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<p>一个典型的企业数据仓库通常包含数据采集、数据加工和存储、数据展现等几个过程，本篇文章将按照这个顺序记录部门当前建设数据仓库的过程。</p>

<h1 id="1-数据采集和存储">1. 数据采集和存储</h1>

<p>采集数据之前，先要定义数据如何存放在 hadoop 以及一些相关约束。约束如下：</p>

<ul>
<li>所有的日志数据都存放在 hdfs 上的 <code>/logroot</code> 路径下面</li>
<li>hive 中数据库命名方式为 <code>dw_XXXX</code>，例如：dw_srclog 存放外部来源的原始数据，dw_stat 存放统计结果的数据</li>
<li>原始数据都加工成为结构化的文本文件，字段分隔符统一使用制表符，并在 lzo 压缩之后上传到 hdfs 中。</li>
<li>hive 中使用外部表保存数据，数据存放在 <code>/logroot</code> 下，如果不是分区表，则文件名为表名；如果是分区表，则按月和天分区，每天分区下的文件名为<code>表名_日期</code>，例如：<code>test_20141023</code></li>
</ul>

<p>数据采集的来源可能是关系数据库或者一些系统日志，采集工具可以是日志采集系统，例如：flume、sqoop 、storm以及一些 ETL 工具等等。</p>

<p>目前，主要是从 mysql 中导出数据然后在导入到 hdfs 中，对于存储不需要按天分区的表，这部分过程代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="cp">#!/bin/bash
</span><span class="cp"></span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> 
  <span class="nv">DAY</span><span class="o">=</span><span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span>
<span class="k">else</span> 
  <span class="nv">DAY</span><span class="o">=</span><span class="s2">&#34;yesterday&#34;</span>
<span class="k">fi</span>

<span class="nv">datestr</span><span class="o">=</span><span class="sb">`</span>date +%Y-%m-%d -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span><span class="p">;</span>
<span class="nv">logday</span><span class="o">=</span><span class="sb">`</span>date +%Y%m%d -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span><span class="p">;</span>
<span class="nv">logmonth</span><span class="o">=</span><span class="sb">`</span>date +%Y%m -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span>

<span class="c1">#hive table</span>
<span class="nv">table</span><span class="o">=</span><span class="nb">test</span>
<span class="c1">#mysql db config file</span>
<span class="nv">srcdb</span><span class="o">=</span>db_name

<span class="nv">sql</span><span class="o">=</span><span class="s2">&#34;select * from test&#34;</span>

<span class="nv">hql</span><span class="o">=</span><span class="s2">&#34;
</span><span class="s2">use dw_srclog;
</span><span class="s2">create external table if not exists test (
</span><span class="s2">  id int,
</span><span class="s2">  name int
</span><span class="s2">)ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;
</span><span class="s2">STORED AS INPUTFORMAT
</span><span class="s2">  &#39;com.hadoop.mapred.DeprecatedLzoTextInputFormat&#39;
</span><span class="s2">OUTPUTFORMAT
</span><span class="s2">  &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;
</span><span class="s2">LOCATION
</span><span class="s2">  &#39;/logroot/test&#39;;
</span><span class="s2">&#34;</span>

<span class="c1">#begin</span>
chmod +x <span class="nv">$srcdb</span>.sql
. ./<span class="nv">$srcdb</span>.sql

<span class="nv">file</span><span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">table</span><span class="si">}</span><span class="s2">&#34;</span>
<span class="nv">sql_var</span><span class="o">=</span><span class="s2">&#34; -r -quick --default-character-set=utf8  --skip-column&#34;</span>

mysql <span class="nv">$sql_var</span> -h<span class="si">${</span><span class="nv">db_host</span><span class="si">}</span> -u<span class="si">${</span><span class="nv">db_user</span><span class="si">}</span> -p<span class="si">${</span><span class="nv">db_pass</span><span class="si">}</span> -P<span class="si">${</span><span class="nv">db_port</span><span class="si">}</span> -D<span class="si">${</span><span class="nv">db_name</span><span class="si">}</span> -e <span class="s2">&#34;</span><span class="nv">$sql</span><span class="s2">&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s/NULL/\\\\N/g&#34;</span>&gt; <span class="nv">$file</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>

lzop -U <span class="nv">$file</span>
hadoop fs -mkdir -p /logroot/<span class="nv">$table</span>
hadoop fs -ls /logroot/<span class="nv">$table</span> <span class="p">|</span>grep lzo<span class="p">|</span>awk <span class="s1">&#39;{print $8}&#39;</span><span class="p">|</span>xargs -i hadoop fs -rm <span class="o">{}</span> 
hadoop fs -moveFromLocal <span class="nv">$file</span>.lzo /logroot/<span class="nv">$table</span>/
hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer  /logroot/<span class="nv">$table</span>/<span class="nv">$file</span>.lzo <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>

<span class="nb">echo</span> <span class="s2">&#34;create table if not exists&#34;</span>
hive -v -e <span class="s2">&#34;</span><span class="nv">$hql</span><span class="s2">;&#34;</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> </code></pre></td></tr></table>
</div>
</div>
<p>上面 bash 代码逻辑如下：</p>

<ul>
<li>1、判断是否输入参数，如果没有参数，则取昨天，意思是每天读取 mysql 数据库中昨天的数据。</li>
<li>2、定义 mysql 中 select 查询语句</li>
<li>3、定义 hive 中建表语句</li>
<li>4、读取 mysql 数据库连接信息，上例中为从 db_name.sql 中读取 <code>db_host</code>、<code>db_user</code>、<code>db_pass</code>、<code>db_port</code>、<code>db_name</code> 五个变量</li>
<li>5、运行 mysql 命令导出指定 sql 查询的结果，并将结果中的 NULL 字段转换为 <code>\\N</code>，因为 <code>\</code> 在 bash 中是转义字符，故需要使用两个 <code>\</code></li>
<li>6、lzo 压缩文件并上传到 hdfs，并且创建 lzo 索引</li>
<li>7、最后删除本地文件</li>
</ul>

<p>对于分区表来说，建表语句如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-sql" data-lang="sql"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-sql" data-lang="sql"><span class="n">use</span> <span class="n">dw_srclog</span><span class="p">;</span>
<span class="k">create</span> <span class="k">external</span> <span class="k">table</span> <span class="k">if</span> <span class="k">not</span> <span class="k">exists</span> <span class="n">test_p</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">name</span> <span class="nb">int</span>
<span class="p">)</span>
<span class="n">partitioned</span> <span class="k">by</span> <span class="p">(</span><span class="n">key_ym</span> <span class="nb">int</span><span class="p">,</span> <span class="n">key_ymd</span> <span class="nb">int</span><span class="p">)</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span> <span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">&#39;\t&#39;</span>
<span class="n">STORED</span> <span class="k">AS</span> <span class="n">INPUTFORMAT</span>
  <span class="s1">&#39;com.hadoop.mapred.DeprecatedLzoTextInputFormat&#39;</span>
<span class="n">OUTPUTFORMAT</span>
  <span class="s1">&#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;</span>
<span class="k">LOCATION</span>
  <span class="s1">&#39;/logroot/test_p&#39;</span><span class="p">;</span></code></pre></td></tr></table>
</div>
</div>
<p>从 mysql 导出文件并上传到 hdfs 命令如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1">#begin</span>
chmod +x <span class="nv">$srcdb</span>.sql
. ./<span class="nv">$srcdb</span>.sql

<span class="nv">file</span><span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">table</span><span class="si">}</span><span class="s2">_</span><span class="nv">$logday</span><span class="s2">&#34;</span>
<span class="nv">sql_var</span><span class="o">=</span><span class="s2">&#34; -r -quick --default-character-set=utf8  --skip-column&#34;</span>

mysql <span class="nv">$sql_var</span> -h<span class="si">${</span><span class="nv">db_host</span><span class="si">}</span> -u<span class="si">${</span><span class="nv">db_user</span><span class="si">}</span> -p<span class="si">${</span><span class="nv">db_pass</span><span class="si">}</span> -P<span class="si">${</span><span class="nv">db_port</span><span class="si">}</span> -D<span class="si">${</span><span class="nv">db_name</span><span class="si">}</span> -e <span class="s2">&#34;</span><span class="nv">$sql</span><span class="s2">&#34;</span> <span class="p">|</span> sed <span class="s2">&#34;s/NULL/\\\\N/g&#34;</span>&gt; <span class="nv">$file</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>

lzop -U <span class="nv">$file</span>
hadoop fs -mkdir -p /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>
hadoop fs -ls /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>/ <span class="p">|</span>grep lzo<span class="p">|</span>awk <span class="s1">&#39;{print $8}&#39;</span><span class="p">|</span>xargs -i hadoop fs -rm <span class="o">{}</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
hadoop fs -moveFromLocal <span class="nv">$file</span>.lzo /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>/
hadoop jar /usr/lib/hadoop/lib/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer  /logroot/<span class="nv">$table</span>/key_ym<span class="o">=</span><span class="nv">$logmonth</span>/key_ymd<span class="o">=</span><span class="nv">$logday</span>/<span class="nv">$file</span>.lzo <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>

hive -v -e <span class="s2">&#34;</span><span class="nv">$hql</span><span class="s2">;ALTER TABLE </span><span class="nv">$table</span><span class="s2"> ADD IF NOT EXISTS PARTITION(key_ym=</span><span class="nv">$logmonth</span><span class="s2">,key_ymd=</span><span class="nv">$logday</span><span class="s2">) location &#39;/logroot/</span><span class="nv">$table</span><span class="s2">/key_ym=</span><span class="nv">$logmonth</span><span class="s2">/key_ymd=</span><span class="nv">$logday</span><span class="s2">&#39; &#34;</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span></code></pre></td></tr></table>
</div>
</div>
<p>通过上面的两个命令就可以实现将 mysql 中的数据导入到 hdfs 中。</p>

<p>这里需要注意以下几点：</p>

<ul>
<li>1、 hive 中原始日志使用默认的 textfile 方式存储，是为了保证日志的可读性，方便以后从 hdfs 下载来之后能够很方便的转换为结构化的文本文件并能浏览文件内容。</li>
<li>2、使用 lzo 压缩是为了节省存储空间</li>
<li>3、使用外包表建表，在删除表结构之后数据不会删，方便修改表结构和分区。</li>
</ul>

<h2 id="使用-sqoop">使用 Sqoop</h2>

<p>使用 sqoop 主要是用于从 oracle 中通过 jdbc 方式导出数据到 hdfs，sqoop 命令如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sqoop import --connect jdbc:oracle:thin:@192.168.56.121:2154:db --username bi_user_limit --password <span class="s1">&#39;XXXX&#39;</span> --query <span class="s2">&#34;select * from test where  \$CONDITIONS&#34;</span> --split-by id  -m <span class="m">5</span> --fields-terminated-by <span class="s1">&#39;\t&#39;</span> --lines-terminated-by <span class="s1">&#39;\n&#39;</span>  --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --target-dir <span class="s2">&#34;/logroot/test/key_ymd=20140315&#34;</span>  --delete-target-dir</code></pre></td></tr></table>
</div>
</div>
<h1 id="2-数据加工">2. 数据加工</h1>

<p>对于数据量比较小任务可以使用 impala 处理，对于数据量大的任务使用 hive hql 来处理。</p>

<p>impala 处理数据：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">impala-shell -i <span class="s1">&#39;192.168.56.121:21000&#39;</span> -r -q <span class="s2">&#34;</span><span class="nv">$sql</span><span class="s2">;&#34;</span> </code></pre></td></tr></table>
</div>
</div>
<p>有时候需要使用 impala 导出数据：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span></pre></td>
<td class="lntd">
<pre class="chroma">impala-shell -i &#39;192.168.56.121:21000&#39; -r -q &#34;$sql;&#34; -B --output_delimiter=&#34;\t&#34; -o $file
sed -i &#39;1d&#39; $file  #导出的第一行有不可见的字符</pre></td></tr></table>
</div>
</div>
<p>使用 hive 处理数据生成结果表：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="cp">#!/bin/bash
</span><span class="cp"></span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> 
  <span class="nv">DAY</span><span class="o">=</span><span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span>
<span class="k">else</span> 
  <span class="nv">DAY</span><span class="o">=</span><span class="s2">&#34;yesterday&#34;</span>
<span class="k">fi</span>

<span class="nb">echo</span> <span class="s2">&#34;DAY=</span><span class="nv">$DAY</span><span class="s2">&#34;</span>

<span class="nv">datestr</span><span class="o">=</span><span class="sb">`</span>date +%Y-%m-%d -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span><span class="p">;</span>
<span class="nv">logday</span><span class="o">=</span><span class="sb">`</span>date +%Y%m%d -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span><span class="p">;</span>
<span class="nv">logmonth</span><span class="o">=</span><span class="sb">`</span>date +%Y%m -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span>

<span class="c1">#target table</span>
<span class="nv">table</span><span class="o">=</span>stat_test_p
<span class="nv">sql</span><span class="o">=</span><span class="s2">&#34;use dw_srclog;insert OVERWRITE table stat_test_p partition(key_ym=</span><span class="nv">$logmonth</span><span class="s2">,key_ymd=</span><span class="nv">$logday</span><span class="s2">)
</span><span class="s2">select id,count(name) from test_p where key_ymd=</span><span class="nv">$logday</span><span class="s2"> group by id
</span><span class="s2">&#34;</span>

<span class="nv">hql</span><span class="o">=</span><span class="s2">&#34;
</span><span class="s2">use dw_web;
</span><span class="s2">create external table if not exists goods_sales_info_day (
</span><span class="s2">  id int,
</span><span class="s2">  count int
</span><span class="s2">) partitioned by (key_ym int, key_ymd int)
</span><span class="s2">STORED AS RCFILE
</span><span class="s2">LOCATION &#39;/logroot/stat_test_p&#39;;
</span><span class="s2">&#34;</span>
<span class="c1">#begin</span>
hive -v -e <span class="s2">&#34;
</span><span class="s2"></span><span class="nv">$hql</span><span class="s2">;
</span><span class="s2">SET hive.exec.compress.output=true;
</span><span class="s2">SET mapreduce.input.fileinputformat.split.maxsize=128000000;
</span><span class="s2">SET mapred.output.compression.type=BLOCK;
</span><span class="s2">SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
</span><span class="s2"></span><span class="nv">$sql</span><span class="s2">&#34;</span> <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> </code></pre></td></tr></table>
</div>
</div>
<p>这里主要是先判断是否创建外包表（外包表存储为 RCFILE 格式），然后设置 map 的输出结果使用 snappy 压缩，并设置每个 map 的大小，最后运行 insert 语句。结果表存储为 RCFILE 的原因是，*在 CDH 5.2 之前，该格式的表可以被 impala 读取*。</p>

<h2 id="任务调度">任务调度</h2>

<p>当任务多了之后，每个任务之间会有一些依赖，为了保证任务的先后执行顺序，这里使用的是 azkaban 任务调度框架。</p>

<p>该框架的使用方式很简单：</p>

<ul>
<li>首先创建一个 bi_etl 目录，用于存放执行脚本。</li>
<li>在 bi_etl 目录下创建一个 properties 文件，文件名称任意，文件内容为：<code>DAY=yesterday</code>，这是一个系统默认参数，即默认 DAY 变量的值为 yesterday，该变量在运行时可以被覆盖：在 azkaban 的 web 管理界面，运行一个 Flow 时，添加一个 <code>Flow Parameters</code> 参数，Name 为 DAY，Value 为你想要指定的值，例如：20141023。</li>
<li>创建一个 bash 脚本 test.sh，文件内容如第一章节内容，需要注意的是该脚本中会判断是否有输出参数。</li>
<li>针对 bash 脚本，创建 azkaban 需要的 job 文件，文件内容如下（azkaban 运行该 job 时候，会替换 <code>${DAY}</code> 变量为实际的值 ）：</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-properties" data-lang="properties"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-properties" data-lang="properties">type=command
command=sh test.sh ${DAY}
failure.emails=XXX@163.com
dependencies=xxx</code></pre></td></tr></table>
</div>
</div>
<ul>
<li>最后，将 bi_etl 目录打包成 zip 文件，然后上传到 azkaban 管理界面上去，就可以运行或者是设置调度任务了。</li>
</ul>

<p>使用上面的方式编写 bash 脚本和 azkaban 的 job 的好处是：</p>

<ul>
<li>azkaban 的 job 可以指定参数来控制运行哪一天的任务</li>
<li>job 中实际上运行的是 bash 脚本，这些脚本脱离了 azkaban 也能正常运行，同样也支持传参数。</li>
</ul>

<h1 id="3-数据展现">3. 数据展现</h1>

<p>目前是将 hive 或者 impala 的处理结果推送到关系数据库中，由传统的 BI 报表工具展示数据或者直接通过 impala 查询数据生成报表并发送邮件。</p>

<p>为了保证报表的正常发送，需要监控任务的正常运行，当任务失败的时候能够发送邮件，这部分通过 azkaban 可以做到。另外，还需要监控每天运行的任务同步的记录数，下面脚本是统计记录数为0的任务：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="cp">#!/bin/bash
</span><span class="cp"></span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="nv">DAY</span><span class="o">=</span><span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span>
<span class="k">else</span>
  <span class="nv">DAY</span><span class="o">=</span><span class="s2">&#34;yesterday&#34;</span>
<span class="k">fi</span>

<span class="nb">echo</span> <span class="s2">&#34;DAY=</span><span class="nv">$DAY</span><span class="s2">&#34;</span>

<span class="nv">datestr</span><span class="o">=</span><span class="sb">`</span>date +%Y-%m-%d -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span><span class="p">;</span>
<span class="nv">logday</span><span class="o">=</span><span class="sb">`</span>date +%Y%m%d -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span><span class="p">;</span>
<span class="nv">logmonth</span><span class="o">=</span><span class="sb">`</span>date +%Y%m -d<span class="s2">&#34;</span><span class="nv">$DAY</span><span class="s2">&#34;</span><span class="sb">`</span>
<span class="nv">datemod</span><span class="o">=</span><span class="sb">`</span>date +%w -d <span class="s2">&#34;yesterday&#34;</span><span class="sb">`</span>

rm -rf /tmp/stat_table_day_count_<span class="nv">$logday</span>
touch /tmp/stat_table_day_count_<span class="nv">$logday</span>
<span class="k">for</span> db in <span class="sb">`</span>hadoop fs -ls /user/hive/warehouse<span class="p">|</span>grep -vE <span class="s1">&#39;testdb|dw_etl&#39;</span><span class="p">|</span>grep <span class="s1">&#39;.db&#39;</span><span class="p">|</span>awk <span class="s1">&#39;{print $8}&#39;</span><span class="p">|</span>awk -F <span class="s1">&#39;/&#39;</span> <span class="s1">&#39;{print $5}&#39;</span> <span class="p">|</span>awk -F <span class="s1">&#39;.&#39;</span> <span class="s1">&#39;{print $1}&#39;</span><span class="sb">`</span><span class="p">;</span><span class="k">do</span>
    <span class="k">for</span> table in <span class="sb">`</span>hive -S -e <span class="s2">&#34;set hive.cli.print.header=false; use </span><span class="nv">$db</span><span class="s2">;show tables&#34;</span> <span class="sb">`</span> <span class="p">;</span><span class="k">do</span>
        <span class="nv">count_new</span><span class="o">=</span><span class="s2">&#34;&#34;</span>
        <span class="nv">result</span><span class="o">=</span><span class="sb">`</span>hive -S -e <span class="s2">&#34;set hive.cli.print.header=false; use </span><span class="nv">$db</span><span class="s2">;show create table </span><span class="nv">$table</span><span class="s2">;&#34;</span>  <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> grep PARTITIONED<span class="sb">`</span>
        <span class="k">if</span> <span class="o">[</span> <span class="si">${#</span><span class="nv">result</span><span class="si">}</span> -gt <span class="m">0</span> <span class="o">]</span><span class="p">;</span><span class="k">then</span>
      <span class="nv">is_part</span><span class="o">=</span><span class="m">1</span>
      <span class="nv">count_new</span><span class="o">=</span><span class="sb">`</span>impala-shell -k -i <span class="m">10</span>.168.35.127:21089 --quiet -B --output_delimiter<span class="o">=</span><span class="s2">&#34;\t&#34;</span> -q <span class="s2">&#34;select count(1) from </span><span class="si">${</span><span class="nv">db</span><span class="si">}</span><span class="s2">.</span><span class="nv">$table</span><span class="s2"> where key_ymd=</span><span class="nv">$logday</span><span class="s2"> &#34;</span><span class="sb">`</span>
        <span class="k">else</span>
      <span class="nv">is_part</span><span class="o">=</span><span class="m">0</span>
      <span class="nv">count_new</span><span class="o">=</span><span class="sb">`</span>impala-shell -k -i <span class="m">10</span>.168.35.127:21089 --quiet -B --output_delimiter<span class="o">=</span><span class="s2">&#34;\t&#34;</span> -q <span class="s2">&#34;select count(1) from </span><span class="si">${</span><span class="nv">db</span><span class="si">}</span><span class="s2">.</span><span class="nv">$table</span><span class="s2">; &#34;</span><span class="sb">`</span>
        <span class="k">fi</span>
        <span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$db</span><span class="s2">,</span><span class="nv">$table</span><span class="s2">,</span><span class="nv">$is_part</span><span class="s2">,</span><span class="nv">$count_new</span><span class="s2">&#34;</span> &gt;&gt; /tmp/stat_table_day_count_<span class="nv">$logday</span>
    <span class="k">done</span>
<span class="k">done</span>

<span class="c1">#mail -s &#34;The count of the table between old and new cluster in $datestr&#34; -c $mails &lt; /tmp/stat_table_day_count_$logday</span>

sed -i <span class="s1">&#39;s/1034h//g&#39;</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>
sed -i <span class="s1">&#39;s/\[//g&#39;</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>
sed -i <span class="s1">&#39;s/\?//g&#39;</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>
sed -i <span class="s1">&#39;s/\x1B//g&#39;</span> /tmp/stat_table_day_count_<span class="nv">$logday</span>

<span class="nv">res</span><span class="o">=</span><span class="sb">`</span>cat /tmp/stat_table_day_count_<span class="nv">$logday</span><span class="p">|</span>grep -E <span class="s1">&#39;1,0|0,0&#39;</span><span class="p">|</span>grep -v stat_table_day_count<span class="sb">`</span>

<span class="nb">echo</span> <span class="nv">$res</span>

hive -e <span class="s2">&#34;use dw_default;
</span><span class="s2">LOAD DATA LOCAL INPATH &#39;/tmp/stat_table_day_count_</span><span class="nv">$logday</span><span class="s2">&#39; overwrite INTO TABLE stat_table_day_count  PARTITION (key_ym=</span><span class="nv">$logmonth</span><span class="s2">,key_ymd=</span><span class="nv">$logday</span><span class="s2">)
</span><span class="s2">&#34;</span>
python mail.py <span class="s2">&#34;Count is 0 in </span><span class="nv">$datestr</span><span class="s2">&#34;</span> <span class="s2">&#34;</span><span class="nv">$res</span><span class="s2">&#34;</span></code></pre></td></tr></table>
</div>
</div>
<h1 id="4-总结">4. 总结</h1>

<p>上面介绍了数据采集、加工和任务调度的过程，有些地方还可以改进：</p>

<ul>
<li>引入 ETL 工具实现关系数据库导入到 hadoop，例如：Kettle 工具</li>
<li>目前是每天一次从 mysql 同步数据到 hadoop，以后需要修改同步频率，做到更实时</li>
<li>hive 和 impala 在字段类型、存储方式、函数的兼容性上存在一些问题</li>
</ul>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">JavaChen</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2014-10-23
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/wechatpay.jpg">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/alipay.jpg">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/hive/">hive</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2014/10/24/impala-query-table-tutorial/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Impala查询功能测试</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/2014/09/29/spring-source-codes/">
            <span class="next-text nav-default">Spring源码整体架构</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="javachen/javachen.github.io"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:junecloud@163.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/javachen" class="iconfont icon-github" title="github"></a>
      <a href="http://weibo.com/chenzhijun" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://space.bilibili.com/287563020/" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://blog.javachen.space/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2009 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">JavaChen</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.e1476869.min.js"></script>

<script id="baidu_analytics">
  var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?7eaf37274cf8796df56903a88389e82f";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
