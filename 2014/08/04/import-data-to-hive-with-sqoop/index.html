<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Sqoop导入关系数据库到Hive - JavaChen Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="JavaChen" /><meta name="description" content="Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。" /><meta name="keywords" content="Sqoop, 是, apache, 下用于, RDBMS, 和, HDFS, 互相导数据的工具。" />


<meta name="baidu-site-verification" content="OMsbiDfo1G" />



<meta name="generator" content="Hugo 0.54.0 with theme even" />


<link rel="canonical" href="https://blog.javachen.space/2014/08/04/import-data-to-hive-with-sqoop/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<link href="/dist/even.b90a1cc1.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">
<link rel="stylesheet" href="/css/custom.css">


<meta property="og:title" content="Sqoop导入关系数据库到Hive" />
<meta property="og:description" content="Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.javachen.space/2014/08/04/import-data-to-hive-with-sqoop/" />
<meta property="article:published_time" content="2014-08-04T08:00:00&#43;08:00"/>
<meta property="article:modified_time" content="2014-08-04T08:00:00&#43;08:00"/>

<meta itemprop="name" content="Sqoop导入关系数据库到Hive">
<meta itemprop="description" content="Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。">


<meta itemprop="datePublished" content="2014-08-04T08:00:00&#43;08:00" />
<meta itemprop="dateModified" content="2014-08-04T08:00:00&#43;08:00" />
<meta itemprop="wordCount" content="3396">



<meta itemprop="keywords" content="sqoop," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Sqoop导入关系数据库到Hive"/>
<meta name="twitter:description" content="Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">JavaChen Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">JavaChen Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Sqoop导入关系数据库到Hive</h1>

      <div class="post-meta">
        <span class="post-time"> 2014-08-04 </span>
        <div class="post-category">
            <a href="/categories/hive/"> hive </a>
            </div>
          <span class="more-meta"> 约 3396 字 </span>
          <span class="more-meta"> 预计阅读 7 分钟 </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#1-安装-sqoop">1. 安装 Sqoop</a></li>
<li><a href="#2-使用">2. 使用</a>
<ul>
<li><a href="#2-1-命令说明">2.1 命令说明</a></li>
</ul></li>
<li><a href="#2-2-导入数据到-hdfs">2.2 导入数据到 hdfs</a>
<ul>
<li>
<ul>
<li><a href="#使用-sql-语句">使用 sql 语句</a></li>
<li><a href="#使用-direct-模式">使用 direct 模式：</a></li>
<li><a href="#指定文件输出格式">指定文件输出格式：</a></li>
</ul></li>
</ul></li>
<li><a href="#2-3-创建-hive-表">2.3 创建 hive 表</a></li>
<li><a href="#2-4-导入数据到-hive">2.4 导入数据到 hive</a></li>
<li><a href="#2-5-增量导入">2.5 增量导入</a></li>
<li><a href="#2-6-合并-hdfs-文件">2.6 合并 hdfs 文件</a></li>
<li><a href="#3-参考文章">3. 参考文章</a></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<p>Sqoop 是 apache 下用于 RDBMS 和 HDFS 互相导数据的工具。本文以 mysql 数据库为例，实现关系数据库导入到 hdfs 和 hive。</p>

<h1 id="1-安装-sqoop">1. 安装 Sqoop</h1>

<p>使用 rpm 安装即可。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">yum install sqoop sqoop-metastore -y</pre></td></tr></table>
</div>
</div>
<blockquote>
<p>安装完之后需要下载 mysql jar 包到 sqoop 的 lib 目录。</p>
</blockquote>

<p>这里使用 hive 的 metastore 的 mysql 数据库作为关系数据库，以 TBLS 表为例，该表结构和数据如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-sql" data-lang="sql"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-sql" data-lang="sql"><span class="n">mysql</span><span class="o">&gt;</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">TBLS</span> <span class="k">limit</span> <span class="mi">3</span><span class="p">;</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+
</span><span class="c1"></span><span class="o">|</span><span class="n">TBL_ID</span><span class="o">|</span><span class="n">CREATE_TIME</span><span class="o">|</span><span class="n">DB_ID</span><span class="o">|</span><span class="n">LAST_ACCESS_TIME</span><span class="o">|</span><span class="k">OWNER</span><span class="o">|</span><span class="n">RETENTI</span> <span class="o">|</span> <span class="n">SD_ID</span><span class="o">|</span> <span class="n">TBL_NAME</span><span class="o">|</span> <span class="n">TBL_TYPE</span>       <span class="o">|</span><span class="n">VIEW_EXPANDED_TEXT</span><span class="o">|</span> <span class="n">VIEW_ORIGINAL_TEXT</span><span class="o">|</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+
</span><span class="c1"></span><span class="o">|</span>    <span class="mi">34</span><span class="o">|</span><span class="mi">1406784308</span> <span class="o">|</span>    <span class="mi">8</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">45</span><span class="o">|</span> <span class="n">test1</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">|</span>    <span class="mi">40</span><span class="o">|</span><span class="mi">1406797005</span> <span class="o">|</span>    <span class="mi">9</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">52</span><span class="o">|</span> <span class="n">test2</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">|</span>    <span class="mi">42</span><span class="o">|</span><span class="mi">1407122307</span> <span class="o">|</span>    <span class="mi">7</span><span class="o">|</span>               <span class="mi">0</span><span class="o">|</span><span class="n">root</span> <span class="o">|</span>       <span class="mi">0</span><span class="o">|</span>    <span class="mi">59</span><span class="o">|</span> <span class="n">test3</span>   <span class="o">|</span> <span class="n">EXTERNAL_TABLE</span> <span class="o">|</span> <span class="k">NULL</span>             <span class="o">|</span> <span class="k">NULL</span>              <span class="o">|</span>
<span class="o">+</span><span class="c1">------+-----------+-----+----------------+-----+--------+------+---------+----------------+------------------+-------------------+</span></code></pre></td></tr></table>
</div>
</div>
<h1 id="2-使用">2. 使用</h1>

<h2 id="2-1-命令说明">2.1 命令说明</h2>

<p>查看 sqoop 命令说明：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop <span class="nb">help</span>
usage: sqoop COMMAND <span class="o">[</span>ARGS<span class="o">]</span>

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  <span class="nb">eval</span>               Evaluate a SQL statement and display the results
  <span class="nb">export</span>             Export an HDFS directory to a database table
  <span class="nb">help</span>               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  version            Display version information

See <span class="s1">&#39;sqoop help COMMAND&#39;</span> <span class="k">for</span> information on a specific command.</code></pre></td></tr></table>
</div>
</div>
<p>你也可以查看某一个命令的使用说明：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --help
$ sqoop <span class="nb">help</span> import</code></pre></td></tr></table>
</div>
</div>
<p>你也可以使用别名来代替 <code>sqoop (toolname)</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop-import</code></pre></td></tr></table>
</div>
</div>
<p>sqoop import 的一个示例如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS</code></pre></td></tr></table>
</div>
</div>
<p>你还可以使用 <code>--options-file</code> 来传入一个文件，使用这种方式可以重用一些配置参数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop --options-file /users/homer/work/import.txt --table TEST</code></pre></td></tr></table>
</div>
</div>
<p>/users/homer/work/import.txt 文件内容如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></pre></td>
<td class="lntd">
<pre class="chroma">import
--connect
jdbc:mysql://192.168.56.121:3306/metastore
--username
hiveuser
--password 
redhat</pre></td></tr></table>
</div>
</div>
<h1 id="2-2-导入数据到-hdfs">2.2 导入数据到 hdfs</h1>

<p>使用 sqoop-import 命令可以从关系数据库导入数据到 hdfs。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --target-dir /user/hive/result</code></pre></td></tr></table>
</div>
</div>
<p>注意：</p>

<ul>
<li>mysql jdbc url 请使用 ip 地址</li>
<li>如果重复执行，会提示目录已经存在，可以手动删除</li>
<li>如果不指定 <code>--target-dir</code>，导入到用户家目录下的 TBLS 目录</li>
</ul>

<p>你还可以指定其他的参数：</p>

<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><code>--append</code></td>
<td align="left">将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。</td>
</tr>

<tr>
<td align="left"><code>--as-avrodatafile</code></td>
<td align="left">将数据导入到一个Avro数据文件中</td>
</tr>

<tr>
<td align="left"><code>--as-sequencefile</code></td>
<td align="left">将数据导入到一个sequence文件中</td>
</tr>

<tr>
<td align="left"><code>--as-textfile</code></td>
<td align="left">将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。</td>
</tr>

<tr>
<td align="left"><code>--boundary-query &lt;statement&gt;</code></td>
<td align="left">边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：<code>--boundary-query 'select id,no from t where id = 3'</code>，表示导入的数据为id=3的记录，或者 <code>select min(&lt;split-by&gt;), max(&lt;split-by&gt;) from &lt;table name&gt;</code>，注意查询的字段中不能有数据类型为字符串的字段，否则会报错</td>
</tr>

<tr>
<td align="left"><code>--columns&lt;col,col&gt;</code></td>
<td align="left">指定要导入的字段值，格式如：<code>--columns id,username</code></td>
</tr>

<tr>
<td align="left"><code>--direct</code></td>
<td align="left">直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快</td>
</tr>

<tr>
<td align="left"><code>--direct-split-size</code></td>
<td align="left">在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。</td>
</tr>

<tr>
<td align="left"><code>--inline-lob-limit</code></td>
<td align="left">设定大对象数据类型的最大值</td>
</tr>

<tr>
<td align="left"><code>-m,--num-mappers</code></td>
<td align="left">启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数</td>
</tr>

<tr>
<td align="left"><code>--query，-e &lt;sql&gt;</code></td>
<td align="left">从查询结果中导入数据，该参数使用时必须指定<code>–target-dir</code>、<code>–hive-table</code>，在查询语句中一定要有where条件且在where条件中需要包含 <code>\$CONDITIONS</code>，示例：<code>--query 'select * from t where \$CONDITIONS ' --target-dir /tmp/t –hive-table t</code></td>
</tr>

<tr>
<td align="left"><code>--split-by &lt;column&gt;</code></td>
<td align="left">表的列名，用来切分工作单元，一般后面跟主键ID</td>
</tr>

<tr>
<td align="left"><code>--table &lt;table-name&gt;</code></td>
<td align="left">关系数据库表名，数据从该表中获取</td>
</tr>

<tr>
<td align="left"><code>--delete-target-dir</code></td>
<td align="left">删除目标目录</td>
</tr>

<tr>
<td align="left"><code>--target-dir &lt;dir&gt;</code></td>
<td align="left">指定hdfs路径</td>
</tr>

<tr>
<td align="left"><code>--warehouse-dir &lt;dir&gt;</code></td>
<td align="left">与 <code>--target-dir</code> 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录</td>
</tr>

<tr>
<td align="left"><code>--where</code></td>
<td align="left">从关系数据库导入数据时的查询条件，示例：<code>--where &quot;id = 2&quot;</code></td>
</tr>

<tr>
<td align="left"><code>-z,--compress</code></td>
<td align="left">压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件</td>
</tr>

<tr>
<td align="left"><code>--compression-codec</code></td>
<td align="left">Hadoop压缩编码，默认是gzip</td>
</tr>

<tr>
<td align="left"><code>--null-string &lt;null-string&gt;</code></td>
<td align="left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>

<tr>
<td align="left"><code>--null-non-string &lt;null-string&gt;</code></td>
<td align="left">可选参数，如果没有指定，则字符串null将被使用</td>
</tr>
</tbody>
</table>

<p>示例程序：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --columns <span class="s2">&#34;tbl_id,create_time&#34;</span> --where <span class="s2">&#34;tbl_id &gt; 1&#34;</span> --target-dir /user/hive/result</code></pre></td></tr></table>
</div>
</div>
<h3 id="使用-sql-语句">使用 sql 语句</h3>

<p>参照上表，使用 sql 语句查询时，需要指定 <code>$CONDITIONS</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --query <span class="s1">&#39;SELECT * from TBLS where \$CONDITIONS &#39;</span> --split-by tbl_id -m <span class="m">4</span> --target-dir /user/hive/result</code></pre></td></tr></table>
</div>
</div>
<p>上面命令通过 <code>-m 1</code> 控制并发的 map 数。</p>

<h3 id="使用-direct-模式">使用 direct 模式：</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --delete-target-dir --direct --default-character-set UTF-8 --target-dir /user/hive/result</code></pre></td></tr></table>
</div>
</div>
<h3 id="指定文件输出格式">指定文件输出格式：</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&#34;\t&#34;</span> --lines-terminated-by <span class="s2">&#34;\n&#34;</span> --delete-target-dir  --target-dir /user/hive/result</code></pre></td></tr></table>
</div>
</div>
<p>这时候查看 hdfs 中数据(观察分隔符是否为制表符)：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ hadoop fs -ls result
Found <span class="m">5</span> items
-rw-r--r--   <span class="m">3</span> root hadoop          <span class="m">0</span> <span class="m">2014</span>-08-04 <span class="m">16</span>:07 result/_SUCCESS
-rw-r--r--   <span class="m">3</span> root hadoop         <span class="m">69</span> <span class="m">2014</span>-08-04 <span class="m">16</span>:07 result/part-m-00000
-rw-r--r--   <span class="m">3</span> root hadoop          <span class="m">0</span> <span class="m">2014</span>-08-04 <span class="m">16</span>:07 result/part-m-00001
-rw-r--r--   <span class="m">3</span> root hadoop        <span class="m">142</span> <span class="m">2014</span>-08-04 <span class="m">16</span>:07 result/part-m-00002
-rw-r--r--   <span class="m">3</span> root hadoop         <span class="m">62</span> <span class="m">2014</span>-08-04 <span class="m">16</span>:07 result/part-m-00003

$ hadoop fs -cat result/part-m-00000
<span class="m">34</span>	<span class="m">1406784308</span>	<span class="m">8</span>	<span class="m">0</span>	root	<span class="m">0</span>	<span class="m">45</span>	test1	EXTERNAL_TABLE	null	null	null

$ hadoop fs -cat result/part-m-00002
<span class="m">40</span>	<span class="m">1406797005</span>	<span class="m">9</span>	<span class="m">0</span>	root	<span class="m">0</span>	<span class="m">52</span>	test2	EXTERNAL_TABLE	null	null	null
<span class="m">42</span>	<span class="m">1407122307</span>	<span class="m">7</span>	<span class="m">0</span>	root	<span class="m">0</span>	<span class="m">59</span>	test3	EXTERNAL_TABLE	null	null	null</code></pre></td></tr></table>
</div>
</div>
<p>指定空字符串：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&#34;\t&#34;</span> --lines-terminated-by <span class="s2">&#34;\n&#34;</span> --delete-target-dir --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --target-dir /user/hive/result</code></pre></td></tr></table>
</div>
</div>
<p>如果需要指定压缩：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&#34;\t&#34;</span> --lines-terminated-by <span class="s2">&#34;\n&#34;</span> --delete-target-dir --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --compression-codec <span class="s2">&#34;com.hadoop.compression.lzo.LzopCodec&#34;</span> --target-dir /user/hive/result</code></pre></td></tr></table>
</div>
</div>
<p>附：可选的文件参数如下表。</p>

<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><code>--enclosed-by &lt;char&gt;</code></td>
<td align="left">给字段值前后加上指定的字符，比如双引号，示例：<code>--enclosed-by '\&quot;'</code>，显示例子：&rdquo;3&rdquo;,&ldquo;jimsss&rdquo;,&ldquo;dd@dd.com&rdquo;</td>
</tr>

<tr>
<td align="left"><code>--escaped-by &lt;char&gt;</code></td>
<td align="left">给双引号作转义处理，如字段值为&rdquo;测试&rdquo;，经过 <code>--escaped-by &quot;\\&quot;</code> 处理后，在hdfs中的显示值为：<code>\&quot;测试\&quot;</code>，对单引号无效</td>
</tr>

<tr>
<td align="left"><code>--fields-terminated-by &lt;char&gt;</code></td>
<td align="left">设定每个字段是以什么符号作为结束的，默认是逗号，也可以改为其它符号，如句号<code>.</code>，示例如：<code>--fields-terminated-by</code></td>
</tr>

<tr>
<td align="left"><code>--lines-terminated-by &lt;char&gt;</code></td>
<td align="left">设定每条记录行之间的分隔符，默认是换行串，但也可以设定自己所需要的字符串，示例如：<code>--lines-terminated-by &quot;#&quot;</code> 以#号分隔</td>
</tr>

<tr>
<td align="left"><code>--mysql-delimiters</code></td>
<td align="left">Mysql默认的分隔符设置，字段之间以<code>,</code>隔开，行之间以换行<code>\n</code>隔开，默认转义符号是<code>\</code>，字段值以单引号<code>'</code>包含起来。</td>
</tr>

<tr>
<td align="left"><code>--optionally-enclosed-by &lt;char&gt;</code></td>
<td align="left">enclosed-by是强制给每个字段值前后都加上指定的符号，而<code>--optionally-enclosed-by</code>只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的</td>
</tr>
</tbody>
</table>

<h1 id="2-3-创建-hive-表">2.3 创建 hive 表</h1>

<p>生成与关系数据库表的表结构对应的HIVE表：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop create-hive-table --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS </code></pre></td></tr></table>
</div>
</div>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><code>--hive-home &lt;dir&gt;</code></td>
<td align="left">Hive的安装目录，可以通过该参数覆盖掉默认的hive目录</td>
</tr>

<tr>
<td align="left"><code>--hive-overwrite</code></td>
<td align="left">覆盖掉在hive表中已经存在的数据</td>
</tr>

<tr>
<td align="left"><code>--create-hive-table</code></td>
<td align="left">默认是false，如果目标表已经存在了，那么创建任务会失败</td>
</tr>

<tr>
<td align="left"><code>--hive-table</code></td>
<td align="left">后面接要创建的hive表</td>
</tr>

<tr>
<td align="left"><code>--table</code></td>
<td align="left">指定关系数据库表名</td>
</tr>
</tbody>
</table>

<h1 id="2-4-导入数据到-hive">2.4 导入数据到 hive</h1>

<p>执行下面的命令会将 mysql 中的数据导入到 hdfs 中，然后创建一个hive 表，最后再将 hdfs 上的文件移动到 hive 表的目录下面。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&#34;\t&#34;</span> --lines-terminated-by <span class="s2">&#34;\n&#34;</span> --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --delete-target-dir</code></pre></td></tr></table>
</div>
</div>
<p>说明：</p>

<ul>
<li>可以在 hive 的表名前面指定数据库名称</li>
<li>可以通过 <code>--create-hive-table</code> 创建表，如果表已经存在则会执行失败</li>
</ul>

<p>接下来可以查看 hive 中的数据：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ hive -e <span class="s1">&#39;select * from dw_srclog.tbls&#39;</span>
<span class="m">34</span>	<span class="m">1406784308</span>	<span class="m">8</span>	<span class="m">0</span>	root	<span class="m">0</span>	<span class="m">45</span>	test1	EXTERNAL_TABLE	null	null	NULL
<span class="m">40</span>	<span class="m">1406797005</span>	<span class="m">9</span>	<span class="m">0</span>	root	<span class="m">0</span>	<span class="m">52</span>	test2	EXTERNAL_TABLE	null	null	NULL
<span class="m">42</span>	<span class="m">1407122307</span>	<span class="m">7</span>	<span class="m">0</span>	root	<span class="m">0</span>	<span class="m">59</span>	test3	EXTERNAL_TABLE	null	null	NULL</code></pre></td></tr></table>
</div>
</div>
<p>直接查看文件内容：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ hadoop fs -cat /user/hive/warehouse/dw_srclog.db/tbls/part-m-00000
34140678430880root045go_goodsEXTERNAL_TABLEnullnullnull
40140679700590root052merchantEXTERNAL_TABLEnullnullnull</code></pre></td></tr></table>
</div>
</div>
<p>从上面可见，数据导入到 hive 中之后分隔符为默认分隔符，参考上文你可以通过设置参数指定其他的分隔符。</p>

<p>另外，Sqoop 默认地导入空值（NULL）为 null 字符串，而 hive 使用 \N 去标识空值（NULL），故你在 import 或者 export 时候，需要做相应的处理。在 import 时，使用如下命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import  ... --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span></code></pre></td></tr></table>
</div>
</div>
<p>在导出时，使用下面命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import  ... --input-null-string <span class="s1">&#39;&#39;</span> --input-null-non-string <span class="s1">&#39;&#39;</span></code></pre></td></tr></table>
</div>
</div>
<p>一个完整的例子如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sqoop import --connect jdbc:mysql://192.168.56.121:3306/metastore --username hiveuser --password redhat --table TBLS --fields-terminated-by <span class="s2">&#34;\t&#34;</span> --lines-terminated-by <span class="s2">&#34;\n&#34;</span> --hive-import --hive-overwrite --create-hive-table --hive-table dw_srclog.TBLS --null-string <span class="s1">&#39;\\N&#39;</span> --null-non-string <span class="s1">&#39;\\N&#39;</span> --compression-codec <span class="s2">&#34;com.hadoop.compression.lzo.LzopCodec&#34;</span></code></pre></td></tr></table>
</div>
</div>
<h1 id="2-5-增量导入">2.5 增量导入</h1>

<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><code>--check-column (col)</code></td>
<td align="left">用来作为判断的列名，如id</td>
</tr>

<tr>
<td align="left"><code>--incremental (mode)</code></td>
<td align="left">append：追加，比如对大于last-value指定的值之后的记录进行追加导入。lastmodified：最后的修改时间，追加last-value指定的日期之后的记录</td>
</tr>

<tr>
<td align="left"><code>--last-value (value)</code></td>
<td align="left">指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值</td>
</tr>
</tbody>
</table>

<h1 id="2-6-合并-hdfs-文件">2.6 合并 hdfs 文件</h1>

<p>将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中，示例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sqoop merge –new-data /test/p1/person –onto /test/p2/person –target-dir /test/merged –jar-file /opt/data/sqoop/person/Person.jar –class-name Person –merge-key id</code></pre></td></tr></table>
</div>
</div>
<p>其中，<code>–class-name</code> 所指定的 class 名是对应于 Person.jar 中的 Person 类，而 Person.jar 是通过 Codegen 生成的</p>

<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">说明</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left"><code>--new-data &lt;path&gt;</code></td>
<td align="left">Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能优先保留的，原则上一般是存放越新数据的目录就对应这个参数。</td>
</tr>

<tr>
<td align="left"><code>--onto &lt;path&gt;</code></td>
<td align="left">Hdfs中存放数据的一个目录，该目录中的数据是希望在合并后能被更新数据替换掉的，原则上一般是存放越旧数据的目录就对应这个参数。</td>
</tr>

<tr>
<td align="left"><code>--merge-key &lt;col&gt;</code></td>
<td align="left">合并键，一般是主键ID</td>
</tr>

<tr>
<td align="left"><code>--jar-file &lt;file&gt;</code></td>
<td align="left">合并时引入的jar包，该jar包是通过Codegen工具生成的jar包</td>
</tr>

<tr>
<td align="left"><code>--class-name &lt;class&gt;</code></td>
<td align="left">对应的表名或对象名，该class类是包含在jar包中的。</td>
</tr>

<tr>
<td align="left"><code>--target-dir &lt;path&gt;</code></td>
<td align="left">合并后的数据在HDFS里的存放目录</td>
</tr>
</tbody>
</table>

<h1 id="3-参考文章">3. 参考文章</h1>

<ul>
<li><a href="http://www.zihou.me/html/2014/01/28/9114.html">Sqoop中文手册</a></li>
</ul>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">JavaChen</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2014-08-04
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<div class="post-reward">
  
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/wechatpay.jpg">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/alipay.jpg">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/sqoop/">sqoop</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2014/08/19/upgrading-from-cdh4-to-cdh5/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">升级cdh4到cdh5</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/2014/07/31/summary-of-july-in-2014/">
            <span class="next-text nav-default">2014年7月总结</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="junetalk/junetalk.github.io"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:junecloud@163.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/javachen" class="iconfont icon-github" title="github"></a>
      <a href="http://weibo.com/chenzhijun" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://space.bilibili.com/287563020/" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://blog.javachen.space/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2009 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">JavaChen</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.e1476869.min.js"></script>

<script id="baidu_analytics">
  var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?7eaf37274cf8796df56903a88389e82f";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
