<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>使用Scrapy抓取数据 - JavaChen Blog - Ramblings of a coder</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="JavaChen" /><meta name="description" content="Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。" /><meta name="keywords" content="scrapy,python" />


<meta name="baidu-site-verification" content="OMsbiDfo1G" />



<meta name="generator" content="Hugo 0.58.3 with theme even" />


<link rel="canonical" href="http://localhost:1313/2014/05/24/using-scrapy-to-cralw-data/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.5d87ca31.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">
<link rel="stylesheet" href="/css/custom.css">


<meta property="og:title" content="使用Scrapy抓取数据" />
<meta property="og:description" content="Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/2014/05/24/using-scrapy-to-cralw-data/" />
<meta property="article:published_time" content="2014-05-24T08:00:00+08:00" />
<meta property="article:modified_time" content="2014-05-24T08:00:00+08:00" />
<meta itemprop="name" content="使用Scrapy抓取数据">
<meta itemprop="description" content="Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。">


<meta itemprop="datePublished" content="2014-05-24T08:00:00&#43;08:00" />
<meta itemprop="dateModified" content="2014-05-24T08:00:00&#43;08:00" />
<meta itemprop="wordCount" content="4603">



<meta itemprop="keywords" content="scrapy," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="使用Scrapy抓取数据"/>
<meta name="twitter:description" content="Scrapy是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">JavaChen Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">JavaChen Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">使用Scrapy抓取数据</h1>

      <div class="post-meta">
        <span class="post-time"> 2014-05-24 </span>
        <div class="post-category">
            <a href="/categories/python/"> python </a>
            </div>
          <span class="more-meta"> 约 4603 字 </span>
          <span class="more-meta"> 预计阅读 10 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#1-安装">1. 安装</a>
<ul>
<li><a href="#安装-python">安装 python</a></li>
<li><a href="#安装">安装</a></li>
<li><a href="#安装-zope-interface">安装 zope.interface</a></li>
<li><a href="#安装-twisted">安装 twisted</a></li>
<li><a href="#安装-pyopenssl">安装 pyOpenSSL</a></li>
<li><a href="#安装-scrapy">安装 Scrapy</a></li>
</ul></li>
<li><a href="#2-使用-scrapy">2. 使用 Scrapy</a>
<ul>
<li><a href="#新建工程">新建工程</a></li>
<li><a href="#定义item">定义Item</a></li>
<li><a href="#编写爬虫-spider">编写爬虫(Spider)</a></li>
<li><a href="#运行项目">运行项目</a></li>
<li><a href="#xpath选择器">xpath选择器</a></li>
<li><a href="#提取数据">提取数据</a></li>
<li><a href="#使用item">使用Item</a></li>
<li><a href="#使用item-pipeline">使用Item Pipeline</a></li>
<li><a href="#保存抓取的数据">保存抓取的数据</a></li>
<li><a href="#总结">总结</a></li>
</ul></li>
<li><a href="#3-学习资料">3. 学习资料</a></li>
<li><a href="#4-总结">4. 总结</a></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<p>Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>

<ul>
<li>官方主页： <a href="http://www.scrapy.org/">http://www.scrapy.org/</a></li>
<li>中文文档：<a href="http://scrapy-chs.readthedocs.org/zh_CN/latest/index.html">Scrapy 0.22 文档</a></li>
<li>GitHub项目主页：<a href="https://github.com/scrapy/scrapy">https://github.com/scrapy/scrapy</a></li>
</ul>

<p>Scrapy 使用了 Twisted 异步网络库来处理网络通讯。整体架构大致如下（注：图片来自互联网）：</p>

<p><img src="http://blog.pluskid.org/wp-content/uploads/2009/08/scrapy_architecture.png" alt="scrapy" /></p>

<p>Scrapy主要包括了以下组件：</p>

<ul>
<li>引擎，用来处理整个系统的数据流处理，触发事务。</li>
<li>调度器，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。</li>
<li>下载器，用于下载网页内容，并将网页内容返回给蜘蛛。</li>
<li>蜘蛛，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。</li>
<li>项目管道，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li>下载器中间件，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li>蜘蛛中间件，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li>调度中间件，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>

<p>使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。</p>

<h1 id="1-安装">1. 安装</h1>

<h2 id="安装-python">安装 python</h2>

<p>Scrapy 目前最新版本为0.22.2，该版本需要 python 2.7，故需要先安装 python 2.7。这里我使用 centos 服务器来做测试，因为系统自带了 python ，需要先检查 python 版本。</p>

<p>查看python版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ python -V
Python <span class="m">2</span>.6.6</code></pre></td></tr></table>
</div>
</div>
<p>升级版本到2.7：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ wget http://python.org/ftp/python/2.7.6/Python-2.7.6.tar.xz
$ tar xf Python-2.7.6.tar.xz
$ <span class="nb">cd</span> Python-2.7.6
$ ./configure --prefix<span class="o">=</span>/usr/local --enable-unicode<span class="o">=</span>ucs4 --enable-shared <span class="nv">LDFLAGS</span><span class="o">=</span><span class="s2">&#34;-Wl,-rpath /usr/local/lib&#34;</span>
$ make <span class="o">&amp;&amp;</span> make altinstall</code></pre></td></tr></table>
</div>
</div>
<p>建立软连接，使系统默认的 python指向 python2.7</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ mv /usr/bin/python /usr/bin/python2.6.6 
$ ln -s /usr/local/bin/python2.7 /usr/bin/python </code></pre></td></tr></table>
</div>
</div>
<p>再次查看python版本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ python -V
Python <span class="m">2</span>.7.6</code></pre></td></tr></table>
</div>
</div>
<h2 id="安装">安装</h2>

<p>这里使用 wget 的方式来安装 <a href="http://pypi.python.org/pypi/setuptools">setuptools</a> :</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ wget https://bootstrap.pypa.io/ez_setup.py -O - <span class="p">|</span> python</code></pre></td></tr></table>
</div>
</div>
<h2 id="安装-zope-interface">安装 zope.interface</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ easy_install zope.interface</code></pre></td></tr></table>
</div>
</div>
<h2 id="安装-twisted">安装 twisted</h2>

<p>Scrapy 使用了 Twisted 异步网络库来处理网络通讯，故需要安装 twisted。</p>

<p>安装 twisted 前，需要先安装 gcc：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ yum install gcc -y</code></pre></td></tr></table>
</div>
</div>
<p>然后，再通过 easy_install 安装 twisted：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ easy_install twisted</code></pre></td></tr></table>
</div>
</div>
<p>如果出现下面错误：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ easy_install twisted
Searching <span class="k">for</span> twisted
Reading https://pypi.python.org/simple/twisted/
Best match: Twisted <span class="m">14</span>.0.0
Downloading https://pypi.python.org/packages/source/T/Twisted/Twisted-14.0.0.tar.bz2#md5<span class="o">=</span>9625c094e0a18da77faa4627b98c9815
Processing Twisted-14.0.0.tar.bz2
Writing /tmp/easy_install-kYHKjn/Twisted-14.0.0/setup.cfg
Running Twisted-14.0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-kYHKjn/Twisted-14.0.0/egg-dist-tmp-vu1n6Y
twisted/runner/portmap.c:10:20: error: Python.h: No such file or directory
twisted/runner/portmap.c:14: error: expected ‘<span class="o">=</span>’, ‘,’, ‘<span class="p">;</span>’, ‘asm’ or ‘__attribute__’ before ‘*’ token
twisted/runner/portmap.c:31: error: expected ‘<span class="o">=</span>’, ‘,’, ‘<span class="p">;</span>’, ‘asm’ or ‘__attribute__’ before ‘*’ token
twisted/runner/portmap.c:45: error: expected ‘<span class="o">=</span>’, ‘,’, ‘<span class="p">;</span>’, ‘asm’ or ‘__attribute__’ before ‘PortmapMethods’
twisted/runner/portmap.c: In <span class="k">function</span> ‘initportmap’:
twisted/runner/portmap.c:55: warning: implicit declaration of <span class="k">function</span> ‘Py_InitModule’
twisted/runner/portmap.c:55: error: ‘PortmapMethods’ undeclared <span class="o">(</span>first use in this <span class="k">function</span><span class="o">)</span>
twisted/runner/portmap.c:55: error: <span class="o">(</span>Each undeclared identifier is reported only once
twisted/runner/portmap.c:55: error: <span class="k">for</span> each <span class="k">function</span> it appears in.<span class="o">)</span></code></pre></td></tr></table>
</div>
</div>
<p>请安装 python-devel 然后再次运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ yum install python-devel -y
$ easy_install twisted</code></pre></td></tr></table>
</div>
</div>
<p>如果出现下面异常：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">error: Not a recognized archive type: /tmp/easy_install-tVwC5O/Twisted-14.0.0.tar.bz2</pre></td></tr></table>
</div>
</div>
<p>请手动下载然后安装，下载地址在<a href="http://pypi.python.org/pypi/Twisted">这里</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ wget https://pypi.python.org/packages/source/T/Twisted/Twisted-14.0.0.tar.bz2#md5<span class="o">=</span>9625c094e0a18da77faa4627b98c9815
$ tar -vxjf Twisted-14.0.0.tar.bz2
$ <span class="nb">cd</span> Twisted-14.0.0
$ python setup.py install</code></pre></td></tr></table>
</div>
</div>
<h2 id="安装-pyopenssl">安装 pyOpenSSL</h2>

<p>先安装一些依赖：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ yum install libffi libffi-devel openssl-devel -y</code></pre></td></tr></table>
</div>
</div>
<p>然后，再通过 easy_install 安装 pyOpenSSL：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ easy_install pyOpenSSL</code></pre></td></tr></table>
</div>
</div>
<h2 id="安装-scrapy">安装 Scrapy</h2>

<p>先安装一些依赖：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ yum install libxml2 libxslt libxslt-devel -y</code></pre></td></tr></table>
</div>
</div>
<p>最后再来安装 Scrapy ：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ easy_install scrapy</code></pre></td></tr></table>
</div>
</div>
<h1 id="2-使用-scrapy">2. 使用 Scrapy</h1>

<p>在安装成功之后，你可以了解一些 Scrapy 的基本概念和使用方法，并学习 Scrapy 项目的例子 dirbot 。</p>

<p>Dirbot 项目位于 <a href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a>，该项目包含一个 README 文件，它详细描述了项目的内容。如果你熟悉 git，你可以 checkout 它的源代码。或者你可以通过点击 Downloads 下载 tarball 或 zip 格式的文件。</p>

<p>下面以该例子来描述如何使用 Scrapy 创建一个爬虫项目。</p>

<h2 id="新建工程">新建工程</h2>

<p>在抓取之前，你需要新建一个 Scrapy 工程。进入一个你想用来保存代码的目录，然后执行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ scrapy startproject tutorial</code></pre></td></tr></table>
</div>
</div>
<p>这个命令会在当前目录下创建一个新目录 tutorial，它的结构如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></pre></td>
<td class="lntd">
<pre class="chroma">.
├── scrapy.cfg
└── tutorial
    ├── __init__.py
    ├── items.py
    ├── pipelines.py
    ├── settings.py
    └── spiders
        └── __init__.py</pre></td></tr></table>
</div>
</div>
<p>这些文件主要是：</p>

<ul>
<li>scrapy.cfg: 项目配置文件</li>
<li>tutorial/: 项目python模块, 呆会代码将从这里导入</li>
<li>tutorial/items.py: 项目items文件</li>
<li>tutorial/pipelines.py: 项目管道文件</li>
<li>tutorial/settings.py: 项目配置文件</li>
<li>tutorial/spiders: 放置spider的目录</li>
</ul>

<h2 id="定义item">定义Item</h2>

<p>Items是将要装载抓取的数据的容器，它工作方式像 python 里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误。</p>

<p>它通过创建一个 <code>scrapy.item.Item</code> 类来声明，定义它的属性为 <code>scrpy.item.Field</code> 对象，就像是一个对象关系映射(ORM).
我们通过将需要的item模型化，来控制从 dmoz.org 获得的站点数据，比如我们要获得站点的名字，url 和网站描述，我们定义这三种属性的域。要做到这点，我们编辑在 tutorial 目录下的 items.py 文件，我们的 Item 类将会是这样</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scrapy.item</span> <span class="kn">import</span> <span class="n">Item</span><span class="p">,</span> <span class="n">Field</span> 
<span class="k">class</span> <span class="nc">DmozItem</span><span class="p">(</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span></code></pre></td></tr></table>
</div>
</div>
<p>刚开始看起来可能会有些困惑，但是定义这些 item 能让你用其他 Scrapy 组件的时候知道你的 items 到底是什么。</p>

<h2 id="编写爬虫-spider">编写爬虫(Spider)</h2>

<p>Spider 是用户编写的类，用于从一个域（或域组）中抓取信息。们定义了用于下载的URL的初步列表，如何跟踪链接，以及如何来解析这些网页的内容用于提取items。</p>

<p>要建立一个 Spider，你可以为 <code>scrapy.spider.BaseSpider</code> 创建一个子类，并确定三个主要的、强制的属性：</p>

<ul>
<li><code>name</code>：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字.</li>
<li><code>start_urls</code>：爬虫开始爬的一个 URL 列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些 URLS 开始。其他子 URL 将会从这些起始 URL 中继承性生成。</li>
<li><code>parse()</code>：爬虫的方法，调用时候传入从每一个 URL 传回的 Response 对象作为参数，response 将会是 parse 方法的唯一的一个参数,</li>
</ul>

<p>这个方法负责解析返回的数据、匹配抓取的数据(解析为 item )并跟踪更多的 URL。</p>

<p>在 tutorial/spiders 目录下创建 DmozSpider.py</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scrapy.spider</span> <span class="kn">import</span> <span class="n">BaseSpider</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">BaseSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;dmoz&#34;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;dmoz.org&#34;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&#34;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&#34;</span><span class="p">,</span>
        <span class="s2">&#34;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&#34;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;/&#34;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span></code></pre></td></tr></table>
</div>
</div>
<h2 id="运行项目">运行项目</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ scrapy crawl dmoz</code></pre></td></tr></table>
</div>
</div>
<p>该命令从 dmoz.org 域启动爬虫，第三个参数为 DmozSpider.py 中的 name 属性值。</p>

<h2 id="xpath选择器">xpath选择器</h2>

<p>Scrapy 使用一种叫做 XPath selectors 的机制，它基于 XPath 表达式。如果你想了解更多selectors和其他机制你可以查阅<a href="http://doc.scrapy.org/topics/selectors.html#topics-selectors">资料</a>。</p>

<p>这是一些XPath表达式的例子和他们的含义：</p>

<ul>
<li><code>/html/head/title</code>: 选择HTML文档 <code>&lt;head&gt;</code> 元素下面的 <code>&lt;title&gt;</code> 标签。</li>
<li><code>/html/head/title/text()</code>: 选择前面提到的<code>&lt;title&gt;</code> 元素下面的文本内容</li>
<li><code>//td</code>: 选择所有 <code>&lt;td&gt;</code> 元素</li>
<li><code>//div[@class=&quot;mine&quot;]</code>: 选择所有包含 <code>class=&quot;mine&quot;</code> 属性的div 标签元素</li>
</ul>

<p>这只是几个使用 XPath 的简单例子，但是实际上 XPath 非常强大。如果你想了解更多 XPATH 的内容，我们向你推荐这个 <a href="http://www.w3schools.com/XPath/default.asp">XPath 教程</a></p>

<p>为了方便使用 XPaths，Scrapy 提供 Selector 类， 有三种方法</p>

<ul>
<li><code>xpath()</code>：返回selectors列表, 每一个select表示一个xpath参数表达式选择的节点.</li>
<li><code>extract()</code>：返回一个unicode字符串，该字符串为XPath选择器返回的数据</li>
<li><code>re()</code>： 返回unicode字符串列表，字符串作为参数由正则表达式提取出来</li>
<li><code>css()</code></li>
</ul>

<h2 id="提取数据">提取数据</h2>

<p>我们可以通过如下命令选择每个在网站中的 <code>&lt;li&gt;</code> 元素:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li&#39;</span><span class="p">)</span> </code></pre></td></tr></table>
</div>
</div>
<p>然后是网站描述:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span></code></pre></td></tr></table>
</div>
</div>
<p>网站标题:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span></code></pre></td></tr></table>
</div>
</div>
<p>网站链接:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span></code></pre></td></tr></table>
</div>
</div>
<p>如前所述，每个 <code>xpath()</code> 调用返回一个 selectors 列表，所以我们可以结合 <code>xpath()</code> 去挖掘更深的节点。我们将会用到这些特性，所以:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sites</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">site</span> <span class="ow">in</span> <span class="n">sites</span><span class="p">:</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">site</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">site</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">site</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
    <span class="k">print</span> <span class="n">title</span><span class="p">,</span> <span class="n">link</span><span class="p">,</span> <span class="n">desc</span></code></pre></td></tr></table>
</div>
</div>
<h2 id="使用item">使用Item</h2>

<p><code>scrapy.item.Item</code> 的调用接口类似于 python 的 dict ，Item 包含多个 <code>scrapy.item.Field</code>。这跟 django 的 Model 与</p>

<p>Item 通常是在 Spider 的 parse 方法里使用，它用来保存解析到的数据。</p>

<p>最后修改爬虫类，使用 Item 来保存数据，代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scrapy.spider</span> <span class="kn">import</span> <span class="n">Spider</span>
<span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>

<span class="kn">from</span> <span class="nn">dirbot.items</span> <span class="kn">import</span> <span class="n">Website</span>


<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;dmoz&#34;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;dmoz.org&#34;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&#34;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&#34;</span><span class="p">,</span>
        <span class="s2">&#34;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&#34;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="s2">&#34;&#34;&#34;
</span><span class="s2">        The lines below is a spider contract. For more info see:
</span><span class="s2">        http://doc.scrapy.org/en/latest/topics/contracts.html
</span><span class="s2">
</span><span class="s2">        @url http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/
</span><span class="s2">        @scrapes name
</span><span class="s2">        &#34;&#34;&#34;</span>
        <span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="n">sites</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul[@class=&#34;directory-url&#34;]/li&#39;</span><span class="p">)</span>
        <span class="n">items</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">site</span> <span class="ow">in</span> <span class="n">sites</span><span class="p">:</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">Website</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">site</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">site</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">site</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s1">&#39;-\s([^</span><span class="se">\n</span><span class="s1">]*?)</span><span class="se">\\</span><span class="s1">n&#39;</span><span class="p">)</span>
            <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">items</span></code></pre></td></tr></table>
</div>
</div>
<p>现在，可以再次运行该项目查看运行结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ scrapy crawl dmoz</code></pre></td></tr></table>
</div>
</div>
<h2 id="使用item-pipeline">使用Item Pipeline</h2>

<p>在 settings.py 中设置 <code>ITEM_PIPELINES</code>，其默认为<code>[]</code>，与 django 的 <code>MIDDLEWARE_CLASSES</code> 等相似。
从 Spider 的 parse 返回的 Item 数据将依次被 <code>ITEM_PIPELINES</code> 列表中的 Pipeline 类处理。</p>

<p>一个 Item Pipeline 类必须实现以下方法：</p>

<ul>
<li><code>process_item(item, spider)</code> 为每个 item pipeline 组件调用，并且需要返回一个 <code>scrapy.item.Item</code> 实例对象或者抛出一个 <code>scrapy.exceptions.DropItem</code> 异常。当抛出异常后该 item 将不会被之后的 pipeline 处理。参数:

<ul>
<li><code>item (Item object)</code> – 由 parse 方法返回的 Item 对象</li>
<li><code>spider (BaseSpider object)</code> – 抓取到这个 Item 对象对应的爬虫对象</li>
</ul></li>
</ul>

<p>也可额外的实现以下两个方法：</p>

<ul>
<li><code>open_spider(spider)</code> 当爬虫打开之后被调用。参数: <code>spider (BaseSpider object)</code> – 已经运行的爬虫</li>
<li><code>close_spider(spider)</code> 当爬虫关闭之后被调用。参数: <code>spider (BaseSpider object)</code> – 已经关闭的爬虫</li>
</ul>

<h2 id="保存抓取的数据">保存抓取的数据</h2>

<p>保存信息的最简单的方法是通过 <a href="http://doc.scrapy.org/en/0.22/topics/feed-exports.html#topics-feed-exports">Feed exports</a>，命令如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ scrapy crawl dmoz -o items.json -t json</code></pre></td></tr></table>
</div>
</div>
<p>除了 json 格式之外，还支持 JSON lines、CSV、XML格式，你也可以通过接口扩展一些格式。</p>

<p>对于小项目用这种方法也足够了。如果是比较复杂的数据的话可能就需要编写一个 Item Pipeline 进行处理了。</p>

<p>所有抓取的 items 将以 JSON 格式被保存在新生成的 items.json 文件中</p>

<h2 id="总结">总结</h2>

<p>上面描述了如何创建一个爬虫项目的过程，你可以参照上面过程联系一遍。作为学习的例子，你还可以参考这篇文章：<a href="http://wsky.org/archives/191.html">scrapy 中文教程（爬cnbeta实例）</a> 。</p>

<p>这篇文章中的爬虫类代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scrapy.contrib.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.contrib.linkextractors.sgml</span> <span class="kn">import</span> <span class="n">SgmlLinkExtractor</span>
<span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
 
<span class="kn">from</span> <span class="nn">cnbeta.items</span> <span class="kn">import</span> <span class="n">CnbetaItem</span>
 
<span class="k">class</span> <span class="nc">CBSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;cnbeta&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cnbeta.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.cnbeta.com&#39;</span><span class="p">]</span>
 
    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">SgmlLinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;/articles/.*\.htm&#39;</span><span class="p">,</span> <span class="p">)),</span>
             <span class="n">callback</span><span class="o">=</span><span class="s1">&#39;parse_page&#39;</span><span class="p">,</span> <span class="n">follow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="p">)</span>
 
    <span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">CnbetaItem</span><span class="p">()</span>
        <span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
        <span class="k">return</span> <span class="n">item</span></code></pre></td></tr></table>
</div>
</div>
<p>需要说明的是：</p>

<ul>
<li>该爬虫类继承的是 <code>CrawlSpider</code> 类，并且定义规则，rules指定了含有 <code>/articles/.*\.htm</code> 的链接都会被匹配。</li>
<li>该类并没有实现parse方法，并且规则中定义了回调函数 <code>parse_page</code>，你可以参考更多资料了解 CrawlSpider 的用法</li>
</ul>

<h1 id="3-学习资料">3. 学习资料</h1>

<p>接触 Scrapy，是因为想爬取一些知乎的数据，最开始的时候搜索了一些相关的资料和别人的实现方式。</p>

<p>Github 上已经有人或多或少的实现了对知乎数据的爬取，我搜索到的有以下几个仓库：</p>

<ul>
<li><a href="https://github.com/KeithYue/Zhihu_Spider">https://github.com/KeithYue/Zhihu_Spider</a> 实现先通过用户名和密码登陆再爬取数据，代码见 <a href="https://github.com/KeithYue/Zhihu_Spider/blob/master/zhihu/zhihu/spiders/zhihu_spider.py">zhihu_spider.py</a>。</li>
<li><a href="https://github.com/immzz/zhihu-scrapy">https://github.com/immzz/zhihu-scrapy</a> 使用 selenium 下载和执行 javascript 代码。</li>
<li><a href="https://github.com/tangerinewhite32/zhihu-stat-py">https://github.com/tangerinewhite32/zhihu-stat-py</a></li>
<li><a href="https://github.com/Zcc/zhihu">https://github.com/Zcc/zhihu</a> 主要是爬指定话题的topanswers，还有用户个人资料，添加了登录代码。</li>
<li><a href="https://github.com/pelick/VerticleSearchEngine">https://github.com/pelick/VerticleSearchEngine</a> 基于爬取的学术资源，提供搜索、推荐、可视化、分享四块。使用了 Scrapy、MongoDB、Apache Lucene/Solr、Apache Tika等技术。</li>
<li><a href="https://github.com/geekan/scrapy-examples">https://github.com/geekan/scrapy-examples</a> scrapy的一些例子，包括获取豆瓣数据、linkedin、腾讯招聘数据等例子。</li>
<li><a href="https://github.com/owengbs/deeplearning">https://github.com/owengbs/deeplearning</a> 实现分页获取话题。</li>
<li><a href="https://github.com/gnemoug/distribute_crawler">https://github.com/gnemoug/distribute_crawler</a> 使用scrapy、redis、mongodb、graphite实现的一个分布式网络爬虫,底层存储mongodb集群,分布式使用redis实现,爬虫状态显示使用graphite实现</li>
<li><a href="https://github.com/weizetao/spider-roach">https://github.com/weizetao/spider-roach</a> 一个分布式定向抓取集群的简单实现。</li>
<li><a href="https://github.com/scrapinghub/portia">https://github.com/scrapinghub/portia</a> 这是一个可视化爬虫，基于Scrapy。它提供了可视化操作的Web页面，你只需点击页面上你要抽取的数据就行</li>
<li><a href="https://github.com/binux/pyspider">https://github.com/binux/pyspider</a> 你如果不喜欢 Scrapy，可以试试 pyspider ，他让你在 WEB 界面编写调试脚本，监控执行状态，查看历史和结果 ，你可以在线试下 demo：<a href="http://demo.pyspider.org/">Dashboard - pyspider</a></li>
</ul>

<p>其他资料：</p>

<ul>
<li><a href="http://www.52ml.net/tags/Scrapy">http://www.52ml.net/tags/Scrapy</a> 收集了很多关于 Scrapy 的文章，<strong>推荐阅读</strong></li>
<li><a href="http://zihaolucky.github.io/using-python-to-build-zhihu-cralwer/">用Python Requests抓取知乎用户信息</a></li>
<li><a href="http://www.it165.net/pro/html/images05/13112.html">使用scrapy框架爬取自己的博文</a></li>
<li><a href="http://github.windwild.net/images/03/scrapy002/">Scrapy 深入一点点</a></li>
<li><a href="http://www.kankanews.com/ICkengine/archives/94817.shtml">使用python，scrapy写（定制）爬虫的经验，资料，杂。</a></li>
<li><a href="http://blog.pluskid.org/?p=366&amp;cpage=1">Scrapy 轻松定制网络爬虫</a></li>
<li><a href="http://my.oschina.net/chengye/blog/124162">在scrapy中怎么让Spider自动去抓取豆瓣小组页面</a></li>
</ul>

<p>scrapy 和 javascript 交互例子：</p>

<ul>
<li><a href="http://www.xuebuyuan.com/2017949.html">用scrapy框架爬取js交互式表格数据</a></li>
<li><a href="http://wsky.org/archives/211.html">scrapy + selenium 解析javascript 实例</a></li>
</ul>

<p>还有一些待整理的知识点：</p>

<ul>
<li><em>如何先登陆再爬数据</em></li>
<li><em>如何使用规则做过滤</em></li>
<li><em>如何递归爬取数据</em></li>
<li><em>scrapy的参数设置和优化</em></li>
<li><em>如何实现分布式爬取</em></li>
</ul>

<h1 id="4-总结">4. 总结</h1>

<p>以上就是最近几天学习 Scrapy 的一个笔记和知识整理，参考了一些网上的文章才写成此文，对此表示感谢，也希望这篇文章能够对你有所帮助。如果你有什么想法，欢迎留言；如果喜欢此文，请帮忙分享，谢谢!</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">JavaChen</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2014-05-24
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/wechatpay.jpg">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/alipay.jpg">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/scrapy/">scrapy</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2014/05/30/note-about-brewers-cap-theorem/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">关于CAP理论的一些笔记</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/2014/05/20/nutch-intro/">
            <span class="next-text nav-default">Nutch介绍及使用</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="javachen/javachen.github.io"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:junecloud@163.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/javachen" class="iconfont icon-github" title="github"></a>
      <a href="http://weibo.com/chenzhijun" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://space.bilibili.com/287563020/" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="http://localhost:1313/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2009 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">JavaChen</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.20b54c22.min.js"></script>

<script id="baidu_analytics">
  var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?7eaf37274cf8796df56903a88389e82f";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>






</body>
</html>
