<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>安装单节点Ceph集群 - JavaChen Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="JavaChen" /><meta name="description" content="本文主要记录在虚拟机中安装Ceph单节点集群的过程，其实多配置几个节点，也就是安装集群的过程。 Ceph中午文档：http://docs.ce" /><meta name="keywords" content="Java, Hadoop, Docker, Kubernetes" />


<meta name="baidu-site-verification" content="OMsbiDfo1G" />



<meta name="generator" content="Hugo 0.54.0 with theme even" />


<link rel="canonical" href="https://blog.javachen.space/2019/11/24/install-ceph-sinle-node/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<link href="/dist/even.b90a1cc1.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">
<link rel="stylesheet" href="/css/custom.css">


<meta property="og:title" content="安装单节点Ceph集群" />
<meta property="og:description" content="本文主要记录在虚拟机中安装Ceph单节点集群的过程，其实多配置几个节点，也就是安装集群的过程。 Ceph中午文档：http://docs.ce" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.javachen.space/2019/11/24/install-ceph-sinle-node/" />
<meta property="article:published_time" content="2019-11-24T08:00:00&#43;08:00"/>
<meta property="article:modified_time" content="2019-11-24T08:00:00&#43;08:00"/>

<meta itemprop="name" content="安装单节点Ceph集群">
<meta itemprop="description" content="本文主要记录在虚拟机中安装Ceph单节点集群的过程，其实多配置几个节点，也就是安装集群的过程。 Ceph中午文档：http://docs.ce">


<meta itemprop="datePublished" content="2019-11-24T08:00:00&#43;08:00" />
<meta itemprop="dateModified" content="2019-11-24T08:00:00&#43;08:00" />
<meta itemprop="wordCount" content="7515">



<meta itemprop="keywords" content="ceph," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="安装单节点Ceph集群"/>
<meta name="twitter:description" content="本文主要记录在虚拟机中安装Ceph单节点集群的过程，其实多配置几个节点，也就是安装集群的过程。 Ceph中午文档：http://docs.ce"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">JavaChen Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">JavaChen Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">安装单节点Ceph集群</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-11-24 </span>
        <div class="post-category">
            <a href="/categories/devops/"> devops </a>
            </div>
          <span class="more-meta"> 约 7515 字 </span>
          <span class="more-meta"> 预计阅读 15 分钟 </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#集群规划">集群规划</a></li>
<li><a href="#准备环境">准备环境</a>
<ul>
<li><a href="#设置hosts">设置hosts</a></li>
<li><a href="#创建部署-ceph-的用户">创建部署 CEPH 的用户</a></li>
<li><a href="#允许无密码-ssh-登录">允许无密码 SSH 登录</a></li>
<li><a href="#配置时钟同步">配置时钟同步</a></li>
<li><a href="#设置selinux">设置SELINUX</a></li>
<li><a href="#设置tty">设置TTY</a></li>
<li><a href="#安装yum-plugin-priorities">安装yum-plugin-priorities</a></li>
<li><a href="#加载rbd模块">加载rbd模块</a></li>
</ul></li>
<li><a href="#创建集群">创建集群</a>
<ul>
<li><a href="#安装-ceph-deploy">安装 Ceph-Deploy</a></li>
<li><a href="#创建配置目录">创建配置目录</a></li>
<li><a href="#创建集群-1">创建集群</a></li>
<li><a href="#安装-ceph-到各节点">安装 Ceph 到各节点</a></li>
<li><a href="#初始化-monitor">初始化 Monitor</a></li>
<li><a href="#初始化磁盘">初始化磁盘</a></li>
<li><a href="#创建-osd-存储节点">创建 OSD 存储节点</a></li>
<li><a href="#分配-admin-配置文件">分配 admin 配置文件</a></li>
<li><a href="#查看集群状态">查看集群状态</a></li>
<li><a href="#部署-mgr">部署 MGR</a></li>
<li><a href="#安装-dashboard">安装 Dashboard</a></li>
<li><a href="#清除集群">清除集群</a></li>
</ul></li>
<li><a href="#配置调优">配置调优</a></li>
<li><a href="#集群扩容">集群扩容</a>
<ul>
<li><a href="#添加-osd">添加 OSD</a></li>
<li><a href="#添加-monitor">添加 Monitor</a></li>
<li><a href="#添加-mds">添加 MDS</a></li>
<li><a href="#添加-rgw">添加 RGW</a></li>
</ul></li>
<li><a href="#使用">使用</a>
<ul>
<li><a href="#cephfs">CephFS</a></li>
<li><a href="#rbd">RBD</a></li>
<li><a href="#k8s">k8s</a>
<ul>
<li><a href="#rbd用作volume">RBD用作volume</a></li>
<li><a href="#rbd用作pv-pvc">RBD用作PV/PVC</a></li>
<li><a href="#rbd用作storage-class">RBD用作storage class</a></li>
</ul></li>
</ul></li>
<li><a href="#参考文章">参考文章</a></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<p>本文主要记录在虚拟机中安装Ceph单节点集群的过程，其实多配置几个节点，也就是安装集群的过程。</p>

<p>Ceph中午文档：<a href="http://docs.ceph.org.cn/">http://docs.ceph.org.cn/</a></p>

<h1 id="集群规划">集群规划</h1>

<p>集群环境：</p>

<table>
<thead>
<tr>
<th>主机</th>
<th>IP</th>
<th>操作系统</th>
<th>软件</th>
<th>组件</th>
</tr>
</thead>

<tbody>
<tr>
<td>k8s-rke-node001</td>
<td>192.168.56.111</td>
<td>Centos7.7.1908</td>
<td>Ceph 14.2.4</td>
<td>ceph_deploy、OSD、MON</td>
</tr>

<tr>
<td>k8s-rke-node002</td>
<td>192.168.56.112</td>
<td>Centos7.7.1908</td>
<td>Ceph 14.2.4</td>
<td>OSD、MON</td>
</tr>

<tr>
<td>k8s-rke-node003</td>
<td>192.168.56.113</td>
<td>Centos7.7.1908</td>
<td>Ceph 14.2.4</td>
<td>OSD、MON</td>
</tr>
</tbody>
</table>

<p>安装用户：chenzj</p>

<h1 id="准备环境">准备环境</h1>

<p>你的管理节点必须能够通过 SSH 无密码地访问各 Ceph 节点。如果 <code>ceph-deploy</code> 以某个普通用户登录，那么这个用户必须有无密码使用 <code>sudo</code> 的权限。</p>

<h2 id="设置hosts">设置hosts</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">echo</span> <span class="s">&lt;&lt; EOF &gt; /etc/hosts
</span><span class="s">192.168.56.111 k8s-rke-node001
</span><span class="s">192.168.56.112 k8s-rke-node002
</span><span class="s">192.168.56.113 k8s-rke-node003
</span><span class="s">EOF</span></code></pre></td></tr></table>
</div>
</div>
<h2 id="创建部署-ceph-的用户">创建部署 CEPH 的用户</h2>

<p><code>ceph-deploy</code> 工具必须以普通用户登录 Ceph 节点，且此用户拥有无密码使用 <code>sudo</code> 的权限，因为它需要在安装软件及配置文件的过程中，不必输入密码。</p>

<p>在各 Ceph 节点创建新用户并设置密码。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nv">USER</span><span class="o">=</span>chenzj
useradd -G docker -d /home/<span class="nv">$USER</span> -m <span class="nv">$USER</span> 
<span class="nb">echo</span> <span class="nv">$USER</span><span class="p">|</span>passwd <span class="nv">$USER</span> --stdin &gt;/dev/null <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span></code></pre></td></tr></table>
</div>
</div>
<p>确保各 Ceph 节点上新创建的用户都有 <code>sudo</code> 权限。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo <span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$USER</span><span class="s2"> ALL = (root) NOPASSWD:ALL&#34;</span> <span class="p">|</span> sudo tee /etc/sudoers.d/<span class="nv">$USER</span>
sudo chmod <span class="m">0440</span> /etc/sudoers.d/<span class="nv">$USER</span></code></pre></td></tr></table>
</div>
</div>
<h2 id="允许无密码-ssh-登录">允许无密码 SSH 登录</h2>

<p>正因为 <code>ceph-deploy</code> 不支持输入密码，你必须在管理节点上生成 SSH 密钥并把其公钥分发到各 Ceph 节点。 <code>ceph-deploy</code> 会尝试给初始 monitors 生成 SSH 密钥对。</p>

<p>生成 SSH 密钥对，但不要用 <code>sudo</code> 或 <code>root</code> 用户。提示 “Enter passphrase” 时，直接回车，口令即为空：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">su - <span class="nv">$USER</span>
yes<span class="p">|</span>ssh-keygen -f ~/.ssh/id_rsa -t rsa -N <span class="s2">&#34;&#34;</span></code></pre></td></tr></table>
</div>
</div>
<p>把公钥拷贝到各 Ceph 节点</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ssh-copy-id <span class="nv">$USER</span>@k8s-rke-node001
ssh-copy-id <span class="nv">$USER</span>@k8s-rke-node002
ssh-copy-id <span class="nv">$USER</span>@k8s-rke-node003</code></pre></td></tr></table>
</div>
</div>
<p>修改 <code>ceph-deploy</code> 管理节点上的 <code>~/.ssh/config</code> 文件，这样 <code>ceph-deploy</code> 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 <code>ceph-deploy</code> 都要指定 <code>--username</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">cat <span class="s">&lt;&lt; EOF &gt;&gt; ~/.ssh/config
</span><span class="s">Host      	k8s-rke-node001
</span><span class="s">	Hostname  k8s-rke-node001
</span><span class="s">	User      $USER
</span><span class="s">
</span><span class="s">Host      	k8s-rke-node002
</span><span class="s">	Hostname  k8s-rke-node002
</span><span class="s">	User      $USER
</span><span class="s">
</span><span class="s">Host      	k8s-rke-node003
</span><span class="s">	Hostname  k8s-rke-node003
</span><span class="s">	User      $USER
</span><span class="s">EOF</span>

chmod <span class="m">644</span> ~/.ssh/config</code></pre></td></tr></table>
</div>
</div>
<h2 id="配置时钟同步">配置时钟同步</h2>

<h2 id="设置selinux">设置SELINUX</h2>

<p>在 CentOS 和 RHEL 上， SELinux 默认为 <code>Enforcing</code> 开启状态。为简化安装，我们建议把 SELinux 设置为 <code>Permissive</code> 或者完全禁用，也就是在加固系统配置前先确保集群的安装、配置没问题。用下列命令把 SELinux 设置为 <code>Permissive</code> ：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo setenforce <span class="m">0</span></code></pre></td></tr></table>
</div>
</div>
<h2 id="设置tty">设置TTY</h2>

<p>在 CentOS 和 RHEL 上执行 <code>ceph-deploy</code> 命令时可能会报错。如果你的 Ceph 节点默认设置了 <code>requiretty</code> ，执行 <code>sudo visudo</code> 禁用它，并找到 <code>Defaults requiretty</code> 选项，把它改为 <code>Defaults:ceph !requiretty</code> 或者直接注释掉，这样 <code>ceph-deploy</code> 就可以用之前创建的用户（<a href="http://docs.ceph.org.cn/start/quick-start-preflight/#id3">创建部署 Ceph 的用户</a> ）连接了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sed -i <span class="s1">&#39;s/Defaults *requiretty/#Defaults requiretty/g&#39;</span> /etc/sudoers
sed -i <span class="s1">&#39;s/Defaults *!visiblepw/Defaults   visiblepw/g&#39;</span> /etc/sudoers</code></pre></td></tr></table>
</div>
</div>
<h2 id="安装yum-plugin-priorities">安装yum-plugin-priorities</h2>

<p>确保你的包管理器安装了优先级/首选项包且已启用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo yum install yum-plugin-priorities -y</code></pre></td></tr></table>
</div>
</div>
<h2 id="加载rbd模块">加载rbd模块</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">sudo modprobe rbd</pre></td></tr></table>
</div>
</div>
<h1 id="创建集群">创建集群</h1>

<h2 id="安装-ceph-deploy">安装 Ceph-Deploy</h2>

<p>把 Ceph 仓库添加到 <code>ceph-deploy</code> 管理节点，然后安装 <code>ceph-deploy</code> 。</p>

<p>安装包管理工具：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo yum install -y yum-utils 
curl -k -s -o /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repo</code></pre></td></tr></table>
</div>
</div>
<p>配置yum源，注意：这里ceph使用的是nautilus版本。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">cat <span class="s">&lt;&lt; EOF &gt; ceph.repo
</span><span class="s">[ceph-base]
</span><span class="s">name=Ceph Base packages
</span><span class="s">baseurl=https://mirrors.cloud.tencent.com/ceph/rpm-nautilus/el7/x86_64
</span><span class="s">enabled=1
</span><span class="s">gpgcheck=1
</span><span class="s">type=rpm-md
</span><span class="s">gpgkey=https://download.ceph.com/keys/release.asc
</span><span class="s">
</span><span class="s">[ceph-noarch]
</span><span class="s">name=Ceph noarch packages
</span><span class="s">baseurl=https://mirrors.cloud.tencent.com/ceph/rpm-nautilus/el7/noarch
</span><span class="s">enabled=1
</span><span class="s">gpgcheck=1
</span><span class="s">priority=1
</span><span class="s">type=rpm-md
</span><span class="s">gpgkey=http://mirrors.cloud.tencent.com/ceph/keys/release.asc
</span><span class="s">EOF</span>

sudo mv ceph.repo /etc/yum.repos.d/</code></pre></td></tr></table>
</div>
</div>
<p>设置环境变量，使ceph-deploy使用腾讯云源。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">export</span> <span class="nv">CEPH_DEPLOY_REPO_URL</span><span class="o">=</span>https://mirrors.cloud.tencent.com/ceph/rpm-nautilus/el7
<span class="nb">export</span> <span class="nv">CEPH_DEPLOY_GPG_URL</span><span class="o">=</span>https://mirrors.cloud.tencent.com/ceph/keys/release.asc</code></pre></td></tr></table>
</div>
</div>
<p>更新软件库并安装 <code>ceph-deploy</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo yum update -y <span class="o">&amp;&amp;</span> sudo yum install ceph-deploy python-pip -y</code></pre></td></tr></table>
</div>
</div>
<h2 id="创建配置目录">创建配置目录</h2>

<p>先在ceph-deploy管理节点上创建一个目录，用于保存 <code>ceph-deploy</code> 生成的配置文件和密钥对。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">mkdir ceph-cluster <span class="o">&amp;&amp;</span> <span class="nb">cd</span> ceph-cluster</code></pre></td></tr></table>
</div>
</div>
<h2 id="创建集群-1">创建集群</h2>

<p>在管理节点上，进入刚创建的放置配置文件的目录，用 <code>ceph-deploy</code> 执行如下步骤，安装规划在k8s-rke-node001节点初始化Monitor。</p>

<p>1、创建集群，部署新的 monitor 节点</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph-deploy new k8s-rke-node001 <span class="se">\
</span><span class="se"></span>  --public-network <span class="m">192</span>.168.56.0/24 <span class="se">\
</span><span class="se"></span>  --cluster-network <span class="m">192</span>.168.56.0/24 </code></pre></td></tr></table>
</div>
</div>
<blockquote>
<p>注意：</p>

<ul>
<li><p>如有多网卡环境，应该把集群网络和数据网络区分开：（官方文档：<a href="http://docs.ceph.org.cn/rados/configuration/network-config-ref/）">http://docs.ceph.org.cn/rados/configuration/network-config-ref/）</a></p></li>

<li><p>public-network要和当前ip的网段保持对应。</p></li>
</ul>
</blockquote>

<p>2、在当前目录下用 <code>ls</code> 和 <code>cat</code> 检查 <code>ceph-deploy</code> 的输出，应该有一个 Ceph 配置文件、一个 monitor 密钥环和一个日志文件。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">-rw-r--r-- <span class="m">1</span> root root    <span class="m">291</span> 9月  <span class="m">17</span> <span class="m">22</span>:15 ceph.conf
-rw-r--r-- <span class="m">1</span> root root <span class="m">212100</span> 9月  <span class="m">17</span> <span class="m">22</span>:16 ceph-deploy-ceph.log
-rw------- <span class="m">1</span> root root     <span class="m">73</span> 9月  <span class="m">16</span> <span class="m">09</span>:40 ceph.mon.keyring</code></pre></td></tr></table>
</div>
</div>
<p>3、如果出现异常：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
  File <span class="s2">&#34;/usr/bin/ceph-deploy&#34;</span>, line <span class="m">18</span>, in &lt;module&gt;
    from ceph_deploy.cli import main
  File <span class="s2">&#34;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&#34;</span>, line <span class="m">1</span>, in &lt;module&gt;
    import pkg_resources
ImportError: No module named pkg_resources</code></pre></td></tr></table>
</div>
</div>
<p>安装相关依赖即可：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo yum install python-pkg-resources python-setuptools -y</code></pre></td></tr></table>
</div>
</div>
<p>4、把 Ceph 配置文件里的默认副本数从 <code>3</code> 改成 <code>2</code> ，这样只有两个 OSD 也可以达到 <code>active + clean</code> 状态。把下面这行加入 <code>[global]</code> 段：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></pre></td>
<td class="lntd">
<pre class="chroma">echo &#34;
osd_pool_default_size =2

[mon]
mon_allow_pool_delete = true
mon_max_pg_per_osd = 1024
#时钟同步
mon_clock_drift_allowed = 2
mon_clock_drift_warn_backoff = 30
&#34; &gt;&gt; ceph.conf</pre></td></tr></table>
</div>
</div>
<h2 id="安装-ceph-到各节点">安装 Ceph 到各节点</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph-deploy install k8s-rke-node001 k8s-rke-node002 k8s-rke-node003</code></pre></td></tr></table>
</div>
</div>
<p>此过程需要等待一段时间，因为 ceph-deploy 会 SSH 登录到各 node 上去，依次执行安装 ceph 依赖的组件包。</p>

<p>如果下载太慢，可以设置镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo rpm --import https://mirrors.cloud.tencent.com/ceph/keys/release.asc
ceph-deploy install --repo-url https://mirrors.cloud.tencent.com/ceph/rpm-nautilus k8s-rke-node001</code></pre></td></tr></table>
</div>
</div>
<p>如果还是太慢，则在<strong>每个节点配置ceph的yum源</strong>，然后通过yum安装：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo yum -y install ceph ceph-radosgw</code></pre></td></tr></table>
</div>
</div>
<p>安装成功之后，检查每个节点的ceph版本是否一致：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span></pre></td>
<td class="lntd">
<pre class="chroma">sudo ceph --version
ceph version 14.2.4 (75f4de193b3ea58512f204623e6c5a16e6c1e1ba) nautilus (stable)</pre></td></tr></table>
</div>
</div>
<h2 id="初始化-monitor">初始化 Monitor</h2>

<p>初始化 Monitor , 获取秘钥，生产对应的 key：（服务默认端口 6789）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy mon create-initial</pre></td></tr></table>
</div>
</div>
<p>完成上述操作后，当前目录里应该会出现这些密钥：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph.client.admin.keyring
ceph.bootstrap-mgr.keyring
ceph.bootstrap-osd.keyring
ceph.bootstrap-mds.keyring
ceph.bootstrap-rgw.keyring
ceph.bootstrap-rbd.keyring
ceph.bootstrap-rbd-mirror.keyring</pre></td></tr></table>
</div>
</div>
<h2 id="初始化磁盘">初始化磁盘</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></pre></td>
<td class="lntd">
<pre class="chroma">fdisk -l

sudo mkfs /dev/sdb

parted /dev/sdb</pre></td></tr></table>
</div>
</div>
<h2 id="创建-osd-存储节点">创建 OSD 存储节点</h2>

<p>列出节点所有磁盘信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph-deploy disk list k8s-rke-node001
ceph-deploy disk list k8s-rke-node002
ceph-deploy disk list k8s-rke-node003</code></pre></td></tr></table>
</div>
</div>
<p>创建 OSD 存储节点:（服务默认端口 74053）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy osd create k8s-rke-node001 --data /dev/sdb1
ceph-deploy osd create k8s-rke-node002 --data /dev/sdb1
ceph-deploy osd create k8s-rke-node003 --data /dev/sdb1</pre></td></tr></table>
</div>
</div>
<p>查看创建的osd：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy osd list k8s-rke-node001
ceph-deploy osd list k8s-rke-node002
ceph-deploy osd list k8s-rke-node003</pre></td></tr></table>
</div>
</div>
<p>清除磁盘分区和内容：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy disk zap k8s-rke-node001 /dev/sdb
ceph-deploy disk zap k8s-rke-node002 /dev/sdb
ceph-deploy disk zap k8s-rke-node003 /dev/sdb</pre></td></tr></table>
</div>
</div>
<h2 id="分配-admin-配置文件">分配 admin 配置文件</h2>

<p>用 <code>ceph-deploy</code> 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 <code>ceph.client.admin.keyring</code> 了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph-deploy --overwrite-conf admin k8s-rke-node001 k8s-rke-node002 k8s-rke-node003</code></pre></td></tr></table>
</div>
</div>
<p>确保你对 <code>ceph.client.admin.keyring</code> 有正确的操作权限。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo chmod +r /etc/ceph/ceph.client.admin.keyring</code></pre></td></tr></table>
</div>
</div>
<h2 id="查看集群状态">查看集群状态</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph -s</pre></td></tr></table>
</div>
</div>
<p>此时出现错误，没有活跃的 mgr 服务。</p>

<h2 id="部署-mgr">部署 MGR</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph-deploy mgr create k8s-rke-node001 k8s-rke-node002 k8s-rke-node003</code></pre></td></tr></table>
</div>
</div>
<p>继续查看状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph -s</pre></td></tr></table>
</div>
</div>
<p>此时状态 HEALTH_OK 则集群正常。</p>

<h2 id="安装-dashboard">安装 Dashboard</h2>

<p>安装 Dashboard，首先必须有 mgr</p>

<p>开启模块：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph mgr module enable dashboard</pre></td></tr></table>
</div>
</div>
<p>创建证书：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph dashboard create-self-signed-cert</pre></td></tr></table>
</div>
</div>
<p>创建密码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph dashboard set-login-credentials admin admin</pre></td></tr></table>
</div>
</div>
<p>查看服务访问：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph mgr services</pre></td></tr></table>
</div>
</div>
<p>修改<strong>ceph.conf</strong>配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span></pre></td>
<td class="lntd">
<pre class="chroma">[mgr]
mgr modules = dashboard</pre></td></tr></table>
</div>
</div>
<p>推送配置文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy --overwrite-conf admin k8s-rke-node001 k8s-rke-node002 k8s-rke-node003</pre></td></tr></table>
</div>
</div>
<p>重启服务：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">systemctl restart ceph\*.service ceph\*.target</pre></td></tr></table>
</div>
</div>
<p>可以修改访问地址：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span></pre></td>
<td class="lntd">
<pre class="chroma">$ ceph config-key put mgr/dashboard/server_addr 34.97.202.172
$ ceph config-key put mgr/dashboard/server_port 8000</pre></td></tr></table>
</div>
</div>
<h2 id="清除集群">清除集群</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ ceph-deploy purge k8s-rke-node001 k8s-rke-node002 k8s-rke-node003
$ ceph-deploy purgedata k8s-rke-node001 k8s-rke-node002 k8s-rke-node003
$ ceph-deploy forgetkeys</code></pre></td></tr></table>
</div>
</div>
<h1 id="配置调优">配置调优</h1>

<p>配置文件优化如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span></pre></td>
<td class="lntd">
<pre class="chroma">[global]
fsid = bf24f836-509b-4dbb-9f73-2518818a5cd2
mon_initial_members = instance-template-3, instance-template-4, instance-template-5
mon_host = 10.174.0.16,10.174.0.17,10.174.0.18
 
# 集群认证
auth_cluster_required = cephx
 
# 服务认证
auth_service_required = cephx
 
# 客户端认证
auth_client_required = cephx
 
# 如果设置了该选项，Ceph 会设置系统的 max open fds，默认值为 0
max open files = 0
 
# 最小副本数 默认是3
osd pool default size = 3
 
 #PG 处于 degraded 状态不影响其 IO 能力,min_size 是一个PG 能接受 IO 的最小副本数
osd pool default min size = 1
 
 
[mon.a]
host = instance-template-3
mon addr = 10.174.0.16:6789
 
[mon.b]
host = instance-template-4
mon addr = 10.174.0.17:6789
 
[mon.c]
host = instance-template-5
mon addr = 10.174.0.18:6789
 
[mon]
mon data = /var/lib/ceph/mon/ceph-$id
 
# monitor 间的 clock drift，默认值 0.05
mon clock drift allowed = 1
 
# 向 monitor 报告 down 的最小 OSD 数，默认值 1
mon osd min down reporters = 1
 
# 标记一个OSD状态为down和out之前ceph等待的秒数，默认值300
mon osd down out interval = 600
 
[osd]
# osd 数据路径
osd data = /var/lib/ceph/osd/ceph-$id
 
# 默认 pool pg,pgp 数量
osd pool default pg num =  1024
osd pool default pgp num = 1024
 
# osd 的 journal 写日志时的大小默认 5120
osd journal size = 20000
 
# 格式化文件系统类型
osd mkfs type = xfs
 
# 格式化文件系统时附加参数
osd mkfs options xfs = -f
 
# 为 XATTRS 使用 object map，EXT4 文件系统时使用，XFS 或者 btrf 也可以使用，默认 false
filestore xattr use omap = true
 
# 从日志到数据盘最小同步间隔(seconds)，默认值 0.1
filestore min sync interval = 10
 
# 从日志到数据盘最大同步间隔(seconds)，默认值 5
filestore max sync interval = 15
 
# 数据盘最大接受的操作数，默认值 500
filestore queue max ops = 25000
 
# 数据盘能够 commit 的最大字节数(bytes)，默认值 100
filestore queue max bytes = 10485760
 
# 数据盘能够 commit 的操作数，500
filestore queue committing max ops = 5000
 
# 数据盘能够 commit 的最大字节数(bytes)，默认值 100
filestore queue committing max bytes = 10485760000
 
# 前一个子目录分裂成子目录中的文件的最大数量，默认值 2
filestore split multiple = 8
 
# 前一个子类目录中的文件合并到父类的最小数量，默认值10
filestore merge threshold = 40
 
# 对象文件句柄缓存大小，默认值 128
filestore fd cache size = 1024
 
# 并发文件系统操作数，默认值 2
filestore op threads = 32
 
# journal 一次性写入的最大字节数(bytes)，默认值 1048560
journal max write bytes = 1073714824
 
# journal一次性写入的最大记录数，默认值 100
journal max write entries = 10000
 
# journal一次性最大在队列中的操作数，默认值 50
journal queue max ops = 50000
 
# journal一次性最大在队列中的字节数(bytes)，默认值 33554432
journal queue max bytes = 10485760000
 
# # OSD一次可写入的最大值(MB), 默认 90
osd max write size = 512
 
# 客户端允许在内存中的最大数据(bytes), 默认值100
osd client message size cap = 2147483648
 
# 在 Deep Scrub 时候允许读取的字节数(bytes), 默认值524288
osd deep scrub stride = 131072
 
# 并发文件系统操作数, 默认值 2
osd op threads = 8
 
# OSD 密集型操作例如恢复和 Scrubbing 时的线程, 默认值1
osd disk threads = 4
 
# 保留 OSD Map 的缓存(MB), 默认 500
osd map cache size = 1024
 
# OSD 进程在内存中的 OSD Map 缓存(MB), 默认 50
osd map cache bl size = 128
 
# 默认值rw,noatime,inode64, Ceph OSD xfs Mount选项
osd mount options xfs = &#34;rw,noexec,nodev,noatime,nodiratime,nobarrier&#34;
 
# 恢复操作优先级，取值 1-63，值越高占用资源越高, 默认值 10
osd recovery op priority = 4
 
# 同一时间内活跃的恢复请求数, 默认值 15
osd recovery max active = 10
 
# 一个 OSD 允许的最大 backfills 数, 默认值 10
osd max backfills = 4
 
[client]
# RBD缓存, 默认 true
rbd cache = true
 
# RBD缓存大小(bytes), 默认 335544320（320M）
rbd cache size = 268435456
 
# 缓存为 write-back 时允许的最大 dirty 字节数(bytes)，如果为0，使用 write-through，默认值为 25165824
rbd cache max dirty = 134217728
 
# 在被刷新到存储盘前 dirty 数据存在缓存的时间(seconds), 默认值为 1
rbd cache max dirty age = 5
 
# 该选项是为了兼容 linux-2.6.32 之前的 virtio 驱动，避免因为不发送 flush 请求，数据不回写 默认值 true 
# 设置该参数后，librbd 会以 writethrough 的方式执行 io，直到收到第一个 flush 请求，才切换为 writeback 方式。
# rbd cache writethrough until flush = false
 
# 最大的 Object 对象数，默认为 0，表示通过 rbd cache size 计算得到，librbd 默认以 4MB 为单位对磁盘 Image 进行逻辑切分，默认值 0
# 每个 chunk 对象抽象为一个 Object；librbd 中以 Object 为单位来管理缓存，增大该值可以提升性能
# rbd cache max dirty object = 2
 
# 开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty，默认值 16777216
# rbd cache target dirty = 235544320
  
# GW 端口，默认 7480
# rgw frontends = civetweb port=7480</pre></td></tr></table>
</div>
</div>
<p>配置完成后，需要重启加载配置文件到所有节点：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy --overwrite-conf admin k8s-rke-node001 k8s-rke-node002 k8s-rke-node003</pre></td></tr></table>
</div>
</div>
<p>重启服务：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">systemctl restart ceph<span class="se">\*</span>.service ceph<span class="se">\*</span>.target</code></pre></td></tr></table>
</div>
</div>
<h1 id="集群扩容">集群扩容</h1>

<p>一个基本的集群启动并开始运行后，下一步就是扩展集群。在 k8s-rke-node004 上添加一个 OSD 守护进程和一个元数据服务器。然后分别在 k8s-rke-node002 和 k8s-rke-node003 上添加 Ceph Monitor ，以形成 Monitors 的法定人数。</p>

<h2 id="添加-osd">添加 OSD</h2>

<p>在 k8s-rke-node004 上添加一个 OSD：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ssh k8s-rke-node004 <span class="s2">&#34;sudo mkdir -p /data/ceph/osd&#34;</span>
ceph-deploy osd create --data /data/ceph/osd k8s-rke-node004</code></pre></td></tr></table>
</div>
</div>
<p>一旦你新加了 OSD ， Ceph 集群就开始重均衡，把归置组迁移到新 OSD 。可以用下面的 <code>ceph</code> 命令观察此过程：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph -w</pre></td></tr></table>
</div>
</div>
<p>你应该能看到归置组状态从 <code>active + clean</code> 变为 <code>active</code> ，还有一些降级的对象；迁移完成后又会回到 <code>active + clean</code> 状态（ Control-C 退出）。</p>

<p>查看 osd 状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph osd tree</pre></td></tr></table>
</div>
</div>
<h2 id="添加-monitor">添加 Monitor</h2>

<p>Ceph 存储集群需要至少一个 Monitor 才能运行。为达到高可用，典型的 Ceph 存储集群会运行多个 Monitors，这样在单个 Monitor 失败时不会影响 Ceph 存储集群的可用性。Ceph 使用 PASOX 算法，此算法要求有多半 monitors（即 1 、 2:3 、 3:4 、 3:5 、 4:6 等 ）形成法定人数。</p>

<p>k8s-rke-node002 和 k8s-rke-node003 上添加 Ceph Monitor</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy mon add k8s-rke-node002
ceph-deploy mon add k8s-rke-node003</pre></td></tr></table>
</div>
</div>
<p>新增 Monitor 后，Ceph 会自动开始同步并形成法定人数。你可以用下面的命令检查法定人数状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph quorum_status --format json-pretty</pre></td></tr></table>
</div>
</div>
<ul>
<li>请确保ntp时钟同步</li>
</ul>

<h2 id="添加-mds">添加 MDS</h2>

<p>如果需要使用 Cephfs，则需要创建 MDS:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy mds create k8s-rke-node001</pre></td></tr></table>
</div>
</div>
<blockquote>
<p>注意：</p>

<p>当前生产环境下的 Ceph 只能运行一个元数据服务器。</p>
</blockquote>

<p>查看服务状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ netstat -lnput
Active Internet connections <span class="o">(</span>only servers<span class="o">)</span>
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
...
tcp        <span class="m">0</span>      <span class="m">0</span> <span class="m">0</span>.0.0.0:6805            <span class="m">0</span>.0.0.0:*               LISTEN      <span class="m">71047</span>/ceph-mds</code></pre></td></tr></table>
</div>
</div>
<h2 id="添加-rgw">添加 RGW</h2>

<p>添加 RGW 支持使用 Object Storage 存储：(服务端 7480)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">ceph-deploy rgw create k8s-rke-node001</pre></td></tr></table>
</div>
</div>
<p>可以更改该节点 ceph.conf 内服务端口：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>client<span class="o">]</span>
rgw <span class="nv">frontends</span> <span class="o">=</span> civetweb <span class="nv">port</span><span class="o">=</span><span class="m">80</span></code></pre></td></tr></table>
</div>
</div>
<p>浏览器访问：<a href="http://192.168.56.111:7480">http://192.168.56.111:7480</a></p>

<h1 id="使用">使用</h1>

<h2 id="cephfs">CephFS</h2>

<p>查看 pool 存储池：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph osd pool ls</code></pre></td></tr></table>
</div>
</div>
<p>创建 pool 名为 test_data 的 pg 32：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph osd pool create test_data <span class="m">32</span></code></pre></td></tr></table>
</div>
</div>
<blockquote>
<p>pg 数量：</p>

<p>创建pool 通常在创建pool之前，需要覆盖默认的 pg_num，官方推荐： 若少于 5 个OSD， 设置 pg_num 为 128。 5~10 个 OSD，设置 pg_num 为 512。 10~50 个 OSD，设置 pg_num 为 4096。 超过 50 个 OSD，可以参考 pgcalc 计算。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">Total <span class="nv">PGs</span> <span class="o">=</span> <span class="o">(</span>Total_number_of_OSD * <span class="m">100</span><span class="o">)</span> / max_replication_count</code></pre></td></tr></table>
</div>
</div>
<p>创建 pool 名为 test_metadata 的 pg 32：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph osd pool create test_metadata <span class="m">32</span></code></pre></td></tr></table>
</div>
</div>
<p>创建名为 data 的 fs：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ ceph fs new data test_metadata test_data
new fs with metadata pool <span class="m">10</span> and data pool <span class="m">9</span></code></pre></td></tr></table>
</div>
</div>
<p>查看 cephfs 列表：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ ceph fs ls
name: data, metadata pool: test_metadata, data pools: <span class="o">[</span>test_data <span class="o">]</span></code></pre></td></tr></table>
</div>
</div>
<p>安装 ceph-fuse 进行挂载文件系统的支持：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo yum install -y ceph-fuse</code></pre></td></tr></table>
</div>
</div>
<p>挂载：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ mkdir /data
$ ceph-fuse -m <span class="m">192</span>.168.1.121:6789,192.168.1.122:6789,192.168.1.123:6789 /data
$ df -h
Filesystem      Size  Used Avail Use% Mounted on
...
ceph-fuse        47G     <span class="m">0</span>   47G   <span class="m">0</span>% /data</code></pre></td></tr></table>
</div>
</div>
<p>可以使用 mount 进行挂载：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ mount -t ceph -o <span class="nv">name</span><span class="o">=</span>admin,secret<span class="o">=</span><span class="sb">`</span>ceph auth get-key client.admin<span class="sb">`</span>,rw <span class="m">192</span>.168.1.121:6789,192.168.1.122:6789,192.168.1.123:6789:/ /data</code></pre></td></tr></table>
</div>
</div>
<p>可能会超时：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ mount -vvvv -t ceph -o <span class="nv">name</span><span class="o">=</span>admin,secret<span class="o">=</span><span class="sb">`</span>ceph auth get-key client.admin<span class="sb">`</span>,rw <span class="m">192</span>.168.1.121:6789,192.168.1.122:6789,192.168.1.123:6789:/ /data         
parsing options: rw,name<span class="o">=</span>admin,secret<span class="o">=</span>xxx
mount error <span class="nv">110</span> <span class="o">=</span> Connection timed out
mount error <span class="nv">110</span> <span class="o">=</span> Connection timed out</code></pre></td></tr></table>
</div>
</div>
<p>通过如下方式解决：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ ceph osd crush tunables hammer
$ ceph osd crush reweight-all</code></pre></td></tr></table>
</div>
</div>
<p>删除 pool：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph osd pool rm test_data test_data --yes-i-really-really-mean-it</code></pre></td></tr></table>
</div>
</div>
<p>如出现如下错误：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ ceph osd pool rm test_data test_data --yes-i-really-really-mean-it
Error EPERM: WARNING: this will *PERMANENTLY DESTROY* all data stored in pool data.  If you are *ABSOLUTELY CERTAIN* that is what you want, pass the pool name *twice*, followed by --yes-i-really-really-mean-it.</code></pre></td></tr></table>
</div>
</div>
<p>请添加配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">[mon]
...
mon_allow_pool_delete = true</pre></td></tr></table>
</div>
</div>
<p>重启服务:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">systemctl restart ceph<span class="se">\*</span>.service ceph<span class="se">\*</span>.target</code></pre></td></tr></table>
</div>
</div>
<p>然后再进行删除。</p>

<h2 id="rbd">RBD</h2>

<p>使用 RBD 存储，加载模块：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ modprobe rbd

$ modinfo rbd
filename:       /lib/modules/4.4.197-1.el7.elrepo.x86_64/kernel/drivers/block/rbd.ko
license:        GPL
description:    RADOS Block Device <span class="o">(</span>RBD<span class="o">)</span> driver
author:         Jeff Garzik &lt;jeff@garzik.org&gt;
author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
author:         Sage Weil &lt;sage@newdream.net&gt;
author:         Alex Elder &lt;elder@inktank.com&gt;
srcversion:     6B2F83CD15A20F9CDD5DDF0
depends:        libceph
retpoline:      Y
intree:         Y
vermagic:       <span class="m">4</span>.4.197-1.el7.elrepo.x86_64 SMP mod_unload modversions
parm:           single_major:Use a single major number <span class="k">for</span> all rbd devices <span class="o">(</span>default: <span class="nb">false</span><span class="o">)</span> <span class="o">(</span>bool<span class="o">)</span></code></pre></td></tr></table>
</div>
</div>
<p>查看存储池：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ ceph osd lspools
<span class="m">7</span> rbd
<span class="m">8</span> k8s
<span class="m">9</span> test_data
<span class="m">10</span> test_metadata</code></pre></td></tr></table>
</div>
</div>
<p>查看某个存储池中的镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">rbd list k8s</pre></td></tr></table>
</div>
</div>
<p>查看各个存储池使用情况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rados df
POOL_NAME        USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD   WR_OPS      WR USED COMPR UNDER COMPR
k8s            <span class="m">48</span> GiB    <span class="m">5235</span>      <span class="m">0</span>  <span class="m">15705</span>                  <span class="m">0</span>       <span class="m">0</span>        <span class="m">0</span> <span class="m">3331022</span> <span class="m">180</span> GiB <span class="m">52592993</span> <span class="m">533</span> GiB        <span class="m">0</span> B         <span class="m">0</span> B
rbd           <span class="m">192</span> KiB       <span class="m">3</span>      <span class="m">0</span>      <span class="m">9</span>                  <span class="m">0</span>       <span class="m">0</span>        <span class="m">0</span>    <span class="m">2100</span>  <span class="m">16</span> MiB      <span class="m">522</span> <span class="m">360</span> MiB        <span class="m">0</span> B         <span class="m">0</span> B
test_data         <span class="m">0</span> B       <span class="m">0</span>      <span class="m">0</span>      <span class="m">0</span>                  <span class="m">0</span>       <span class="m">0</span>        <span class="m">0</span>       <span class="m">0</span>     <span class="m">0</span> B        <span class="m">0</span>     <span class="m">0</span> B        <span class="m">0</span> B         <span class="m">0</span> B
test_metadata <span class="m">1</span>.5 MiB      <span class="m">22</span>      <span class="m">0</span>     <span class="m">66</span>                  <span class="m">0</span>       <span class="m">0</span>        <span class="m">0</span>       <span class="m">0</span>     <span class="m">0</span> B       <span class="m">45</span>  <span class="m">13</span> KiB        <span class="m">0</span> B         <span class="m">0</span> B
test_rbd      <span class="m">192</span> KiB       <span class="m">3</span>      <span class="m">0</span>      <span class="m">9</span>                  <span class="m">0</span>       <span class="m">0</span>        <span class="m">0</span>     <span class="m">608</span> <span class="m">1</span>.8 MiB      <span class="m">235</span>  <span class="m">23</span> MiB        <span class="m">0</span> B         <span class="m">0</span> B

total_objects    <span class="m">5263</span>
total_used       <span class="m">51</span> GiB
total_avail      <span class="m">1</span>.4 TiB
total_space      <span class="m">1</span>.5 TiB</code></pre></td></tr></table>
</div>
</div>
<p>创建名为 test_rbd 的 Pool：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">ceph osd pool create test_rbd <span class="m">70</span></code></pre></td></tr></table>
</div>
</div>
<p>初始化 rbd pool 块：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">rbd pool init test_rbd</code></pre></td></tr></table>
</div>
</div>
<p>创建 rbd，名为 rbd1 大小为 10GB：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">rbd create test_rbd/rbd1 --size <span class="m">10240</span>
<span class="c1"># rbd create test_rbd/rbd1 --size 10240 --image-format 2 --object-size 22</span>
<span class="c1"># rbd create test_rbd/rbd1  --size 1024 --image-feature layering</span></code></pre></td></tr></table>
</div>
</div>
<p>说明：</p>

<ul>
<li>默认创建 image 时，没有任何模式也就是 &ndash;image-format 为 1。</li>
<li>如果 &ndash;image-format 为 2，则支持 RBD分层（layering），是实现 COW （Copy-On-Write）的前提。</li>
<li>如果 &ndash;object-size 24（16MB） 则设置 Object 的大小，默认值 22（4MB）</li>
</ul>

<p>查看池中的 image：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rbd ls test_rbd
rbd1
  
$ rbd info test_rbd/rbd1
rbd image <span class="s1">&#39;rbd1&#39;</span>:
	size <span class="m">10</span> GiB in <span class="m">2560</span> objects
	order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span>
	snapshot_count: <span class="m">0</span>
	id: 51e6e345fdee6
	block_name_prefix: rbd_data.51e6e345fdee6
	format: <span class="m">2</span>
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span>
	access_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span>
	modify_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span></code></pre></td></tr></table>
</div>
</div>
<p>改写镜像大小：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rbd resize test_rbd/rbd1 --size <span class="m">20480</span>
Resizing image: <span class="m">100</span>% complete...done.
  
$ rbd info test_rbd/rbd1
rbd image <span class="s1">&#39;rbd1&#39;</span>:
	size <span class="m">20</span> GiB in <span class="m">5120</span> objects
	order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span>
	snapshot_count: <span class="m">0</span>
	id: 51e6e345fdee6
	block_name_prefix: rbd_data.51e6e345fdee6
	format: <span class="m">2</span>
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features:
	flags:
	create_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span>
	access_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span>
	modify_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span></code></pre></td></tr></table>
</div>
</div>
<p>映射到物理机：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rbd map test_rbd/rbd1
/dev/rbd0
  
<span class="c1"># 卸载</span>
<span class="c1"># rbd unmap test_rbd/rbd1</span></code></pre></td></tr></table>
</div>
</div>
<p>如出现如下提示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rbd map test_rbd/rbd1
rbd: sysfs write failed
RBD image feature <span class="nb">set</span> mismatch. You can disable features unsupported by the kernel with <span class="s2">&#34;rbd feature disable test_rbd/rbd1 object-map fast-diff deep-flatten&#34;</span>.
In some cases useful info is found in syslog - try <span class="s2">&#34;dmesg | tail&#34;</span>.
rbd: map failed: <span class="o">(</span><span class="m">6</span><span class="o">)</span> No such device or address</code></pre></td></tr></table>
</div>
</div>
<p>表示当前系统不支持 feature。</p>

<p>RBD支持的特性，及具体BIT值的计算如下：</p>

<table>
<thead>
<tr>
<th>属性</th>
<th>功能</th>
<th>BIT码</th>
</tr>
</thead>

<tbody>
<tr>
<td>layering</td>
<td>支持分层</td>
<td>1</td>
</tr>

<tr>
<td>striping</td>
<td>支持条带化v2</td>
<td>2</td>
</tr>

<tr>
<td>exclusive-lock</td>
<td>支持独占锁</td>
<td>4</td>
</tr>

<tr>
<td>object-map</td>
<td>支持对象映射（依赖 exclusive-lock ）</td>
<td>8</td>
</tr>

<tr>
<td>fast-diff</td>
<td>快速计算差异（依赖 object-map ）</td>
<td>16</td>
</tr>

<tr>
<td>deep-flatten</td>
<td>支持快照扁平化操作</td>
<td>32</td>
</tr>

<tr>
<td>journaling</td>
<td>支持记录 IO 操作（依赖独占锁）</td>
<td>64</td>
</tr>
</tbody>
</table>

<p>而实际上 Centos 7 的 3.10 内核只支持 layering… 所以我们要手动关闭一些 features，然后重新 map；如果想要一劳永逸，可以在 ceph.conf 中加入 rbd_default_features = 1 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)。</p>

<p>禁用内核不支持的 feature：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rbd feature disable test_rbd/rbd1 object-map fast-diff deep-flatten exclusive-lock</code></pre></td></tr></table>
</div>
</div>
<p>如果还想一劳永逸，那么就在执行创建rbd镜像命令的服务器中，修改Ceph配置文件/etc/ceph/ceph.conf，在global section下，增加</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span></pre></td>
<td class="lntd">
<pre class="chroma">rbd_default_features = 1</pre></td></tr></table>
</div>
</div>
<p>如果还无法进行映射通过 info 指令查看是否还有其他的 featires，然后 disable 后重试即可。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rbd info test_rbd/rbd1
rbd image <span class="s1">&#39;rbd1&#39;</span>:
	size <span class="m">20</span> GiB in <span class="m">5120</span> objects
	order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span>
	snapshot_count: <span class="m">0</span>
	id: 51e6e345fdee6
	block_name_prefix: rbd_data.51e6e345fdee6
	format: <span class="m">2</span>
	features: layering, exclusive-lock
	op_features:
	flags:
	create_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span>
	access_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span>
	modify_timestamp: Mon Feb  <span class="m">3</span> <span class="m">20</span>:48:14 <span class="m">2020</span></code></pre></td></tr></table>
</div>
</div>
<p>查看映射列表：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ rbd showmapped
id pool     namespace image snap device
<span class="m">0</span>  test_rbd           rbd1  -    /dev/rbd0</code></pre></td></tr></table>
</div>
</div>
<p>格式化 xfs 格式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo mkfs.xfs /dev/rbd0</code></pre></td></tr></table>
</div>
</div>
<p>挂载磁盘：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo mount /dev/rbd0 /mnt
  
$ df -hT
Filesystem     1K-blocks     Used Available Use% Mounted on
...
/dev/rbd0      xfs              20G   55M   20G   <span class="m">1</span>% /mnt</code></pre></td></tr></table>
</div>
</div>
<p>当 RBD 扩容时，则需要重新执行格式化并进行挂载：(此时数据会丢失)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ sudo umount /mnt
$ sudo mkfs.xfs /dev/rbd0 -f

$ rbd resize test_rbd/rbd1 --size <span class="m">20480</span>

$ sudo mount /dev/rbd0 /mnt</code></pre></td></tr></table>
</div>
</div>
<p>进入磁盘，创建测试文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">$ <span class="nb">cd</span> /mnt
$ sudo dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>/mnt/file <span class="nv">bs</span><span class="o">=</span>10M <span class="nv">count</span><span class="o">=</span><span class="m">1</span> <span class="nv">oflag</span><span class="o">=</span>direct
记录了1+0 的读入
记录了1+0 的写出
10485760字节<span class="o">(</span><span class="m">10</span> MB<span class="o">)</span>已复制，0.0783636 秒，134 MB/秒

$ ls
file

$ df -h /mnt
文件系统        容量  已用  可用 已用% 挂载点
/dev/rbd0        20G   43M   20G    <span class="m">1</span>% /mnt</code></pre></td></tr></table>
</div>
</div>
<p>删除 RBD:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">sudo umount /mnt
sudo rbd unmap test_rbd/rbd1
sudo rbd rm test_rbd/rbd1</code></pre></td></tr></table>
</div>
</div>
<h2 id="k8s">k8s</h2>

<p>创建一个rbd镜像：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">rbd create foobar -s <span class="m">1024</span> -p k8s <span class="c1">#在k8s pool中创建名为foobar的image，大小为1024MB</span></code></pre></td></tr></table>
</div>
</div>
<p>这时候，不要手动不要挂载！</p>

<h3 id="rbd用作volume">RBD用作volume</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>Pod<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>rbd<span class="w">
</span><span class="w"></span>spec<span class="p">:</span><span class="w">
</span><span class="w">  </span>containers<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>image<span class="p">:</span><span class="w"> </span>gcr.io/nginx<span class="w">
</span><span class="w">      </span>name<span class="p">:</span><span class="w"> </span>rbd-rw<span class="w">
</span><span class="w">      </span>volumeMounts<span class="p">:</span><span class="w">
</span><span class="w">      </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>rbdpd<span class="w">
</span><span class="w">        </span>mountPath<span class="p">:</span><span class="w"> </span>/mnt/rbd<span class="w">
</span><span class="w">  </span>volumes<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>rbdpd<span class="w">
</span><span class="w">      </span>rbd<span class="p">:</span><span class="w">
</span><span class="w">        </span>monitors<span class="p">:</span><span class="w">
</span><span class="w">        </span>-<span class="w"> </span><span class="s1">&#39;1.2.3.4:6789&#39;</span><span class="w">
</span><span class="w">        </span>pool<span class="p">:</span><span class="w"> </span>k8s<span class="w">
</span><span class="w">        </span>image<span class="p">:</span><span class="w"> </span>foobar<span class="w">
</span><span class="w">        </span>fsType<span class="p">:</span><span class="w"> </span>ext4<span class="w">
</span><span class="w">        </span>readOnly<span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">        </span>user<span class="p">:</span><span class="w"> </span>admin<span class="w">
</span><span class="w">        </span>keyring<span class="p">:</span><span class="w"> </span>/etc/ceph/ceph.client.admin.keyring<span class="w"> </span></code></pre></td></tr></table>
</div>
</div>
<p>Pod启动后，可以看到文件系统由k8s做好并挂载到了容器里。我们将/etc/hosts文件拷贝到/mnt/rbd/目录去。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></pre></td>
<td class="lntd">
<pre class="chroma">kubectl exec rbd -- df -h|grep rbd
kubectl exec rbd -- cp /etc/hosts /mnt/rbd/
kubectl exec rbd -- cat /mnt/rbd/hosts</pre></td></tr></table>
</div>
</div>
<p>然后将Pod删除、重新挂载foobar image。</p>

<p>前面Pod要求各node上都要有keyring文件，很不方便也不安全。新的Pod我使用推荐的做法：secret（虽然也安全不到哪里）</p>

<p>先创建一个secret。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>Secret<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>ceph-secret<span class="w">
</span><span class="w"></span>type<span class="p">:</span><span class="w"> </span><span class="s2">&#34;kubernetes.io/rbd&#34;</span><span class="w">  
</span><span class="w"></span>data<span class="p">:</span><span class="w">
</span><span class="w">  </span>key<span class="p">:</span><span class="w"> </span>QVFCXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX9PQ==</code></pre></td></tr></table>
</div>
</div>
<p>在新的Pod里Ref这个secret。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>Pod<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>rbd3<span class="w">
</span><span class="w"></span>spec<span class="p">:</span><span class="w">
</span><span class="w">  </span>containers<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>image<span class="p">:</span><span class="w"> </span>gcr.io/nginx<span class="w">
</span><span class="w">      </span>name<span class="p">:</span><span class="w"> </span>rbd-rw<span class="w">
</span><span class="w">      </span>volumeMounts<span class="p">:</span><span class="w">
</span><span class="w">      </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>rbdpd<span class="w">
</span><span class="w">        </span>mountPath<span class="p">:</span><span class="w"> </span>/mnt/rbd<span class="w">
</span><span class="w">  </span>volumes<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>rbdpd<span class="w">
</span><span class="w">      </span>rbd<span class="p">:</span><span class="w">
</span><span class="w">        </span>monitors<span class="p">:</span><span class="w"> 
</span><span class="w">        </span>-<span class="w"> </span><span class="s1">&#39;1.2.3.4:6789&#39;</span><span class="w">
</span><span class="w">        </span>pool<span class="p">:</span><span class="w"> </span>k8s<span class="w">
</span><span class="w">        </span>image<span class="p">:</span><span class="w"> </span>foobar<span class="w">
</span><span class="w">        </span>fsType<span class="p">:</span><span class="w"> </span>ext4<span class="w">
</span><span class="w">        </span>readOnly<span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">        </span>user<span class="p">:</span><span class="w"> </span>admin<span class="w">
</span><span class="w">        </span>secretRef<span class="p">:</span><span class="w">
</span><span class="w">          </span>name<span class="p">:</span><span class="w"> </span>ceph-secret</code></pre></td></tr></table>
</div>
</div>
<p>再来看看前面写到image上的文件还在不在。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl <span class="nb">exec</span> rbd3 -- cat /mnt/rbd/hosts</code></pre></td></tr></table>
</div>
</div>
<h3 id="rbd用作pv-pvc">RBD用作PV/PVC</h3>

<p>先在k8s pool里创建一个名为pv的 image。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">rbd image create pv -s <span class="m">1024</span> -p k8s</code></pre></td></tr></table>
</div>
</div>
<p>再创建一个PV，使用上面创建的image pv。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>PersistentVolume<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>ceph-rbd-pv<span class="w">
</span><span class="w"></span>spec<span class="p">:</span><span class="w">
</span><span class="w">  </span>capacity<span class="p">:</span><span class="w">
</span><span class="w">    </span>storage<span class="p">:</span><span class="w"> </span>1Gi<span class="w">
</span><span class="w">  </span>accessModes<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>ReadWriteOnce<span class="w">
</span><span class="w">  </span>rbd<span class="p">:</span><span class="w">
</span><span class="w">    </span>monitors<span class="p">:</span><span class="w">
</span><span class="w">      </span>-<span class="w"> </span><span class="s1">&#39;1.2.3.4:6789&#39;</span><span class="w">
</span><span class="w">    </span>pool<span class="p">:</span><span class="w"> </span>k8s<span class="w">
</span><span class="w">    </span>image<span class="p">:</span><span class="w"> </span>pv<span class="w">
</span><span class="w">    </span>user<span class="p">:</span><span class="w"> </span>admin<span class="w">
</span><span class="w">    </span>secretRef<span class="p">:</span><span class="w">
</span><span class="w">      </span>name<span class="p">:</span><span class="w"> </span>ceph-secret<span class="w">
</span><span class="w">    </span>fsType<span class="p">:</span><span class="w"> </span>ext4<span class="w">
</span><span class="w">    </span>readOnly<span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">  </span>persistentVolumeReclaimPolicy<span class="p">:</span><span class="w"> </span>Recycle</code></pre></td></tr></table>
</div>
</div>
<p>看下现在pv的状态，还是 Available。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash">kubectl get pv<span class="p">|</span>grep rbd
ceph-rbd-pv                                1Gi        RWO            Recycle          Available</code></pre></td></tr></table>
</div>
</div>
<p>创建一个PVC，要求一块1G的存储。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>PersistentVolumeClaim<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>ceph-rbd-pv-claim<span class="w">
</span><span class="w"></span>spec<span class="p">:</span><span class="w">
</span><span class="w">  </span>accessModes<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>ReadWriteOnce<span class="w">
</span><span class="w">  </span>resources<span class="p">:</span><span class="w">
</span><span class="w">    </span>requests<span class="p">:</span><span class="w">
</span><span class="w">      </span>storage<span class="p">:</span><span class="w"> </span>1Gi</code></pre></td></tr></table>
</div>
</div>
<p>因为上面已经创建满足要求的PV了，可以看到pvc和pv的状态都已经是Bound了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># kubectl get pvc|grep rbd</span>
ceph-rbd-pv-claim   Bound     pvc-bdaf8358-5dc6-11e8-a37d-ecf4bbdeea94   1Gi        RWO            fast           10s
<span class="c1"># kubectl get pv|grep ceph-rbd-pv</span>
ceph-rbd-pv     1Gi    RWO    Recycle    Bound     12m</code></pre></td></tr></table>
</div>
</div>
<h3 id="rbd用作storage-class">RBD用作storage class</h3>

<p>先创建一个SC。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">kind<span class="p">:</span><span class="w"> </span>StorageClass<span class="w">
</span><span class="w"></span>apiVersion<span class="p">:</span><span class="w"> </span>storage.k8s.io/v1<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>fast<span class="w">
</span><span class="w"></span>provisioner<span class="p">:</span><span class="w"> </span>kubernetes.io/rbd<span class="w">
</span><span class="w"></span>parameters<span class="p">:</span><span class="w">
</span><span class="w">  </span>monitors<span class="p">:</span><span class="w"> </span><span class="m">172.25</span>.<span class="m">60.3</span><span class="p">:</span><span class="m">6789</span><span class="w">
</span><span class="w">  </span>adminId<span class="p">:</span><span class="w"> </span>admin<span class="w">
</span><span class="w">  </span>adminSecretName<span class="p">:</span><span class="w"> </span>ceph-secret<span class="w">
</span><span class="w">  </span>adminSecretNamespace<span class="p">:</span><span class="w"> </span>resource-quota<span class="w">
</span><span class="w">  </span>pool<span class="p">:</span><span class="w"> </span>k8s<span class="w">
</span><span class="w">  </span>userId<span class="p">:</span><span class="w"> </span>admin<span class="w">
</span><span class="w">  </span>userSecretName<span class="p">:</span><span class="w"> </span>ceph-secret<span class="w">
</span><span class="w">  </span>fsType<span class="p">:</span><span class="w"> </span>ext4<span class="w">
</span><span class="w">  </span>imageFormat<span class="p">:</span><span class="w"> </span><span class="s2">&#34;2&#34;</span><span class="w">
</span><span class="w">  </span>imageFeatures<span class="p">:</span><span class="w"> </span><span class="s2">&#34;layering&#34;</span></code></pre></td></tr></table>
</div>
</div>
<p>pvc中需要指定其storageClassName为上面创建的sc的name</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">kind<span class="p">:</span><span class="w"> </span>PersistentVolumeClaim<span class="w">
</span><span class="w"></span>apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>rbd-pvc-pod-pvc<span class="w">
</span><span class="w"></span>spec<span class="p">:</span><span class="w">
</span><span class="w">  </span>accessModes<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>ReadWriteOnce<span class="w">
</span><span class="w">  </span>volumeMode<span class="p">:</span><span class="w"> </span>Filesystem<span class="w">
</span><span class="w">  </span>resources<span class="p">:</span><span class="w">
</span><span class="w">    </span>requests<span class="p">:</span><span class="w">
</span><span class="w">      </span>storage<span class="p">:</span><span class="w"> </span>8Gi<span class="w">
</span><span class="w">  </span>storageClassName<span class="p">:</span><span class="w"> </span>fast</code></pre></td></tr></table>
</div>
</div>
<p>RBD只支持 ReadWriteOnce 和 ReadOnlyAll，不支持ReadWriteAll。注意这两者的区别点是，不同nodes之间是否可以同时挂载。同一个node上，即使是ReadWriteOnce，也可以同时挂载到2个容器上的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>Pod<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>labels<span class="p">:</span><span class="w">
</span><span class="w">    </span>test<span class="p">:</span><span class="w"> </span>rbd-pvc-pod<span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>ceph-rbd-sc-pod2<span class="w">
</span><span class="w"></span>spec<span class="p">:</span><span class="w">
</span><span class="w">  </span>containers<span class="p">:</span><span class="w">
</span><span class="w">  </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>ceph-rbd-sc-nginx<span class="w">
</span><span class="w">    </span>image<span class="p">:</span><span class="w"> </span>gcr.io/nginx<span class="w">
</span><span class="w">    </span>volumeMounts<span class="p">:</span><span class="w">
</span><span class="w">    </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>ceph-rbd-vol1<span class="w">
</span><span class="w">      </span>mountPath<span class="p">:</span><span class="w"> </span>/mnt/ceph-rbd-pvc/nginx<span class="w">
</span><span class="w">      </span>readOnly<span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span><span class="w">  </span>volumes<span class="p">:</span><span class="w">
</span><span class="w">  </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>ceph-rbd-vol1<span class="w">
</span><span class="w">    </span>persistentVolumeClaim<span class="p">:</span><span class="w">
</span><span class="w">      </span>claimName<span class="p">:</span><span class="w"> </span>rbd-pvc-pod-pvc</code></pre></td></tr></table>
</div>
</div>
<p>如果是多副本的应用怎么办呢？可以用StatefulSet。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-yaml" data-lang="yaml">apiVersion<span class="p">:</span><span class="w"> </span>apps/v1<span class="w">
</span><span class="w"></span>kind<span class="p">:</span><span class="w"> </span>StatefulSet<span class="w">
</span><span class="w"></span>metadata<span class="p">:</span><span class="w">
</span><span class="w">  </span>name<span class="p">:</span><span class="w"> </span>nginx<span class="w">
</span><span class="w"></span>spec<span class="p">:</span><span class="w">
</span><span class="w">  </span>selector<span class="p">:</span><span class="w">
</span><span class="w">    </span>matchLabels<span class="p">:</span><span class="w">
</span><span class="w">      </span>app<span class="p">:</span><span class="w"> </span>nginx<span class="w">
</span><span class="w">  </span>serviceName<span class="p">:</span><span class="w"> </span><span class="s2">&#34;nginx&#34;</span><span class="w">
</span><span class="w">  </span>replicas<span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span>template<span class="p">:</span><span class="w">
</span><span class="w">    </span>metadata<span class="p">:</span><span class="w">
</span><span class="w">      </span>labels<span class="p">:</span><span class="w">
</span><span class="w">        </span>app<span class="p">:</span><span class="w"> </span>nginx<span class="w">
</span><span class="w">    </span>spec<span class="p">:</span><span class="w">
</span><span class="w">      </span>terminationGracePeriodSeconds<span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span><span class="w">      </span>containers<span class="p">:</span><span class="w">
</span><span class="w">      </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>nginx<span class="w">
</span><span class="w">        </span>image<span class="p">:</span><span class="w"> </span>gcr.io/nginx<span class="w">
</span><span class="w">        </span>volumeMounts<span class="p">:</span><span class="w">
</span><span class="w">        </span>-<span class="w"> </span>name<span class="p">:</span><span class="w"> </span>www<span class="w">
</span><span class="w">          </span>mountPath<span class="p">:</span><span class="w"> </span>/usr/share/nginx/html<span class="w">
</span><span class="w">  </span>volumeClaimTemplates<span class="p">:</span><span class="w">
</span><span class="w">  </span>-<span class="w"> </span>metadata<span class="p">:</span><span class="w">
</span><span class="w">      </span>name<span class="p">:</span><span class="w"> </span>www<span class="w">
</span><span class="w">    </span>spec<span class="p">:</span><span class="w">
</span><span class="w">      </span>accessModes<span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="s2">&#34;ReadWriteOnce&#34;</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="w">      </span>storageClassName<span class="p">:</span><span class="w"> </span><span class="s2">&#34;fast&#34;</span><span class="w">
</span><span class="w">      </span>resources<span class="p">:</span><span class="w">
</span><span class="w">        </span>requests<span class="p">:</span><span class="w">
</span><span class="w">          </span>storage<span class="p">:</span><span class="w"> </span>8Gi</code></pre></td></tr></table>
</div>
</div>
<p>此时去看PVC，可以看到创建了3个PVC。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># kubectl get pvc|grep www</span>
www-nginx-0         Bound     pvc-05f10f64-58df-11e8-8bd4-ecf4bbdeea94   8Gi        RWO            fast           6d
www-nginx-1         Bound     pvc-0e1768f3-58df-11e8-8bd4-ecf4bbdeea94   8Gi        RWO            fast           6d
www-nginx-2         Bound     pvc-17347e8e-58df-11e8-8bd4-ecf4bbdeea94   8Gi        RWO            fast           6d

<span class="c1"># kubectl get pv|grep www</span>
pvc-05f10f64-58df-11e8-8bd4-ecf4bbdeea94   8Gi        RWO            Delete           Bound       dex/www-nginx-0                     fast                     6d
pvc-0e1768f3-58df-11e8-8bd4-ecf4bbdeea94   8Gi        RWO            Delete           Bound       dex/www-nginx-1                     fast                     6d
pvc-17347e8e-58df-11e8-8bd4-ecf4bbdeea94   8Gi        RWO            Delete           Bound       dex/www-nginx-2                     fast                     6d</code></pre></td></tr></table>
</div>
</div>
<p>但注意不要用Deployment。因为，如果Deployment的副本数是1，那么还是可以用的，跟Pod一致；但如果副本数 &gt;1 ，此时创建deployment后会发现，只启动了1个Pod，其他Pod都在ContainerCreating状态。过一段时间describe pod可以看到，等volume等很久都没等到。</p>

<h1 id="参考文章">参考文章</h1>

<ul>
<li><a href="https://ieevee.com/tech/2018/05/16/k8s-rbd.html">kubernetes笔记: Ceph RBD</a></li>
<li><a href="https://programmer.help/blogs/k8s-and-eph-rbd-integration-examples-of-multi-master-and-master-slave-databases.html">K8S and Eph RBD Integration - Examples of Multi-master and Master-slave Databases</a></li>
</ul>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">JavaChen</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2019-11-24
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/wechatpay.jpg">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="/img/reward/alipay.jpg">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/ceph/">ceph</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2019/11/28/some-commads-of-docker/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Docker常用命令</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/2019/11/22/install-file-fdw-for-postgresql/">
            <span class="next-text nav-default">PostgreSQL安装并测试file_fdw</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="javachen/javachen.github.io"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:junecloud@163.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/javachen" class="iconfont icon-github" title="github"></a>
      <a href="http://weibo.com/chenzhijun" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://space.bilibili.com/287563020/" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://blog.javachen.space/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2009 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">JavaChen</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.e1476869.min.js"></script>

<script id="baidu_analytics">
  var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?7eaf37274cf8796df56903a88389e82f";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>

<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
